{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text corpora\n",
    "\n",
    "Your text data usually comes in the form of (long) plain text strings that are stored in one or several files on disk. We can load and transform this data into a [`Corpus`](api.rst#TODO) object so that we can perform all kinds of operations that are implemented as *corpus functions* in tmtoolkit. The `Corpus` class itself resembles a [Python dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) with some additional functionality.\n",
    "\n",
    "Let's import the `Corpus` class first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.corpus import Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading text data\n",
    "\n",
    "Several methods are implemented to load text data from different sources:\n",
    "\n",
    "- load built-in datasets\n",
    "- load plain text files (\".txt files\")\n",
    "- load folder(s) with plain text files\n",
    "- load a tabular (i.e. CSV or Excel) file containing document IDs and texts\n",
    "- load a ZIP file containing plain text or tabular files\n",
    "\n",
    "We can create a `Corpus` object directly by immediately loading a dataset using one of the `Corpus.from_...` methods. This is what we've done when we used `corp = Corpus.from_builtin_corpus('en-News100')` in the [previous chapter](getting_started.ipynb).\n",
    "\n",
    "Let's load a folder with example documents. Make sure that the path is relative to the current working directory. The data for these examples can be downloaded from [GitHub](https://github.com/WZBSocialScienceCenter/tmtoolkit/tree/master/doc/source/data). \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Note: Rich text documents\n",
    "\n",
    "If you want to work with \"rich text documents\", i.e. formatted, non-plain text sources such as PDFs, Word documents, HTML files, etc. you must convert them to one of the supported formats first. For example you can use the [pdftotext](https://www.mankier.com/1/pdftotext) command from the Linux package `poppler-utils` to convert from PDF to plain text files or [pandoc](https://pandoc.org/) to convert from Word or HTML to plain text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [3 documents  / language \"en\"]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp = Corpus.from_folder('data/corpus_example', language='en')\n",
    "corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Again, we can have a look which document labels were created and print one sample document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample1', 'sample2', 'sample3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`corpus_summary`](api.rst#TODO) and [`print_summary`](api.rst#TODO) functions are very helpful to get a first overview of a loaded corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 3 documents in English\n",
      "> sample1 (25 tokens): This is the first example file . ☺ We showcase NER...\n",
      "> sample2 (20 tokens): Here comes the second example .    This one contai...\n",
      "> sample3 (36 tokens): And here we go with the third and final example fi...\n",
      "total number of tokens: 81 / vocabulary size: 53\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import print_summary\n",
    "\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Side note: Corpus functions\n",
    "\n",
    "The [`corpus_summary`](api.rst#TODO) and [`print_summary`](api.rst#TODO) functions are examples of *corpus functions*. All corpus functions accept a `Corpus` object as first argument and operate on it. A corpus function may retrieve information from a corpus and/or modify it. Most functions in the `tmtoolkit.corpus` module are corpus functions.\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to create a `Corpus` object and adding further documents using the `corpus_add_...` functions. Here we create an empty `Corpus` and then add documents via [`corpus_add_files`](api.rst#TODO) which is another example of a corpus function (one that modifies a `Corpus` object). It takes a `Corpus` object and one or more paths to raw text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 0 document in English\n",
      "total number of tokens: 0 / vocabulary size: 0\n"
     ]
    }
   ],
   "source": [
    "corp = Corpus(language='en')\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 1 document in English\n",
      "> data_corpus_example-sample1 (25 tokens): This is the first example file . ☺ We showcase NER...\n",
      "total number of tokens: 25 / vocabulary size: 24\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import corpus_add_files\n",
    "\n",
    "corpus_add_files(corp, 'data/corpus_example/sample1.txt')\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that this time the document label is different. Its prefixed by a normalized version of the path to the document. We can alter the `doc_label_fmt` argument of [Corpus.add_files()](api.rst#tmtoolkit.corpus.Corpus.add_files) in order to control how document labels are generated. But at first, let's remove the previously loaded document from the corpus. Since a `Corpus` instance behaves like a Python `dict`, we can use `del`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 0 document in English\n",
      "total number of tokens: 0 / vocabulary size: 0\n"
     ]
    }
   ],
   "source": [
    "del corp['data_corpus_example-sample1']\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we use a modified `doc_label_fmt` paramater value to generate document labels only from the file name and not from the full path to the document. We also load three files now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 3 documents in English\n",
      "> sample1 (25 tokens): This is the first example file . ☺ We showcase NER...\n",
      "> sample2 (20 tokens): Here comes the second example .    This one contai...\n",
      "> sample3 (36 tokens): And here we go with the third and final example fi...\n",
      "total number of tokens: 81 / vocabulary size: 53\n"
     ]
    }
   ],
   "source": [
    "corpus_add_files(corp, ['data/corpus_example/sample1.txt',\n",
    "                        'data/corpus_example/sample2.txt',\n",
    "                        'data/corpus_example/sample3.txt'],\n",
    "                 doc_label_fmt='{basename}')\n",
    "print_summary(corp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As noted in the beginning, there are more `corpus_add_...` and `Corpus.from_...` functions/methods to load text data from different sources. See the [corpus module API](api.rst#TODO) for details.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Note\n",
    "\n",
    "Please be aware of the difference of the `corpus_add_...` and `Corpus.from_...` functions/methods: The former *modifies* a given `Corpus` object, whereas the latter *creates* a new `Corpus` object.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the NLP pipeline, parallel processing and more via Corpus parameters\n",
    "\n",
    "When initializing a `Corpus`, you can pass several arguments. You *must* at least provide either the `language`, `language_model` or `spacy_instance` argument. The simplest is to just pass the `language` as two-letter ISO 639-1 language code. A respective SpaCy language model will then be automatically selected depending on that language code and the NLP pipeline features that you require (more on that next). Alternatively, you set the SpaCy language model to be loaded via `language_model`. Finally, if you already loaded a [SpaCy pipeline instance](https://spacy.io/api/language) yourself you can also pass it via the `spacy_instance` parameter.\n",
    "\n",
    "You can specify the features, i.e. the components, of the NLP pipeline using the `load_features` parameter. This only applies if you don't provide your own pipeline instance via `spacy_instance`. It determines the pipeline's capabilities, i.e. if POS-tags, named entities, sentences, etc. are recognized. By default, tmtoolkit will load and enable all components but the NER component. The more components are enabled, the slower the text processing works and the more memory it uses. So it makes sense to only enable pipelines that you actually use. See the [SpaCy documentation](https://spacy.io/models#design) for which pipelines are implemented (note that this also depends on the language and language model).\n",
    "\n",
    "Let's create a corpus with only a minimal pipeline. If we set `load_features` to an empty sequence, only a basic tokenizing pipeline is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_minimal = Corpus.from_folder('data/corpus_example', language='en', load_features=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the token table output of the minimal pipeline with the default pipeline used in the previously loaded `corp` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>like_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1</td>\n",
       "      <td>0</td>\n",
       "      <td>This</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample1</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample1</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample1</td>\n",
       "      <td>3</td>\n",
       "      <td>first</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample1</td>\n",
       "      <td>4</td>\n",
       "      <td>example</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>sample3</td>\n",
       "      <td>31</td>\n",
       "      <td>third</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>sample3</td>\n",
       "      <td>32</td>\n",
       "      <td>and</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>sample3</td>\n",
       "      <td>33</td>\n",
       "      <td>final</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>sample3</td>\n",
       "      <td>34</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>sample3</td>\n",
       "      <td>35</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc  position      token  is_punct  is_stop  like_num\n",
       "0   sample1         0       This     False     True     False\n",
       "1   sample1         1         is     False     True     False\n",
       "2   sample1         2        the     False     True     False\n",
       "3   sample1         3      first     False     True      True\n",
       "4   sample1         4    example     False    False     False\n",
       "..      ...       ...        ...       ...      ...       ...\n",
       "76  sample3        31      third     False     True      True\n",
       "77  sample3        32        and     False     True     False\n",
       "78  sample3        33      final     False    False     False\n",
       "79  sample3        34  paragraph     False    False     False\n",
       "80  sample3        35          .      True    False     False\n",
       "\n",
       "[81 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import tokens_table\n",
    "\n",
    "tokens_table(corp_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1</td>\n",
       "      <td>0</td>\n",
       "      <td>This</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>this</td>\n",
       "      <td>False</td>\n",
       "      <td>PRON</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample1</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>be</td>\n",
       "      <td>False</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample1</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample1</td>\n",
       "      <td>3</td>\n",
       "      <td>first</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample1</td>\n",
       "      <td>4</td>\n",
       "      <td>example</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>example</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>sample3</td>\n",
       "      <td>31</td>\n",
       "      <td>third</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>third</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>sample3</td>\n",
       "      <td>32</td>\n",
       "      <td>and</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>and</td>\n",
       "      <td>False</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>sample3</td>\n",
       "      <td>33</td>\n",
       "      <td>final</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>final</td>\n",
       "      <td>False</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>sample3</td>\n",
       "      <td>34</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>sample3</td>\n",
       "      <td>35</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc  position      token  is_punct  is_stop      lemma  like_num  \\\n",
       "0   sample1         0       This     False     True       this     False   \n",
       "1   sample1         1         is     False     True         be     False   \n",
       "2   sample1         2        the     False     True        the     False   \n",
       "3   sample1         3      first     False     True      first      True   \n",
       "4   sample1         4    example     False    False    example     False   \n",
       "..      ...       ...        ...       ...      ...        ...       ...   \n",
       "76  sample3        31      third     False     True      third      True   \n",
       "77  sample3        32        and     False     True        and     False   \n",
       "78  sample3        33      final     False    False      final     False   \n",
       "79  sample3        34  paragraph     False    False  paragraph     False   \n",
       "80  sample3        35          .      True    False          .     False   \n",
       "\n",
       "      pos  tag  \n",
       "0    PRON   DT  \n",
       "1     AUX  VBZ  \n",
       "2     DET   DT  \n",
       "3     ADJ   JJ  \n",
       "4    NOUN   NN  \n",
       "..    ...  ...  \n",
       "76    ADJ   JJ  \n",
       "77  CCONJ   CC  \n",
       "78    ADJ   JJ  \n",
       "79   NOUN   NN  \n",
       "80  PUNCT    .  \n",
       "\n",
       "[81 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_table(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the minimal pipeline produces less information: The lemmata and POS tags are missing from the table for the corpus using the minimal pipeline. Also, if we'd wanted sentences, we'd get an error since sentences recognition is not enabled in the minimal pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tokens_table(corp_minimal, sentences=True)\n",
    "# results in ValueError: sentence numbers requested, but sentence borders not set; Corpus documents probably not parsed with sentence recognition\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the default pipeline as basis and add new components via the `add_features` parameter. Here, we add the NER component for finding out named entities like persons. The table then shows a new column `ent_type` with `\"PERSON\"` entries next to names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sample1</td>\n",
       "      <td>15</td>\n",
       "      <td>famous</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>famous</td>\n",
       "      <td>False</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sample1</td>\n",
       "      <td>16</td>\n",
       "      <td>people</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>people</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sample1</td>\n",
       "      <td>17</td>\n",
       "      <td>like</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sample1</td>\n",
       "      <td>18</td>\n",
       "      <td>Missy</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Missy</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sample1</td>\n",
       "      <td>19</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sample1</td>\n",
       "      <td>20</td>\n",
       "      <td>or</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>or</td>\n",
       "      <td>False</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sample1</td>\n",
       "      <td>21</td>\n",
       "      <td>George</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>George</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sample1</td>\n",
       "      <td>22</td>\n",
       "      <td>Harrison</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Harrison</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sample1</td>\n",
       "      <td>23</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sample1</td>\n",
       "      <td>24</td>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>_SP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc  position     token ent_type  is_punct  is_stop     lemma  \\\n",
       "15  sample1        15    famous              False    False    famous   \n",
       "16  sample1        16    people              False    False    people   \n",
       "17  sample1        17      like              False    False      like   \n",
       "18  sample1        18     Missy   PERSON     False    False     Missy   \n",
       "19  sample1        19   Elliott   PERSON     False    False   Elliott   \n",
       "20  sample1        20        or              False     True        or   \n",
       "21  sample1        21    George   PERSON     False    False    George   \n",
       "22  sample1        22  Harrison   PERSON     False    False  Harrison   \n",
       "23  sample1        23         .               True    False         .   \n",
       "24  sample1        24        \\n              False    False        \\n   \n",
       "\n",
       "    like_num    pos  tag  \n",
       "15     False    ADJ   JJ  \n",
       "16     False   NOUN  NNS  \n",
       "17     False    ADP   IN  \n",
       "18     False  PROPN  NNP  \n",
       "19     False  PROPN  NNP  \n",
       "20     False  CCONJ   CC  \n",
       "21     False  PROPN  NNP  \n",
       "22     False  PROPN  NNP  \n",
       "23     False  PUNCT    .  \n",
       "24     False  SPACE  _SP  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_ner = Corpus.from_folder('data/corpus_example', language='en', add_features=['ner'])\n",
    "\n",
    "tokens_table(corp_ner, select='sample1').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final important parameter is the `max_workers` setting. With it, you can enable parallel processing and specify the number of worker processes that can run in parallel. You should never set this value higher than the number of CPU cores in your machine (it will still work, but it will actually slow down the processing). Parallel processing makes most sense for large corpora with thousands of documents. With the small toy examples in this tutorial, you won't see performance gains (you will actually see performance loss due to increased overhead for setting up the parallel processing).\n",
    "\n",
    "You can provide an integer value to `max_workers`, which sets the number of worker processes. 0 or 1 disable parallel processing (the default behavior).\n",
    "\n",
    "Here, we use two workers, i.e. two CPU cores of our machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [3 documents  / 2 worker processes / language \"en\"]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_2w = Corpus.from_folder('data/corpus_example', language='en', max_workers=2)\n",
    "corp_2w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use a negative number `x`, this means \"use all available CPU cores, but leave `x` spare cores. For example, when your machine has four CPU cores, three of them will be used by tmtoolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [3 documents  / 3 worker processes / language \"en\"]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_allbutone = Corpus.from_folder('data/corpus_example', language='en', max_workers=-1)\n",
    "corp_allbutone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can pass a float value to the `max_workers` parameter which specifies the fraction of available CPU cores to use (rounded). Here, we use 50% of the available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [3 documents  / 2 worker processes / language \"en\"]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_halfcpus = Corpus.from_folder('data/corpus_example', language='en', max_workers=0.5)\n",
    "corp_halfcpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corp_2w, corp_allbutone, corp_halfcpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus properties, iteration and corpus functions for document management\n",
    "\n",
    "A `Corpus` object provides several helpful properties that summarize the text data and several corpus functions to manage the documents.\n",
    "\n",
    "### Properties\n",
    "\n",
    "Let's start with the number of documents in the corpus. There are two ways to obtain this value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.n_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important properties are the language of the documents in the corpus and the SpaCy language model used in the NLP pipeline (note that so far tmtoolkit requires that all documents within one corpus use the same language):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_core_web_sm'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus' SpaCy NLP pipeline is given with the `nlp` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f81bf0b09d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can configure the NLP pipeline (as shown in the previous section), sentence recognition is optional. You can check if sentences were recognized using the following property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.has_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `max_workers` property shows how many worker processes are used during computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.max_workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set new values for this property in order to temporarily enable parallel processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [3 documents  / 2 worker processes / language \"en\"]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.max_workers = 2\n",
    "corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp.max_workers = 1   # reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another important property is the list of document labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample1', 'sample2', 'sample3']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a `Corpus` object like with a dictionary\n",
    "\n",
    "A `Corpus` object resembles a dictionary, i.e. it provides the same methods. The keys of this corpus-dictionary are the unique document labels and the values or elements are the respective `Document` objects.\n",
    "\n",
    "You can access a `Corpus` object's elements via square brackets (`corp[<doc. label>]`) or `corp.get(<doc. label>)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document \"sample1\" (25 tokens, 9 token attributes, 2 document attributes)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp['sample1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document \"sample2\" (20 tokens, 9 token attributes, 2 document attributes)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.get('sample2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square brackets also accept an integer *i*, which will give you the document with the *i*th document label (corresponding to the order in `corp.doc_labels`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document \"sample3\" (36 tokens, 9 token attributes, 2 document attributes)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing is also supported and returns a list of `Document` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document \"sample1\" (25 tokens, 9 token attributes, 2 document attributes),\n",
       " Document \"sample2\" (20 tokens, 9 token attributes, 2 document attributes)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also update existing documents or add new documents using the bracket syntax. For this, you can either pass a string, a `Document` object or a [SpaCy `Doc`](https://spacy.io/api/doc) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 3 documents in English\n",
      "> sample1 (25 tokens): This is the first example file . ☺ We showcase NER...\n",
      "> sample2 (20 tokens): Here comes the second example .    This one contai...\n",
      "> sample3 (36 tokens): And here we go with the third and final example fi...\n",
      "total number of tokens: 81 / vocabulary size: 53\n"
     ]
    }
   ],
   "source": [
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 4 documents in English\n",
      "> sample1 (25 tokens): This is the first example file . ☺ We showcase NER...\n",
      "> sample2 (10 tokens): This is the updated version of the second example ...\n",
      "> sample3 (36 tokens): And here we go with the third and final example fi...\n",
      "> sample4 (8 tokens): This document was added as fourth example .\n",
      "total number of tokens: 79 / vocabulary size: 60\n"
     ]
    }
   ],
   "source": [
    "corp['sample2'] = 'This is the updated version of the second example.'\n",
    "corp['sample4'] = 'This document was added as fourth example.'\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The dictionary-interface of the `Corpus` class also allows to iterate through its contents via `corp.items()`, `corp.keys()` or `corp.values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length for sample1: 25\n",
      "Document length for sample2: 10\n",
      "Document length for sample3: 36\n",
      "Document length for sample4: 8\n"
     ]
    }
   ],
   "source": [
    "# iterate through pairs of document labels `lbl` and\n",
    "# document objects `d` and report each document's length\n",
    "for lbl, d in corp.items():\n",
    "    print(f'Document length for {lbl}: {len(d)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'sample1', 'has_sents': True},\n",
       " {'label': 'sample2', 'has_sents': True},\n",
       " {'label': 'sample3', 'has_sents': True},\n",
       " {'label': 'sample4', 'has_sents': True}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a list of each document's document attributes\n",
    "# (more on document attributes in the next chapter)\n",
    "[d.doc_attrs for d in corp.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Corpus functions for document management\n",
    "\n",
    "#### Splitting documents\n",
    "\n",
    "Sometimes you may want to split larger documents. In our example corpus, document \"sample3\" consists of three paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And here we go with the third and final example file.\n",
      "Another line of text.\n",
      "\n",
      "§2.\n",
      "This is the second paragraph.\n",
      "\n",
      "The third and final paragraph.\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import doc_texts\n",
    "\n",
    "print(doc_texts(corp)['sample3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`corpus_split_by_paragraph`](api.rst#TODO) function for splitting documents by paragraphs. There's also [`corpus_split_by_token`](api.rst#TODO) for splitting by an arbitrary token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 6 documents in English\n",
      "> sample1-1 (25 tokens): This is the first example file . ☺ We showcase NER...\n",
      "> sample2-1 (10 tokens): This is the updated version of the second example ...\n",
      "> sample3-1 (19 tokens): And here we go with the third and final example fi...\n",
      "> sample3-2 (11 tokens): § 2 .   This is the second paragraph .   \n",
      "> sample3-3 (6 tokens): The third and final paragraph .\n",
      "> sample4-1 (8 tokens): This document was added as fourth example .\n",
      "total number of tokens: 79 / vocabulary size: 60\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import corpus_split_by_paragraph\n",
    "\n",
    "corpus_split_by_paragraph(corp)\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the \"sample3\" document was split into three individual documents, one per paragraph. You can further customize the splitting process by tweaking the parameters, e.g. the minimum number of line breaks used to detect paragraphs (default is two line breaks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining documents\n",
    "\n",
    "There's of course also a function for doing the inverse operation, i.e. joining several documents. This function is called [`corpus_join_documents`](api.rst#TODO) and it accepts a dictionary that maps a name for the newly joint document to a string pattern or a list of string patterns of documents to be joint. This function is especially helpful when you want to bundle lots of smaller documents (e.g. tweets) into a bigger document (e.g. all tweets of one account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 4 documents in English\n",
      "> sample1-1 (25 tokens): This is the first example file . ☺ We showcase NER...\n",
      "> sample2-1 (10 tokens): This is the updated version of the second example ...\n",
      "> sample4-1 (8 tokens): This document was added as fourth example .\n",
      "> sample3-rejoined (36 tokens): § 2 .   This is the second paragraph .    And here...\n",
      "total number of tokens: 79 / vocabulary size: 49\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import corpus_join_documents\n",
    "\n",
    "# alternative: no matching, specify each document name in a list\n",
    "# corpus_join_documents(corp, {'sample3-rejoined': ['sample3-1', 'sample3-2', 'sample3-3']})\n",
    "\n",
    "# join all documents matching \"sample3*\" to form a new document \"sample3-rejoined\"\n",
    "# \"sample3*\" is a glob pattern, so we set this as `match_type` (more on that in the next chapter)\n",
    "# we also set glue to an empty string, because we don't need to have additional line breaks\n",
    "# between the documents\n",
    "corpus_join_documents(corp, {'sample3-rejoined': 'sample3*'}, glue='', match_type='glob')\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "§2.\n",
      "This is the second paragraph.\n",
      "\n",
      "And here we go with the third and final example file.\n",
      "Another line of text.\n",
      "\n",
      "The third and final paragraph.\n"
     ]
    }
   ],
   "source": [
    "print(doc_texts(corp)['sample3-rejoined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sampling a corpus   \n",
    "\n",
    "Finally you can sample the documents in a corpus using [Corpus.sample()](api.rst#tmtoolkit.corpus.Corpus.sample). To get a random sample of three documents from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that this returns a new `Corpus` instance by default. You can pass `as_corpus=False` if you only need a Python dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [next chapter](preprocessing.ipynb) will show how to apply several text preprocessing functions to a corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
