{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text corpora\n",
    "\n",
    "Your text data usually comes in the form of (long) plain text strings that are stored in one or several files on disk. We can load and transform this data into a [`Corpus`](api.rst#TODO) object so that we can perform all kinds of operations that are implemented as *corpus functions* in tmtoolkit. The [`Corpus`](api.rst#TODO) class itself resembles a [Python dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) with some additional functionality.\n",
    "\n",
    "Let's import the `Corpus` class first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.corpus import Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading text data\n",
    "\n",
    "Several methods are implemented to load text data from different sources:\n",
    "\n",
    "- load built-in datasets\n",
    "- load plain text files (\".txt files\")\n",
    "- load folder(s) with plain text files\n",
    "- load a tabular (i.e. CSV or Excel) file containing document IDs and texts\n",
    "- load a ZIP file containing plain text or tabular files\n",
    "\n",
    "We can create a `Corpus` object directly by immediately loading a dataset using one of the `Corpus.from_...` methods. This is what we've done when we used `corp = Corpus.from_builtin_corpus('en-News100')` in the [previous chapter](getting_started.ipynb).\n",
    "\n",
    "Let's load a folder with example documents. Make sure that the path is relative to the current working directory. The data for these examples can be downloaded from [GitHub](https://github.com/WZBSocialScienceCenter/tmtoolkit/tree/master/doc/source/data). \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Note: Rich text documents\n",
    "\n",
    "If you want to work with \"rich text documents\", i.e. formatted, non-plain text sources such as PDFs, Word documents, HTML files, etc. you must convert them to one of the supported formats first. For example you can use the [pdftotext](https://www.mankier.com/1/pdftotext) command from the Linux package `poppler-utils` to convert from PDF to plain text files or [pandoc](https://pandoc.org/) to convert from Word or HTML to plain text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [3 documents  / language \"en\"]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp = Corpus.from_folder('data/corpus_example', language='en')\n",
    "corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Again, we can have a look which document labels were created and print one sample document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample1', 'sample2', 'sample3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`corpus_summary`](api.rst#TODO) and [`print_summary`](api.rst#TODO) functions are very helpful to get a first overview of a loaded corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 3 documents in English\n",
      "> sample1 (8 tokens): This is the first example file . ☺\n",
      "> sample2 (20 tokens): Here comes the second example .    This one contai...\n",
      "> sample3 (36 tokens): And here we go with the third and final example fi...\n",
      "total number of tokens: 64 / vocabulary size: 38\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import print_summary\n",
    "\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Side note: Corpus functions\n",
    "\n",
    "The [`corpus_summary`](api.rst#TODO) and [`print_summary`](api.rst#TODO) functions are examples of *corpus functions*. All corpus functions accept a [`Corpus`](api.rst#TODO) object as first argument and operate on it. A corpus function may retrieve information from a corpus and/or modify it. Most functions in the `tmtoolkit.corpus` module are corpus functions.\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to create a `Corpus` object and adding further documents using the `corpus_add_...` functions. Here we create an empty `Corpus` and then add documents via [`corpus_add_files`](api.rst#TODO) which is another example of a corpus function (one that modifies a `Corpus` object). It takes a `Corpus` object and one or more paths to raw text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 0 document in English\n",
      "total number of tokens: 0 / vocabulary size: 0\n"
     ]
    }
   ],
   "source": [
    "corp = Corpus(language='en')\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 1 document in English\n",
      "> data_corpus_example-sample1 (8 tokens): This is the first example file . ☺\n",
      "total number of tokens: 8 / vocabulary size: 8\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import corpus_add_files\n",
    "\n",
    "corpus_add_files(corp, 'data/corpus_example/sample1.txt')\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that this time the document label is different. Its prefixed by a normalized version of the path to the document. We can alter the `doc_label_fmt` argument of [Corpus.add_files()](api.rst#tmtoolkit.corpus.Corpus.add_files) in order to control how document labels are generated. But at first, let's remove the previously loaded document from the corpus. Since a `Corpus` instance behaves like a Python `dict`, we can use `del`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 0 document in English\n",
      "total number of tokens: 0 / vocabulary size: 0\n"
     ]
    }
   ],
   "source": [
    "del corp['data_corpus_example-sample1']\n",
    "print_summary(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we use a modified `doc_label_fmt` paramater value to generate document labels only from the file name and not from the full path to the document. We also load three files now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 3 documents in English\n",
      "> sample1 (8 tokens): This is the first example file . ☺\n",
      "> sample2 (20 tokens): Here comes the second example .    This one contai...\n",
      "> sample3 (36 tokens): And here we go with the third and final example fi...\n",
      "total number of tokens: 64 / vocabulary size: 38\n"
     ]
    }
   ],
   "source": [
    "corpus_add_files(corp, ['data/corpus_example/sample1.txt',\n",
    "                        'data/corpus_example/sample2.txt',\n",
    "                        'data/corpus_example/sample3.txt'],\n",
    "                 doc_label_fmt='{basename}')\n",
    "print_summary(corp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As noted in the beginning, there are more `corpus_add_...` and `Corpus.from_...` functions/methods to load text data from different sources. See the [corpus module API](api.rst#TODO) for details.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Note\n",
    "\n",
    "Please be aware of the difference of the `corpus_add_...` and `Corpus.from_...` functions/methods: The former *modifies* a given `Corpus` object, whereas the latter *creates* a new `Corpus` object.\n",
    "\n",
    "</div>\n",
    "\n",
    "## Corpus properties\n",
    "\n",
    "A `Corpus` object provides several helpful properties that summarize the text data and several methods to manage the documents.\n",
    "\n",
    "Let's start with the number of documents in the corpus. There are two ways to obtain this value: \n",
    "\n",
    "TODO: cont. here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.n_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another important property is the number of characters per document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample1', 'sample2', 'sample3']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.doc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_core_web_sm'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.has_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.max_workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Splitting by paragraphs\n",
    "\n",
    "Another helpful method is [Corpus.split_by_paragraphs()](api.rst#tmtoolkit.corpus.Corpus.split_by_paragraphs). This allows splitting each document of the corpus by paragraph.\n",
    "\n",
    "Again, let's have a look at our current corpus' documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1 :\n",
      "This is the first example file. \n",
      "---\n",
      "\n",
      "sample2 :\n",
      "Here comes the second example.\n",
      "\n",
      "This one contains three lines of plain text which means two paragraphs.\n",
      "---\n",
      "\n",
      "sample3 :\n",
      "And here we go with the third and final example file.\n",
      "Another line of text.\n",
      "\n",
      "§2.\n",
      "This is the second paragraph.\n",
      "\n",
      "The third and final paragraph.\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see, `sample1` contains one paragraph, `sample2` two and `sample3` three paragraphs. Now we can split those and get the expected number of documents (each paragraph is then an individual document):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [6 documents]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.split_by_paragraphs()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our newly created six documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1-1 :\n",
      "This is the first example file. \n",
      "---\n",
      "\n",
      "sample2-1 :\n",
      "Here comes the second example.\n",
      "---\n",
      "\n",
      "sample2-2 :\n",
      "This one contains three lines of plain text which means two paragraphs.\n",
      "---\n",
      "\n",
      "sample3-1 :\n",
      "And here we go with the third and final example file. Another line of text.\n",
      "---\n",
      "\n",
      "sample3-2 :\n",
      "§2. This is the second paragraph.\n",
      "---\n",
      "\n",
      "sample3-3 :\n",
      "The third and final paragraph.\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can further customize the splitting process by tweaking the parameters, e.g. the minimum number of line breaks used to detect paragraphs (default is two line breaks).\n",
    "\n",
    "### Sampling a corpus   \n",
    "\n",
    "Finally you can sample the documents in a corpus using [Corpus.sample()](api.rst#tmtoolkit.corpus.Corpus.sample). To get a random sample of three documents from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [3 documents]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample1-1', 'sample2-1', 'sample2-2', 'sample3-1', 'sample3-2', 'sample3-3']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that this returns a new `Corpus` instance by default. You can pass `as_corpus=False` if you only need a Python dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [next chapter](preprocessing.ipynb) will show how to apply several text preprocessing functions to a corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
