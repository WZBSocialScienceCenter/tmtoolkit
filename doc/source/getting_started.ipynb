{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "This is only quick overview for getting started. Corpus loading, text preprocessing, etc. are explained in depth in the respective chapters.\n",
    "\n",
    "## Loading a built-in text corpus\n",
    "\n",
    "Once you have installed tmtoolkit, you can start by loading a built-in dataset. Note that you must have installed tmtoolkit with the ``[recommended]`` or ``[textproc]`` option for this to work. See the [installation instructions](install.rst) for details.\n",
    "\n",
    "Let's import the [builtin_corpora_info](api.rst#tmtoolkit.corpus.builtin_corpora_info) function first and have a look which datasets are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:03.188169Z",
     "iopub.status.busy": "2022-02-02T08:27:03.187399Z",
     "iopub.status.idle": "2022-02-02T08:27:04.779477Z",
     "shell.execute_reply": "2022-02-02T08:27:04.779902Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de-parlspeech-v2-sample-bundestag',\n",
       " 'en-News100',\n",
       " 'en-NewsArticles',\n",
       " 'en-parlspeech-v2-sample-houseofcommons',\n",
       " 'es-parlspeech-v2-sample-congreso',\n",
       " 'nl-parlspeech-v2-sample-tweedekamer']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import builtin_corpora_info\n",
    "\n",
    "builtin_corpora_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load one of these corpora, a sample of 100 articles from the [News Articles dataset from Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GMFCTR). For this, we import the [Corpus](api.rst#tmtoolkit.corpus.Corpus) class and use [Corpus.from_builtin_corpus](api.rst#tmtoolkit.corpus.Corpus.from_builtin_corpus). The raw text data will then be processed by an [NLP pipeline](https://spacy.io/usage/spacy-101#pipelines) with [SpaCy](https://spacy.io). That is, it will be tokenized and analyzed for the grammatical structure of each sentence and the linguistic attributes of each token, among other things. Since this step is computationally intense, it takes quite some time for large text corpora (it can be sped up by enabling parallel processing as explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:04.785622Z",
     "iopub.status.busy": "2022-02-02T08:27:04.784904Z",
     "iopub.status.idle": "2022-02-02T08:27:13.434589Z",
     "shell.execute_reply": "2022-02-02T08:27:13.434948Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [100 documents  / language \"en\"]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import Corpus\n",
    "\n",
    "corp = Corpus.from_builtin_corpus('en-News100')\n",
    "corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look which documents were loaded (showing only the first ten document labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.439472Z",
     "iopub.status.busy": "2022-02-02T08:27:13.438981Z",
     "iopub.status.idle": "2022-02-02T08:27:13.441450Z",
     "shell.execute_reply": "2022-02-02T08:27:13.441831Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['News100-2338',\n",
       " 'News100-3228',\n",
       " 'News100-1253',\n",
       " 'News100-1615',\n",
       " 'News100-3334',\n",
       " 'News100-92',\n",
       " 'News100-869',\n",
       " 'News100-3092',\n",
       " 'News100-3088',\n",
       " 'News100-1173']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.doc_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing documents and document tokens\n",
    "\n",
    "We can now access each document in this corpus via its document label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.445852Z",
     "iopub.status.busy": "2022-02-02T08:27:13.445208Z",
     "iopub.status.idle": "2022-02-02T08:27:13.448315Z",
     "shell.execute_reply": "2022-02-02T08:27:13.448707Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document \"News100-2338\" (680 tokens, 9 token attributes, 2 document attributes)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp['News100-2338']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By accessing the corpus in this way, we get a [Document](api.rst#tmtoolkit.corpus.Document) object. We can query a document for its contents again using the square brackets syntax. Here, we access its tokens and show only the first ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.453262Z",
     "iopub.status.busy": "2022-02-02T08:27:13.452756Z",
     "iopub.status.idle": "2022-02-02T08:27:13.455889Z",
     "shell.execute_reply": "2022-02-02T08:27:13.455448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'This',\n",
       " 'Is',\n",
       " 'Us',\n",
       " \"'\",\n",
       " 'Makes',\n",
       " 'Surprising',\n",
       " 'Reveal',\n",
       " 'About',\n",
       " 'Jack']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp['News100-2338']['token'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, you won't need to access the `Document` objects of a corpus directly. You can rather use functions that provide a convenient interface to a corpus' contents, e.g. the [doc_tokens](api.rst#tmtoolkit.corpus.doc_tokens) function which allows to retrieve all documents' tokens along with additional token attributes like Part-of-Speech (POS) tags, token lemma, etc.\n",
    "\n",
    "Let's first import `doc_tokens` and then list the first ten tokens of the documents \"News100-2338\" and \"News100-3228\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.482254Z",
     "iopub.status.busy": "2022-02-02T08:27:13.481785Z",
     "iopub.status.idle": "2022-02-02T08:27:13.484229Z",
     "shell.execute_reply": "2022-02-02T08:27:13.483683Z"
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.corpus import doc_tokens\n",
    "\n",
    "tokens = doc_tokens(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.488466Z",
     "iopub.status.busy": "2022-02-02T08:27:13.487803Z",
     "iopub.status.idle": "2022-02-02T08:27:13.490771Z",
     "shell.execute_reply": "2022-02-02T08:27:13.490353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'This',\n",
       " 'Is',\n",
       " 'Us',\n",
       " \"'\",\n",
       " 'Makes',\n",
       " 'Surprising',\n",
       " 'Reveal',\n",
       " 'About',\n",
       " 'Jack']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['News100-2338'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.494676Z",
     "iopub.status.busy": "2022-02-02T08:27:13.494051Z",
     "iopub.status.idle": "2022-02-02T08:27:13.496531Z",
     "shell.execute_reply": "2022-02-02T08:27:13.496908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neil',\n",
       " 'Gorsuch',\n",
       " 'facing',\n",
       " \"'\",\n",
       " 'rigorous',\n",
       " \"'\",\n",
       " 'confirmation',\n",
       " 'hearing',\n",
       " 'this',\n",
       " 'week']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['News100-3228'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve more information than just the tokens. Let's also get the POS tags via `with_attr='pos'` and enable structuring the results according to the sentences in the document via `sentences=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.556954Z",
     "iopub.status.busy": "2022-02-02T08:27:13.540401Z",
     "iopub.status.idle": "2022-02-02T08:27:13.562019Z",
     "shell.execute_reply": "2022-02-02T08:27:13.561204Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = doc_tokens(corp, sentences=True, with_attr='pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we now have a dictionary with two entries, \"token\" and \"pos\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.570043Z",
     "iopub.status.busy": "2022-02-02T08:27:13.569367Z",
     "iopub.status.idle": "2022-02-02T08:27:13.574720Z",
     "shell.execute_reply": "2022-02-02T08:27:13.574139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token', 'pos'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['News100-2338'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within these dictionary entries, the tokens and the POS tags are contained inside a list of sentences. So for example to get the POS tags for each token in the fourth sentence (i.e. index 3), we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.583111Z",
     "iopub.status.busy": "2022-02-02T08:27:13.581821Z",
     "iopub.status.idle": "2022-02-02T08:27:13.608269Z",
     "shell.execute_reply": "2022-02-02T08:27:13.606893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PART',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index 3 is the fourth sentence, since indices start with 0\n",
    "tokens['News100-2338']['pos'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could for example combine the tokens and their POS tags by using `zip`. Here we do that for the first five tokens in the fourth sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.619770Z",
     "iopub.status.busy": "2022-02-02T08:27:13.618484Z",
     "iopub.status.idle": "2022-02-02T08:27:13.622974Z",
     "shell.execute_reply": "2022-02-02T08:27:13.620657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('episode', 'NOUN'),\n",
       " ('started', 'VERB'),\n",
       " ('off', 'ADP'),\n",
       " ('with', 'ADP')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens['News100-2338']['token'][3][:5],\n",
    "         tokens['News100-2338']['pos'][3][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an overview about the contents of a corpus, it's often more useful to get it in a tabular format. The tmtoolkit package provides a function to generate a [pandas DataFrame](https://pandas.pydata.org/) from a corpus, [tokens_table](api.rst#tmtoolkit.corpus.tokens_table).\n",
    "\n",
    "We'll use that now and instruct it to also return the sentence index of each token via `sentences=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.642806Z",
     "iopub.status.busy": "2022-02-02T08:27:13.629568Z",
     "iopub.status.idle": "2022-02-02T08:27:13.972137Z",
     "shell.execute_reply": "2022-02-02T08:27:13.971699Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sent</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Kremlin</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Kremlin</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>gives</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>give</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>comment</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>on</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>on</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            doc  sent  position    token  is_punct  is_stop    lemma  \\\n",
       "0  News100-1026     0         0  Kremlin     False    False  Kremlin   \n",
       "1  News100-1026     0         1    gives     False    False     give   \n",
       "2  News100-1026     0         2       no     False     True       no   \n",
       "3  News100-1026     0         3  comment     False    False  comment   \n",
       "4  News100-1026     0         4       on     False     True       on   \n",
       "\n",
       "   like_num    pos  tag  \n",
       "0     False  PROPN  NNP  \n",
       "1     False   VERB  VBZ  \n",
       "2     False    DET   DT  \n",
       "3     False   NOUN   NN  \n",
       "4     False    ADP   IN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import tokens_table\n",
    "\n",
    "toktbl = tokens_table(corp, sentences=True)\n",
    "toktbl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using subsetting, we can for example select the fourth sentence in the \"News100-2338\" document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:27:13.981851Z",
     "iopub.status.busy": "2022-02-02T08:27:13.981039Z",
     "iopub.status.idle": "2022-02-02T08:27:13.992518Z",
     "shell.execute_reply": "2022-02-02T08:27:13.991968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sent</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28191</th>\n",
       "      <td>News100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>The</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28192</th>\n",
       "      <td>News100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>episode</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>episode</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28193</th>\n",
       "      <td>News100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>started</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>start</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28194</th>\n",
       "      <td>News100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>off</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>off</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28195</th>\n",
       "      <td>News100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>with</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>with</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                doc  sent  position    token  is_punct  is_stop    lemma  \\\n",
       "28191  News100-2338     3       101      The     False     True      the   \n",
       "28192  News100-2338     3       102  episode     False    False  episode   \n",
       "28193  News100-2338     3       103  started     False    False    start   \n",
       "28194  News100-2338     3       104      off     False     True      off   \n",
       "28195  News100-2338     3       105     with     False     True     with   \n",
       "\n",
       "       like_num   pos  tag  \n",
       "28191     False   DET   DT  \n",
       "28192     False  NOUN   NN  \n",
       "28193     False  VERB  VBD  \n",
       "28194     False   ADP   RP  \n",
       "28195     False   ADP   IN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toktbl[(toktbl.doc == 'News100-2338') & (toktbl.sent == 3)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can do much more with text corpora in terms of accessing and transforming their contents. This is shown in great detail in the  [chapter on text preprocessing](preprocessing.ipynb).\n",
    "\n",
    "Next, we proceed with [working with text corpora](text_corpora.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
