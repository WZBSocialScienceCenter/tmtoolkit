{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "This is only quick overview for getting started. Corpus loading, text preprocessing, etc. are explained in depth in the respective chapters.\n",
    "\n",
    "## Loading a built-in text corpus\n",
    "\n",
    "Once you have installed tmtoolkit, you can start by loading a built-in dataset. Note that you must have installed tmtoolkit with the ``[recommended]`` or ``[textproc]`` option for this to work. See the [installation instructions](install.rst) for details.\n",
    "\n",
    "Let's import the [`builtin_corpora_info`](api.rst#TODO) function first and have a look which datasets are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de-parlspeech-v2-sample-bundestag',\n",
       " 'en-News100',\n",
       " 'en-NewsArticles',\n",
       " 'en-parlspeech-v2-sample-houseofcommons',\n",
       " 'es-parlspeech-v2-sample-congreso',\n",
       " 'nl-parlspeech-v2-sample-tweedekamer']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import builtin_corpora_info\n",
    "\n",
    "builtin_corpora_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load one of these corpora, a sample of 100 articles from the [News Articles dataset from Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GMFCTR). For this, we import the [`Corpus`](api.rst#TODO) class and use [`Corpus.from_builtin_corpus`](api.rst#TODO). The raw text data will then be processed by an [NLP pipeline](https://spacy.io/usage/spacy-101#pipelines) with [SpaCy](https://spacy.io). That is, it will be tokenized and analyzed for the grammatical structure of each sentence and the linguistic attributes of each token, among other things. Since this step is computationally intense, it takes quite some time for large text corpora (it can be sped up by enabling parallel processing as explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [100 documents  / language \"en\"]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import Corpus\n",
    "\n",
    "corp = Corpus.from_builtin_corpus('en-News100')\n",
    "corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look which documents were loaded (showing only the first ten document labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['News100-2338',\n",
       " 'News100-3228',\n",
       " 'News100-1253',\n",
       " 'News100-1615',\n",
       " 'News100-3334',\n",
       " 'News100-92',\n",
       " 'News100-869',\n",
       " 'News100-3092',\n",
       " 'News100-3088',\n",
       " 'News100-1173']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.doc_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing documents and document tokens\n",
    "\n",
    "We can now access each document in this corpus via its document label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document \"News100-2338\" (680 tokens, 9 token attributes, 2 document attributes)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp['News100-2338']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By accessing the corpus in this way, we get a [`Document`](api.rst#TODO) object. We can query a document for its contents again using the square brackets syntax. Here, we access its tokens and show only the first ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'This',\n",
       " 'Is',\n",
       " 'Us',\n",
       " \"'\",\n",
       " 'Makes',\n",
       " 'Surprising',\n",
       " 'Reveal',\n",
       " 'About',\n",
       " 'Jack']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp['News100-2338']['token'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, you won't need to access the `Document` objects of a corpus directly. You can rather use functions that provide a convenient interface to a corpus' contents, e.g. the [`doc_tokens`](api.rst#TODO) function which allows to retrieve all documents' tokens along with additional token attributes like Part-of-Speech (POS) tags, token lemma, etc.\n",
    "\n",
    "Let's first import [`doc_tokens`](api.rst#TODO) and then list the first ten tokens of the documents \"News100-2338\" and \"News100-3228\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.corpus import doc_tokens\n",
    "\n",
    "tokens = doc_tokens(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'This',\n",
       " 'Is',\n",
       " 'Us',\n",
       " \"'\",\n",
       " 'Makes',\n",
       " 'Surprising',\n",
       " 'Reveal',\n",
       " 'About',\n",
       " 'Jack']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['News100-2338'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neil',\n",
       " 'Gorsuch',\n",
       " 'facing',\n",
       " \"'\",\n",
       " 'rigorous',\n",
       " \"'\",\n",
       " 'confirmation',\n",
       " 'hearing',\n",
       " 'this',\n",
       " 'week']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['News100-3228'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve more information than just the tokens. Let's also get the POS tags via `with_attr='pos'` and enable structuring the results according to the sentences in the document via `sentences=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = doc_tokens(corp, sentences=True, with_attr='pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we now have a dictionary with two entries, \"token\" and \"pos\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token', 'pos'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['News100-2338'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within these dictionary entries, the tokens and the POS tags are contained inside a list of sentences. So for example to get the POS tags for each token in the fourth sentence (i.e. index 3), we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PART',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PART',\n",
       " 'VERB',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index 3 is the fourth sentence, since indices start with 0\n",
    "tokens['News100-2338']['pos'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could for example combine the tokens and their POS tags by using `zip`. Here we do that for the first five tokens in the fourth sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('episode', 'NOUN'),\n",
       " ('started', 'VERB'),\n",
       " ('off', 'ADP'),\n",
       " ('with', 'ADP')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens['News100-2338']['token'][3][:5],\n",
    "         tokens['News100-2338']['pos'][3][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an overview about the contents of a corpus, it's often more useful to get it in a tabular format. The tmtoolkit package provides a function to generate a [pandas DataFrame](https://pandas.pydata.org/) from a corpus, [`tokens_table`](api.rst#TODO).\n",
    "\n",
    "We'll use that now and instruct it to also return the sentence index of each token via `sentences=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sent</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Kremlin</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Kremlin</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>gives</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>give</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>comment</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News100-1026</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>on</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>on</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            doc  sent  position    token  is_punct  is_stop    lemma  \\\n",
       "0  News100-1026     0         0  Kremlin     False    False  Kremlin   \n",
       "1  News100-1026     0         1    gives     False    False     give   \n",
       "2  News100-1026     0         2       no     False     True       no   \n",
       "3  News100-1026     0         3  comment     False    False  comment   \n",
       "4  News100-1026     0         4       on     False     True       on   \n",
       "\n",
       "   like_num    pos  tag  \n",
       "0     False  PROPN  NNP  \n",
       "1     False   VERB  VBZ  \n",
       "2     False    DET   DT  \n",
       "3     False   NOUN   NN  \n",
       "4     False    ADP   IN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import tokens_table\n",
    "\n",
    "toktbl = tokens_table(corp, sentences=True)\n",
    "toktbl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using subsetting, we can for example select the fourth sentence in the \"News100-2338\" document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sent</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28191</th>\n",
       "      <td>NewsArticles-sample100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>The</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28192</th>\n",
       "      <td>NewsArticles-sample100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>episode</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>episode</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28193</th>\n",
       "      <td>NewsArticles-sample100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>started</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>start</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28194</th>\n",
       "      <td>NewsArticles-sample100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>off</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>off</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28195</th>\n",
       "      <td>NewsArticles-sample100-2338</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>with</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>with</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               doc  sent  position    token  is_punct  \\\n",
       "28191  NewsArticles-sample100-2338     3       101      The     False   \n",
       "28192  NewsArticles-sample100-2338     3       102  episode     False   \n",
       "28193  NewsArticles-sample100-2338     3       103  started     False   \n",
       "28194  NewsArticles-sample100-2338     3       104      off     False   \n",
       "28195  NewsArticles-sample100-2338     3       105     with     False   \n",
       "\n",
       "       is_stop    lemma  like_num   pos  tag  \n",
       "28191     True      the     False   DET   DT  \n",
       "28192    False  episode     False  NOUN   NN  \n",
       "28193    False    start     False  VERB  VBD  \n",
       "28194     True      off     False   ADP   RP  \n",
       "28195     True     with     False   ADP   IN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toktbl[(toktbl.doc == 'News100-2338') & (toktbl.sent == 3)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do much more with text corpora in terms of accessing and transforming their contents. This is shown in great detail in the  [chapter on text preprocessing](preprocessing.ipynb).\n",
    "\n",
    "Next, we proceed with [working with text corpora](text_corpora.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
