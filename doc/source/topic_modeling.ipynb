{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "The [topicmod module](api.rst#module-tmtoolkit.topicmod) offers a wide range of tools to facilitate [topic modeling](https://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext) with Python. This chapter will introduce the following techniques: \n",
    "\n",
    "- [parallel topic model computation for different copora and/or parameter sets](#Computing-topic-models-in-parallel)\n",
    "- [evaluation of topic models (including finding a good set of hyperparameters for the given dataset)](#Evaluation-of-topic-models)\n",
    "- [common statistics and tools for topic models](#Common-statistics-and-tools-for-topic-models)\n",
    "- [export of topic models and summaries to different file formats](#Displaying-and-exporting-topic-modeling-results)\n",
    "- [visualization of topic models](#Visualizing-topic-models)\n",
    "\n",
    "A quick note on terminology: So far, we spoke about *tokens* or sometimes *terms* when we meant the individual elements that our documents consist of after we applied text preprocessing such as *tokenization* to the raw input text strings. These tokens can be lexicographically correct words, but they don't have to, e.g. when you applied stemming you might have tokens like \"argu\" in your vocabulary. There may also be numbers or punctuation symbols in your vocabulary. For those topic modeling techniques that tmtoolkit supports, the results are always two probability distributions: a *document-topic distribution* and a *topic-word distribution*. Since the latter is called topic-*word* and not topic-*token* or *-term* distribution, we will also use the term *word* when we mean any token from the corpus' vocabulary.\n",
    "\n",
    "\n",
    "## An example document-term matrix\n",
    "\n",
    "tmtoolkit supports topic models that are computed from document-term matrices (DTMs). Just as in the previous chapter, we will at first generate a DTM. However, this time the sample will be bigger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(20191120)   # to make the sampling reproducible\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "from tmtoolkit.corpus import Corpus\n",
    "\n",
    "# for topic modeling, the document sizes shouldn't be\n",
    "# too different, hence we set a filter\n",
    "corpus = Corpus.from_builtin_corpus('en-NewsArticles') \\\n",
    "    .filter_by_min_length(1000) \\\n",
    "    .filter_by_max_length(10000) \\\n",
    "    .sample(100)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also now generate two DTMs, because we later want to show how you can compute topic models for two different DTMs in parallel. At first, we to some general preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.preprocess import TMPreproc\n",
    "\n",
    "preproc = TMPreproc(corpus, language='en')\n",
    "preproc.pos_tag() \\\n",
    "    .lemmatize() \\\n",
    "    .tokens_to_lowercase() \\\n",
    "    .remove_special_chars_in_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we at first apply more \"relaxed\" cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "preproc_bigger = preproc.copy() \\\n",
    "    .add_stopwords(['would', 'could', 'nt', 'mr', 'mrs', 'also']) \\\n",
    "    .clean_tokens(remove_shorter_than=2) \\\n",
    "    .remove_common_tokens(df_threshold=0.85) \\\n",
    "    .remove_uncommon_tokens(df_threshold=0.05)\n",
    "\n",
    "preproc_bigger.n_docs, preproc_bigger.vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another copy of `preproc` will apply more aggressive cleaning and hence will result in a smaller vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "preproc_smaller = preproc.copy() \\\n",
    "    .filter_for_pos('N') \\\n",
    "    .clean_tokens(remove_numbers=True, remove_shorter_than=2) \\\n",
    "    .remove_common_tokens(df_threshold=0.9) \\\n",
    "    .remove_uncommon_tokens(df_threshold=0.1)\n",
    "\n",
    "del preproc\n",
    "\n",
    "preproc_smaller.n_docs, preproc_smaller.vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the document labels, vocabulary arrays and DTMs for both versions now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# doc_labels are the same for both\n",
    "\n",
    "doc_labels = np.array(preproc_bigger.doc_labels)\n",
    "doc_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_bg = np.array(preproc_bigger.vocabulary)\n",
    "vocab_sm = np.array(preproc_smaller.vocabulary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dtm_bg = preproc_bigger.dtm\n",
    "dtm_sm = preproc_smaller.dtm\n",
    "\n",
    "preproc_bigger.shutdown_workers()\n",
    "preproc_smaller.shutdown_workers()\n",
    "del preproc_bigger, preproc_smaller  # don't need these any more\n",
    "\n",
    "dtm_bg, dtm_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two sparse DTMs `dtm_bg` (from the bigger preprocessed data) and `dtm_sm` (from the smaller preprocessed data), a list of document labels `doc_labels` that represent the rows of both DTMs and vocabulary arrays `vocab_bg` and `vocab_sm` that represent the columns of the respective DTMs. We will use this data for the remainder of the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing topic models in parallel\n",
    "\n",
    "tmtoolkit allows to compute topic models in parallel, making use of all processor cores in your machine. Parallelization can be done per input DTM, per hyperparameter set and as combination of both. Hyperparameters control the number of topics and their \"granularity\". We will later have a look at the role of hyperparameters and how to find an optimal combination for a given dataset with the means of topic model evaluation.\n",
    "\n",
    "For now, we will concentrate on computing the topic models for both of our two DTMs in parallel. tmtoolkit supports three very popular packages for topic modeling, which provide the work of actually computing the model from the input matrix. They can all be accessed in separate sub-modules of the [topicmod module](api.rst#module-tmtoolkit.topicmod):\n",
    "\n",
    "- [topicmod.tm_lda](api.rst#module-tmtoolkit.topicmod.tm_lda) provides an interface for the [lda](https://lda.readthedocs.io/en/latest/) package\n",
    "- [topicmod.tm_sklearn](api.rst#module-tmtoolkit.topicmod.tm_sklearn) provides an interface for the [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) package\n",
    "- [topicmod.tm_gensim](api.rst#module-tmtoolkit.topicmod.tm_gensim) provides an interface for the [Gensim](https://radimrehurek.com/gensim/) package\n",
    "\n",
    "Each of these sub-modules offer at least two functions that work with the respective package: `compute_models_parallel()` for general parallel model computation and `evaluate_topic_models()` for parallel model computation and evaluation (discussed later). For now, we want to compute two models in parallel with the [lda](https://lda.readthedocs.io/en/latest/) package and hence use [compute_models_parallel()](api.rst#tmtoolkit.topicmod.tm_lda.compute_models_parallel) from [topicmod.tm_lda](api.rst#module-tmtoolkit.topicmod.tm_lda).\n",
    "\n",
    "We need to provide two things for this function: First, the input matrices as a dict that maps labels to the respective DTMs. Second, hyperparameters to use for the model computations. Note that each topic modeling package has different hyperparameters and you should refer to their documentation in order to find out which hyperparameters you need to provide. For lda, we set the number of topics `n_topics` to 10 and the number of iterations for the Gibbs sampling process `n_iter` to 1000. We always want to use the same hyperparameters, so we pass these as `constant_parameters`. If we wanted to create models for a whole range of parameters, e.g. for different numbers of topics, we could provide `varying_parameters`. We will check this out later when we evaluate topic models.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "Note\n",
    "\n",
    "For proper topic modeling, we shouldn't just set the number of topics, but try to find it out via evaluation methods. We should also check if the algorithm converged using the provided likelihood estimations. We will do both later on, but now focus on `compute_models_parallel()`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "\n",
    "# suppress the \"INFO\" messages and warnings from lda\n",
    "logger = logging.getLogger('lda')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set data to use\n",
    "dtms = {\n",
    "    'bigger': dtm_bg,\n",
    "    'smaller': dtm_sm\n",
    "}\n",
    "\n",
    "# and fixed hyperparameters\n",
    "lda_params = {\n",
    "    'n_topics': 10,\n",
    "    'n_iter': 1000,\n",
    "    'random_state': 20191122  # to make results reproducible\n",
    "}\n",
    "\n",
    "models = compute_models_parallel(dtms, constant_parameters=lda_params)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, two models were created. These can be accessed via the labels that we used in the `dtms` dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "models['smaller']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for each input DTM, we get a list of 2-tuples. The first element in each tuple is a dict that represents the hyperparameters that were used to compute the model, the second element is the actual topic model (the `<lda.lda.LDA ...>` object). This structure looks a bit complex, but this is because it also supports varying parameters. Since we only have one fixed set of hyperparameters per DTM, we only have a list of length 1 for each DTM.\n",
    "\n",
    "We will now access the models and print the top words per topic by using [print_ldamodel_topic_words()](api.rst#tmtoolkit.topicmod.model_io.print_ldamodel_topic_words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "\n",
    "model_sm = models['smaller'][0][1]\n",
    "print_ldamodel_topic_words(model_sm.topic_word_, vocab_sm, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_bg = models['bigger'][0][1]\n",
    "print_ldamodel_topic_words(model_bg.topic_word_, vocab_bg, top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also generate models from different parameters in parallel, either for a single DTM or several. In the following example we generate models for a series of four different values for the `alpha` parameter. The parameters `n_iter` and `n_topics` are held constant across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "var_params = [{'alpha': 1/(10**x)} for x in range(1, 5)]\n",
    "\n",
    "const_params = {\n",
    "    'n_iter': 500,\n",
    "    'n_topics': 10,\n",
    "    'random_state': 20191122  # to make results reproducible\n",
    "}\n",
    "\n",
    "models = compute_models_parallel(dtm_sm,  # smaller DTM\n",
    "                                 varying_parameters=var_params,\n",
    "                                 constant_parameters=const_params)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could compare these models now, e.g. by investigating their topics.\n",
    "\n",
    "A more systematic approach on comparing and evaluating topic models, also in order to find a good set of hyperparameters for a given dataset, will be presented in the next section.\n",
    "\n",
    "## Evaluation of topic models\n",
    "\n",
    "tmtoolkit provides several metrics for comparing and evaluating topic models. This can be used for finding a good hyperparameter set for a given dataset, e.g. a good combination of the number of topics and concentration paramaters (often called alpha and beta in literature). For some background on hyperparameters in topic modeling, see [this blog post](https://datascience.blog.wzb.eu/2017/11/09/topic-modeling-evaluation-in-python-with-tmtoolkit/).\n",
    "\n",
    "For each candidate hyperparameter set, a model can be generated and evaluated in parallel. We will do this now for the \"big\" DTM `dtm_bg`. Our candidate values for the number of topics `k` range between 20 and 120, with steps of 10. We make the concentration parameter for a prior over the document-specific topic distributions, alpha, depending on `k` as `1/k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "var_params = [{'n_topics': k, 'alpha': 1/k} for k in range(20, 121, 10)]\n",
    "var_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart of the model evaluation process is the function [evaluate_topic_models()](api.rst#tmtoolkit.topicmod.tm_lda.evaluate_topic_models), which is available for all three topic modeling packages. We stick with lda and import that function from [topicmod.tm_lda](api.rst#module-tmtoolkit.topicmod.tm_lda). It is similar to [compute_models_parallel()](api.rst#tmtoolkit.topicmod.tm_lda.compute_models_parallel) as it accepts varying and constant hyperparameters. However, it doesn't only compute the models in parallel, but also applies several metrics to these models in order to evaluate them. This can be controlled with the `metric` parameter that accepts a string or a list of strings that specify the used metric(s). These metrics refer to functions that are implemented in [topicmod.evaluate](api.rst#module-tmtoolkit.topicmod.evaluate).\n",
    "\n",
    "Each topic modeling sub-module defines two important sequences: `AVAILABLE_METRICS` and `DEFAULT_METRICS`. The former lists all available metrics for that sub-module, the latter lists the default metrics that are used when you don't specify anything with the `metric` parameter. Let's have a look at both sequences in [topicmod.tm_lda](api.rst#module-tmtoolkit.topicmod.tm_lda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod import tm_lda\n",
    "\n",
    "tm_lda.AVAILABLE_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tm_lda.DEFAULT_METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details about the metrics and the academic references, see the respective implementations in the [topicmod.evaluate](api.rst#module-tmtoolkit.topicmod.evaluate) module.\n",
    "\n",
    "We will now run the model evaluations with [evaluate_topic_models()](api.rst#tmtoolkit.topicmod.tm_lda.evaluate_topic_models) using our previously generated list of varying hyperparameters `var_params`, some constant hyperparameters and the default set of metrics. We also set `return_models=True` which means to retain the generated models in the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.tm_lda import evaluate_topic_models\n",
    "from tmtoolkit.topicmod.evaluate import results_by_parameter\n",
    "\n",
    "const_params = {\n",
    "    'n_iter': 1000,\n",
    "    'eta': 0.1,       # \"eta\" aka \"beta\"\n",
    "    'random_state': 20191122  # to make results reproducible\n",
    "}\n",
    "\n",
    "eval_results = evaluate_topic_models(dtm_bg,\n",
    "                                     varying_parameters=var_params,\n",
    "                                     constant_parameters=const_params,\n",
    "                                     return_models=True)\n",
    "eval_results[:3]  # only show first three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results are a list with pairs of hyperparameters and their evaluation results for each metric. Additionally, there is the generated model for each hyperparameter set.\n",
    "\n",
    "We now use [results_by_parameter()](api.rst#tmtoolkit.topicmod.evaluate.results_by_parameter), which takes the \"raw\" evaluation results and sorts them by a specific hyperparameter, in this case `n_topics`. This is important because this is the way that the function for visualizing evaluation results, [plot_eval_results()](api.rst#tmtoolkit.topicmod.visualize.plot_eval_results), expects the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "eval_results_by_topics = results_by_parameter(eval_results, 'n_topics')\n",
    "eval_results_by_topics[:3]  # again only the first three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the results for each metric across the specified range of number of topics using [plot_eval_results()](api.rst#tmtoolkit.topicmod.visualize.plot_eval_results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.visualize import plot_eval_results\n",
    "\n",
    "plot_eval_results(eval_results_by_topics);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest to set the number of topics, `n_topics`, to 50 and alpha to `0.02`. We don't have to generate a model with these hyperparameters again, because it's already in the evaluation results (thanks to `return_models=True`). We extract the model from there in order to use it in the rest of the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "best_tm = [m for k, m in eval_results_by_topics if k == 50][0]['model']\n",
    "best_tm.n_topics, best_tm.alpha, best_tm.eta  # just to make sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common statistics and tools for topic models\n",
    "\n",
    "The [topicmod.model_stats](api.rst#module-tmtoolkit.topicmod.model_stats) module mostly contains functions that compute statistics from the document-topic and topic-word distribution of a topic model and also some helper functions for working with such distributions. We'll start with an important helper function, [generate_topic_labels_from_top_words()](api.rst#tmtoolkit.topicmod.model_stats.generate_topic_labels_from_top_words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating labels for topics\n",
    "\n",
    "In topic modeling, topics are numbered because they're *abstract* – they're simply a probability distribution across all words in the vocabulary. Still, it's useful to give them labels for better identification. The function [generate_topic_labels_from_top_words()](api.rst#tmtoolkit.topicmod.model_stats.generate_topic_labels_from_top_words) is very useful for that, as it finds labels according to the most \"relevant\" words in each topic. We'll later see how we can identify the most relevant words per topic using a special [relevance statistic](#Topic-word-relevance). Note that you can adjust the weight of the relevance measure for the ranking by using the parameter `lambda_` which is in range $[0, 1]$.\n",
    "\n",
    "The function requires at least the topic-word and document-topic distributions from the model, the document lengths and the vocabulary. It then finds the minimum number of relevant words that uniquely label each topic. You can also use a fixed number for that minimum number with the parameter `n_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.bow.bow_stats import doc_lengths\n",
    "from tmtoolkit.topicmod.model_stats import generate_topic_labels_from_top_words\n",
    "\n",
    "doc_lengths_bg = doc_lengths(dtm_bg)\n",
    "topic_labels = generate_topic_labels_from_top_words(\n",
    "    best_tm.topic_word_,\n",
    "    best_tm.doc_topic_,\n",
    "    doc_lengths_bg,\n",
    "    vocab_bg,\n",
    "    lambda_=0.6\n",
    ")\n",
    "\n",
    "topic_labels[:10]   # showing only the first 5 topics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, two words are necessary to label each topic uniquely. By default, each label is prefixed with a number. You can change that with the parameter `labels_format`.\n",
    "\n",
    "Let's have a look at the top words for a specific topic. We can use [ldamodel_top_topic_words()](api.rst#tmtoolkit.topicmod.model_io.ldamodel_top_topic_words) for that from the module [topicmod.model_io](api.rst#module-tmtoolkit.topicmod.model_io), which we will have a closer look at [later](#Displaying-and-exporting-topic-modeling-results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import ldamodel_top_topic_words\n",
    "\n",
    "top_topic_word = ldamodel_top_topic_words(best_tm.topic_word_,\n",
    "                                          vocab_bg,\n",
    "                                          row_labels=topic_labels)\n",
    "top_topic_word[top_topic_word.index == '1_care_cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal topic and word distributions\n",
    "\n",
    "We'll now focus on the marginal topic and word distributions. Let's get the marginal topic distribution first by using [marginal_topic_distrib()](api.rst#tmtoolkit.topicmod.model_stats.marginal_topic_distrib):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import marginal_topic_distrib\n",
    "\n",
    "marg_topic = marginal_topic_distrib(best_tm.doc_topic_, doc_lengths_bg)\n",
    "marg_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal topic distribution can be interpreted as the \"importance\" of each topic for the whole corpus. Let's get the sorted indices into `topic_labels` with `np.argsort()` and get the top five topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# np.argsort() gives ascending order, hence reverse via [::-1]\n",
    "topic_labels[np.argsort(marg_topic)[::-1][:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can get the marginal word distribution with [marginal_word_distrib()](api.rst#tmtoolkit.topicmod.model_stats.marginal_word_distrib) from the model's topic-word distribution and the marginal topic distribution. We'll use this to list the most probable words for the corpus. As expected, these are mostly quite common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import marginal_word_distrib\n",
    "\n",
    "marg_word = marginal_word_distrib(best_tm.topic_word_, marg_topic)\n",
    "vocab_bg[np.argsort(marg_word)[::-1][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two helper functions exist for this purpose: [most_probable_words()](api.rst#tmtoolkit.topicmod.model_stats.most_probable_words) and [least_probable_words()](api.rst#tmtoolkit.topicmod.model_stats.least_probable_words) sort the vocabulary according to the marginal probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import most_probable_words, least_probable_words\n",
    "\n",
    "most_probable_words(vocab_bg, best_tm.topic_word_,\n",
    "                    best_tm.doc_topic_, doc_lengths_bg,\n",
    "                    n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "least_probable_words(vocab_bg, best_tm.topic_word_,\n",
    "                     best_tm.doc_topic_, doc_lengths_bg,\n",
    "                     n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word distinctiveness and saliency\n",
    "\n",
    "Word *distinctiveness* and *saliency* (see below) help to identify the most \"informative\" words in a corpus given its topic model. Both measures are introduced in [Chuang et al. 2012](https://dl.acm.org/citation.cfm?id=2254572).\n",
    "\n",
    "Word distinctiveness is calculated for each word $w$ as\n",
    "\n",
    "$\\text{distinctiveness}(w) = \\sum_T(P(T|w) \\log \\frac{P(T|w)}{P(T)})$.\n",
    "\n",
    "where $P(T)$ is the marginal topic distribution and $P(T|w)$ is the probability of a topic given a word $w$.\n",
    "\n",
    "We can calculate this measure using [word_distinctiveness()](api.rst#tmtoolkit.topicmod.model_stats.word_distinctiveness). To use this measure directly to rank words, we can use [most_distinct_words()](api.rst#tmtoolkit.topicmod.model_stats.most_distinct_words) and [least_distinct_words()](api.rst#tmtoolkit.topicmod.model_stats.least_distinct_words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import word_distinctiveness, \\\n",
    "    most_distinct_words, least_distinct_words\n",
    "\n",
    "word_distinct = word_distinctiveness(best_tm.topic_word_, marg_topic)\n",
    "word_distinct[:10]   # first 10 words in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "most_distinct_words(vocab_bg, best_tm.topic_word_,\n",
    "                    best_tm.doc_topic_, doc_lengths_bg,\n",
    "                    n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "least_distinct_words(vocab_bg, best_tm.topic_word_,\n",
    "                     best_tm.doc_topic_, doc_lengths_bg,\n",
    "                     n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word *saliency* weights each words' distinctiveness by it's marginal probability $P(w)$:\n",
    "\n",
    "$\\text{saliency}(w) = P(w) \\cdot \\text{distinctiveness}(w)$.\n",
    "\n",
    "The respective functions in tmtoolkit are [word_saliency()](api.rst#tmtoolkit.topicmod.model_stats.word_saliency), [most_salient_words()](api.rst#tmtoolkit.topicmod.model_stats.most_salient_words) and [least_salient_words()](api.rst#tmtoolkit.topicmod.model_stats.least_salient_words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import word_saliency, \\\n",
    "    most_salient_words, least_salient_words\n",
    "\n",
    "word_sal = word_saliency(best_tm.topic_word_, best_tm.doc_topic_, doc_lengths_bg)\n",
    "word_sal[:10]   # first 10 words in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "most_salient_words(vocab_bg, best_tm.topic_word_,\n",
    "                   best_tm.doc_topic_, doc_lengths_bg,\n",
    "                   n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "least_salient_words(vocab_bg, best_tm.topic_word_,\n",
    "                    best_tm.doc_topic_, doc_lengths_bg,\n",
    "                    n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-word relevance\n",
    "\n",
    "The topic-word relevance measure as introduced by [Sievert and Shirley 2014](https://www.aclweb.org/anthology/W14-3110/) helps to identify the most relevant words within a topic by also accounting for the marginal probability of each word across the corpus. This is done by integrating a *lift* value, which is the \"ratio of a term's probability within a topic to its marginal probability across the corpus.\" (ibid.)\n",
    "\n",
    "Thus for each word $w$, given a topic-word distribution $\\phi$, a topic $t$ and a weight parameter $\\lambda$, it is calculated as:\n",
    "\n",
    "$\\text{relevance}_{\\phi, \\lambda}(w, t) = \\lambda \\log \\phi_{t,w} + (1-\\lambda) \\log \\frac{\\phi_{t,w}}{p(w)}$.\n",
    "\n",
    "The first term $\\log \\phi_{t,w}$ is the log of the topic-word distribution, the second term $\\log \\frac{\\phi_{t,w}}{p(w)}$ is the *log lift* and $\\lambda$ can be used to control the weight between both terms. The lower $\\lambda$, the more weight is put on the lift term, i.e. the more different are the results from the original topic-word distribution.\n",
    "\n",
    "This measure is implemented in [topic_word_relevance()](api.rst#tmtoolkit.topicmod.model_stats.topic_word_relevance). It returns a matrix of the same shape as the topic-word distribution, i.e. each row represents a topic with a (log-transformed) distribution across all words in the vocabulary. Please note that the lambda parameter ends with an underscore: `lambda_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import topic_word_relevance\n",
    "\n",
    "topic_word_rel = topic_word_relevance(best_tm.topic_word_, best_tm.doc_topic_,\n",
    "                                      doc_lengths_bg, lambda_=0.6)\n",
    "topic_word_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that it's 50 topics across all words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "topic_word_rel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions can be used to get the most or least relevant words for a topic: [most_relevant_words_for_topic()](api.rst#tmtoolkit.topicmod.model_stats.most_relevant_words_for_topic) and [least_relevant_words_for_topic()](api.rst#tmtoolkit.topicmod.model_stats.least_relevant_words_for_topic). You can select the topic with the `topic` parameter which is a **zero-based topic index**.\n",
    "\n",
    "We'll do it for topic with index 0, which is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "topic_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import most_relevant_words_for_topic, \\\n",
    "    least_relevant_words_for_topic\n",
    "\n",
    "most_relevant_words_for_topic(vocab_bg, topic_word_rel, topic=0, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "least_relevant_words_for_topic(vocab_bg, topic_word_rel, topic=0, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic coherence\n",
    "\n",
    "We already used the *coherence* metric ([Mimno et al. 2011](https://dl.acm.org/citation.cfm?id=2145462)) for topic model evaluation. However, this metric cannot only be used to assess the overall quality of a topic model, but also to evaluate the individual topics' coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.evaluate import metric_coherence_mimno_2011\n",
    "\n",
    "# use top 20 words per topic for metric\n",
    "coh = metric_coherence_mimno_2011(best_tm.topic_word_, dtm_bg, top_n=20)\n",
    "coh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a coherence value for each topic. Let's show the distribution of these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(coh, bins=20)\n",
    "plt.xlabel('coherence')\n",
    "plt.ylabel('n')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the best and worst topics according to this metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "top10_t_indices = np.argsort(coh)[::-1][:5]\n",
    "bottom10_t_indices = np.argsort(coh)[:5]\n",
    "\n",
    "topic_labels[top10_t_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels[bottom10_t_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this metric also doesn't spare oneself careful manual evaluation, because it can also be off for some topics. For example, topic `41_go_one` is certainly not a coherent topic as it mostly ranks very common but but less meaningful words high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_word[top_topic_word.index == '41_go_one']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More coherence metrics can be used with the function [metric_coherence_gensim()](api.rst#tmtoolkit.topicmod.evaluate.metric_coherence_gensim). This requires that [gensim](https://radimrehurek.com/gensim/) is installed. Furthemore, most metrics require that a parameter `texts` is passed which is the tokenized text that was used to create the document-term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering topics\n",
    "\n",
    "With the function [filter_topics()](api.rst#tmtoolkit.topicmod.model_stats.filter_topics), you can filter the topics according to their topic-word distribution and the following search criteria:\n",
    "\n",
    "- `search_pattern`: one or more search patterns according to the [common parameters for pattern matching](preprocessing.ipynb#Common-parameters-for-pattern-matching-functions)\n",
    "- `top_n`: pattern match(es) must occur in the first `top_n` most probable words in the topic\n",
    "- `thresh`: matched words' probability must be above this threshold\n",
    "\n",
    "You must specify at least one of `top_n` and `thresh`, but you can also specify both. The function returns an array of topic indices (which start with zero!).\n",
    "\n",
    "Let's find all topics that have the glob pattern (`match_type='glob'`) \"chin*\" (to match both \"china\" and \"chinese\") in the top 5 most probable words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import filter_topics\n",
    "\n",
    "found_topics = filter_topics('chin*', vocab_bg,\n",
    "                             best_tm.topic_word_, match_type='glob',\n",
    "                             top_n=5)\n",
    "found_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these indices with our `topic_labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "topic_labels[found_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to select all topics where *any* of the words matched by the glob patterns `\"chin*\"` *or* `\"business\"` achieve at least a probability of 0.01 (`thresh=0.01`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "found_topics = filter_topics(['chin*', 'business'], vocab_bg,\n",
    "                             best_tm.topic_word_, thresh=0.01, match_type='glob')\n",
    "topic_labels[found_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we specify `cond='all'`, *all* patterns must have at least one match (here in the top 10 list of words per topic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "found_topics = filter_topics(['chin*', 'business'], vocab_bg,\n",
    "                             best_tm.topic_word_, top_n=10, match_type='glob',\n",
    "                             cond='all')\n",
    "topic_labels[found_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also pass a topic-word relevance matrix instead of a topic-word probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "found_topics = filter_topics('chin*', vocab_bg,\n",
    "                             topic_word_rel, match_type='glob',\n",
    "                             top_n=5)\n",
    "topic_labels[found_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding topics\n",
    "\n",
    "It is often the case that some topics of a topic model rank a lot of uninformative (e.g. very common) words the highest. This results in some uninformative topics, which you may want to exclude from further analysis. Note that if a large fraction of topics seems uninformative, it points to a problem with your topic model and/or your preprocessing steps. You should [evaluate](#Evaluation-of-topic-models) your candidate models carefully with the mentioned metrics and/or adjust your text preprocessing pipeline.\n",
    "\n",
    "The function [exclude_topics()](api.rst#tmtoolkit.topicmod.model_stats.exclude_topics) allows to remove a specified set of topics from the document-topic and topic-word distributions. You need to pass the **zero-based** indices of the topics that you want to remove, and both distributions.\n",
    "\n",
    "In this example, I identified the following topics as uninformative (by looking at the top ranked words either by topic-word distribution or topic-word relevance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "uninform_topics = [9, 22, 32, 40, 48]\n",
    "topic_labels[uninform_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass these indices to [exclude_topics()](api.rst#tmtoolkit.topicmod.model_stats.exclude_topics) along with the topic model distributions. We'll get back new, filtered, distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_stats import exclude_topics\n",
    "\n",
    "new_doc_topic, new_topic_word, new_topic_mapping = \\\n",
    "    exclude_topics(uninform_topics, best_tm.doc_topic_,\n",
    "                best_tm.topic_word_, return_new_topic_mapping=True)\n",
    "new_doc_topic.shape, new_topic_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the new distributions' shapes that we now have 45 instead of 50 topics, because we removed five of them. We shouldn't forget to also update the topic labels and remove the unwanted topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "new_topic_labels = np.delete(topic_labels, uninform_topics)\n",
    "new_topic_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying and exporting topic modeling results\n",
    "\n",
    "The [topicmod.model_io](api.rst#module-tmtoolkit.topicmod.model_io) module provides several functions for displaying and exporting topic modeling results, i.e. results derived from the document-topic and topic-word distribution of a given topic model.\n",
    "\n",
    "We already used [ldamodel_top_topic_words()](api.rst#tmtoolkit.topicmod.model_io.ldamodel_top_topic_words) briefly, which generates a dataframe with the top words from a topic-word distribution. You can also use the topic-word relevance matrix instead. With `top_n` we can control the number of top words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# using relevance matrix here and showing only the first 3 topics\n",
    "ldamodel_top_topic_words(topic_word_rel, vocab_bg, top_n=5)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in the top *topics for each word/token*, you can use [ldamodel_top_word_topics()](api.rst#tmtoolkit.topicmod.model_io.ldamodel_top_word_topics). Here, we generate the top five topics for each token in the vocabulary, but only display the output for four specific words. Instead of the generic `\"topic_...\"` topic names, we additionally pass our previously generated topic labels `topic_labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import ldamodel_top_word_topics\n",
    "\n",
    "top_word_topics = ldamodel_top_word_topics(topic_word_rel, vocab_bg,\n",
    "                                           top_n=5, topic_labels=topic_labels)\n",
    "top_word_topics[top_word_topics.index.isin(['china', 'chinese', 'russia', 'russian'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values in parantheses are the corresponding values from the matrix for that word in that topic. They're negative because of the log transformation that is applied in the topic-word relevance measure.\n",
    "\n",
    "Similar functions can be used for the document-topic distribution: [ldamodel_top_doc_topics()](api.rst#tmtoolkit.topicmod.model_io.ldamodel_top_doc_topics) gives the top topics per document and [ldamodel_top_topic_docs()](api.rst#tmtoolkit.topicmod.model_io.ldamodel_top_topic_docs) gives the top documents per topic. Here, `top_n` controls the number of top-ranked topics or documents to return, respectively. This time, we use the filtered document-topic distribution `new_doc_topics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import ldamodel_top_doc_topics\n",
    "\n",
    "ldamodel_top_doc_topics(new_doc_topic, doc_labels, top_n=3,\n",
    "                        topic_labels=new_topic_labels)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the top documents per topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import ldamodel_top_topic_docs\n",
    "\n",
    "ldamodel_top_topic_docs(new_doc_topic, doc_labels, top_n=3,\n",
    "                        topic_labels=new_topic_labels)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also two functions that generate datatables for the full topic-word and document-topic distributions: [ldamodel_full_topic_words()](api.rst#tmtoolkit.topicmod.model_io.ldamodel_full_topic_words) and [ldamodel_full_doc_topics()](api.rst#tmtoolkit.topicmod.model_io.ldamodel_full_doc_topics). The output of both functions is naturally quite big, as long as you're not working with a \"toy dataset\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import ldamodel_full_topic_words\n",
    "\n",
    "datatable_topic_word = ldamodel_full_topic_words(new_topic_word,\n",
    "                                                 vocab_bg,\n",
    "                                                 row_labels=new_topic_labels)\n",
    "# displaying only the first 5 topics with the first\n",
    "# 10 words from the vocabulary (which are all numbers)\n",
    "datatable_topic_word[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import ldamodel_full_doc_topics\n",
    "\n",
    "datatable_doc_topic = ldamodel_full_doc_topics(new_doc_topic,\n",
    "                                               doc_labels,\n",
    "                                               topic_labels=new_topic_labels)\n",
    "# displaying only the first 3 documents with the first\n",
    "# 5 topics\n",
    "datatable_doc_topic[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quick inspection of topics there's also a pair of print functions. We already used [print_ldamodel_topic_words()](api.rst#tmtoolkit.topicmod.model_io.print_ldamodel_topic_words), but we haven't tried [print_ldamodel_doc_topics()](api.rst#tmtoolkit.topicmod.model_io.print_ldamodel_doc_topics) yet. This prints the `top_n` most probable topics for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import print_ldamodel_doc_topics\n",
    "\n",
    "# subsetting new_doc_topic and doc_labels to get only the first\n",
    "# five documents\n",
    "print_ldamodel_doc_topics(new_doc_topic[:5, :], doc_labels[:5],\n",
    "                          val_labels=new_topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also export the results of a topic model to an Excel file using [save_ldamodel_summary_to_excel()](api.rst#tmtoolkit.topicmod.model_io.save_ldamodel_summary_to_excel). The resulting Excel file will contain the following sheets:\n",
    "\n",
    "- `top_doc_topics_vals`: document-topic distribution with probabilities of top topics per document\n",
    "- `top_doc_topics_labels`: document-topic distribution with labels of top topics per document\n",
    "- `top_doc_topics_labelled_vals`: document-topic distribution combining probabilities and labels of top topics per\n",
    "  document (e.g. `\"topic_12 (0.21)\"`)\n",
    "- `top_topic_word_vals`: topic-word distribution with probabilities of top words per topic\n",
    "- `top_topic_word_labels`: topic-word distribution with top words per (e.g. `\"politics\"`) topic\n",
    "- `top_topic_words_labelled_vals`: topic-word distribution combining probabilities and top words per topic\n",
    "  (e.g. `\"politics (0.08)\"`)\n",
    "- optional if `dtm` is given – `marginal_topic_distrib`: marginal topic distribution\n",
    "\n",
    "Additionally to saving the output to the specified Excel file, the function will also return a dict with the sheets and their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import save_ldamodel_summary_to_excel\n",
    "\n",
    "sheets = save_ldamodel_summary_to_excel('data/news_articles_100.xlsx',\n",
    "                                        new_topic_word, new_doc_topic,\n",
    "                                        doc_labels, vocab_bg,\n",
    "                                        dtm = dtm_bg,\n",
    "                                        topic_labels = new_topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly store a topic model to disk for sharing or loading again at a later point in time, there are [save_ldamodel_to_pickle()](api.rst#tmtoolkit.topicmod.model_io.save_ldamodel_to_pickle) and [load_ldamodel_from_pickle()](api.rst#tmtoolkit.topicmod.model_io.load_ldamodel_from_pickle). The function for saving takes a path to a pickle file to create (or update), a topic model object (such as an LDA instance as `best_tm`, but you could also pass a tuple like `(new_doc_topic, new_topic_word)`), the corresponding vocabulary and document labels, and optionally the DTM that was used to create the topic model. The function for loading the data will return the saved data as a dict. We will only show the dict's keys here, as the data itself is too large to be printed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.model_io import save_ldamodel_to_pickle, \\\n",
    "    load_ldamodel_from_pickle\n",
    "\n",
    "save_ldamodel_to_pickle('data/news_articles_100.pickle',\n",
    "                        best_tm, vocab_bg, doc_labels,\n",
    "                        dtm = dtm_bg)\n",
    "\n",
    "loaded = load_ldamodel_from_pickle('data/news_articles_100.pickle')\n",
    "loaded.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing topic models\n",
    "\n",
    "The [topicmod.visualize](api.rst#visualize-topic-models-and-topic-model-evaluation-results) module contains several functions to visualize topic models and evaluation results. We've already used [plot_eval_results()](api.rst#tmtoolkit.topicmod.visualize.plot_eval_results) during [topic model evaluation](#Evaluation-of-topic-models) so we'll now focus on visualizing topic models.\n",
    "\n",
    "### Heatmaps\n",
    "\n",
    "Let's start with heatmap visualizations of document-topic or topic-word distributions from our topic model. This can be done with [plot_doc_topic_heatmap()](api.rst#tmtoolkit.topicmod.visualize.plot_doc_topic_heatmap) and [plot_topic_word_heatmap()](api.rst#tmtoolkit.topicmod.visualize.plot_topic_word_heatmap) respectively. Both functions draw on a [matplotlib](https://matplotlib.org/) figure and *Axes* object, which you must create before using these functions.\n",
    "\n",
    "Heatmap visualizations essentially shade cells in a 2D matrix (like the document-topic or topic-word distributions) according to their value, i.e. the respective probability for a topic in a given document or a word in a given topic. Since these matrices are usually quite large, i.e. with hundreds of rows and/or columns, it doesn't make sense to plot a heatmap of the whole matrix, but rather a certain subset of interest. When we want to visualize a document-topic distribution, we can optionally select a subset of the documents with the `which_documents` parameter and a subset of the topics with the `which_topics` parameter. Let's draw a heatmap of a subset of documents across all topics at first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tmtoolkit.topicmod.visualize import plot_doc_topic_heatmap\n",
    "\n",
    "# create a figure of certain size and\n",
    "# Axes object to draw on\n",
    "fig, ax = plt.subplots(figsize=(32, 8))\n",
    "\n",
    "which_docs = [\n",
    "    'NewsArticles-1473',\n",
    "    'NewsArticles-1646',\n",
    "    'NewsArticles-2252',\n",
    "    'NewsArticles-2473',\n",
    "    'NewsArticles-2583',\n",
    "    'NewsArticles-2765',\n",
    "    'NewsArticles-2922',\n",
    "    'NewsArticles-3396',\n",
    "    'NewsArticles-3601',\n",
    "    'NewsArticles-3753'\n",
    "]\n",
    "\n",
    "plot_doc_topic_heatmap(fig, ax, new_doc_topic, doc_labels,\n",
    "                       topic_labels=new_topic_labels,\n",
    "                       which_documents=which_docs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "\n",
    "which_topics = [\n",
    "    '30_air_commission',\n",
    "    '34_recall_vehicle',\n",
    "    '37_referendum_vote',\n",
    "    '42_eu_uk',\n",
    "    '44_growth_year'\n",
    "]\n",
    "\n",
    "plot_doc_topic_heatmap(fig, ax, new_doc_topic, doc_labels,\n",
    "                       topic_labels=new_topic_labels,\n",
    "                       which_documents=which_docs,\n",
    "                       which_topics=which_topics);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similarily, we can work with [plot_topic_word_heatmap()](api.rst#tmtoolkit.topicmod.visualize.plot_topic_word_heatmap) to visualize a topic-word distribution. We can also select a subset of topics and words from the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.visualize import plot_topic_word_heatmap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "which_words = ['may', 'european', 'referendum', 'brexit',\n",
    "               'eu', 'uk', 'britain', 'company', 'trade', 'growth']\n",
    "\n",
    "plot_topic_word_heatmap(fig, ax, new_topic_word, vocab_bg,\n",
    "                        topic_labels=new_topic_labels,\n",
    "                        which_topics=which_topics,\n",
    "                        which_words=which_words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there's also a generic heatmap plotting function [plot_heatmap()](api.rst#plot-heatmaps-for-topic-models) for any kind of 2D matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word clouds\n",
    "\n",
    "Thanks to the [wordlcloud package](https://pypi.org/project/wordcloud/), topic-word and document-topic distributions can also be visualized as \"word clouds\" with tmtoolkit. The function [generate_wordclouds_for_topic_words()](api.rst#tmtoolkit.topicmod.visualize.generate_wordclouds_for_topic_words) generates a word cloud for each topic by scaling a topic's word by its probability (weight). You can choose to display only the top `top_n` words per topic. The result of this function will be a dictionary mapping topic labels to the respective word cloud image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.visualize import generate_wordclouds_for_topic_words\n",
    "\n",
    "# some options for wordcloud output\n",
    "img_w = 400   # image width\n",
    "img_h = 300   # image height\n",
    "\n",
    "topic_clouds = generate_wordclouds_for_topic_words(\n",
    "    new_topic_word, vocab_bg,\n",
    "    top_n=20, topic_labels=new_topic_labels,\n",
    "    width=img_w, height=img_h\n",
    ")\n",
    "\n",
    "# show all generated word clouds\n",
    "topic_clouds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select specific topics and display their word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_clouds['42_eu_uk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_clouds['37_referendum_vote']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done for the document-topic distribution using [generate_wordclouds_for_document_topics()](api.rst#tmtoolkit.topicmod.visualize.generate_wordclouds_for_document_topics). Here, a word cloud for each document will be generated that contains the `top_n` most probable topics for this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.visualize import generate_wordclouds_for_document_topics\n",
    "\n",
    "doc_clouds = generate_wordclouds_for_document_topics(\n",
    "    new_doc_topic, doc_labels, topic_labels=new_topic_labels,\n",
    "    top_n=5, width=img_w, height=img_h)\n",
    "\n",
    "# show only the first 5 documents for\n",
    "# which word clouds were generated\n",
    "list(doc_clouds.keys())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display a specific document's topic word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clouds['NewsArticles-1099']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the generated images as PNG files to a folder on disk. Here, we store all word clouds in `topic_clouds` to `'data/tm_wordclouds/'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.visualize import write_wordclouds_to_folder\n",
    "\n",
    "write_wordclouds_to_folder(topic_clouds, 'data/tm_wordclouds/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive visualization with pyLDAVis\n",
    "\n",
    "The [pyLDAVis package](https://pyldavis.readthedocs.io/) offers a great interactive tool to explore a topic model. The tmtoolkit function [parameters_for_ldavis()](api.rst#tmtoolkit.topicmod.visualize.generate_wordclouds_for_document_topics) allows to prepare your topic model data for this package so that you can easily pass it on to pyLDAVis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.visualize import parameters_for_ldavis\n",
    "\n",
    "ldavis_params = parameters_for_ldavis(new_topic_word,\n",
    "                                      new_doc_topic,\n",
    "                                      dtm_bg,\n",
    "                                      vocab_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have installed the package, you can now start the LDAVis explorer with the following lines of code in a Jupyter notebook:\n",
    "\n",
    "    import pyLDAvis\n",
    "    pyLDAVis.prepare(**ldavis_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
