{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "During text preprocessing, a corpus of documents is tokenized (i.e. the document strings are split into individual words, punctuation, numbers, etc.) and then these tokens can be transformed, filtered or annotated. The goal is to prepare the raw texts in a way that makes it easier to perform eventual analysis methods in a later stage, e.g. by reducing noise in the dataset. tmtoolkit provides a rich set of tools for this purpose in the [tmtoolkit.preprocess](api.rst#tmtoolkit-preprocess) module.   \n",
    "\n",
    "## Two approaches: functional API and `TMPreproc` class\n",
    "\n",
    "There are two ways to apply text preprocessing methods to your documents: First, there is the [functional API](api.rst#module-tmtoolkit.preprocess) which consists of a set of Python functions that accept a list of (tokenized) documents. An example might be:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Hello world!\",    # document 1\n",
    "    \"Another example\"  # document 2\n",
    "]\n",
    "\n",
    "docs = tokenize(corpus)\n",
    "to_lowercase(docs)\n",
    "# Out: [['hello', 'world', '!'],\n",
    "#       ['another', 'example']]\n",
    "```\n",
    "\n",
    "\n",
    "The advantage of this approach is that it's very straight-forward and flexible. However, you must manage any meta data associated with the documents on your own (e.g. document labels or token metadata). Furthermore, the processing is not done in parallel.\n",
    "\n",
    "Second, there is the [TMPreproc class](api.rst#tmpreproc-class-for-parallel-text-preprocessing) which addresses these limitations. You can create an instance of this class from your (labelled) documents and then apply preprocessing methods to it. This instance is a \"state machine\", i.e. its contents (the documents) an behavior can change when you call a method. An example:\n",
    "\n",
    "```python\n",
    "corpus = {\n",
    "    \"doc1\": \"Hello world!\",\n",
    "    \"doc2\": \"Another example\"\n",
    "}\n",
    "\n",
    "preproc = TMPreproc(corpus)     # documents are directly tokenized\n",
    "preproc.tokens_to_lowercase()   # this changes the documents\n",
    "preproc.tokens                  # one of many ways to access the tokens\n",
    "\n",
    "# Out:\n",
    "# {\n",
    "#   'doc1': ['hello', 'world', '!'],\n",
    "#   'doc2': ['another', 'example']\n",
    "# }\n",
    "```\n",
    "\n",
    "The most important advantage is that `TMPreproc` employs parallel processing, i.e. it uses all available processors on your machine to do the computations necessary during preprocessing. For large text corpora, this can lead to a strong speed up. \n",
    "\n",
    "Both approaches offer mostly the same features in terms of available preprocessing methods. `TMPreproc` has some more methods to export the data to [pandas DataFrames](https://pandas.pydata.org/) or [datatable Frames](https://github.com/h2oai/datatable/). In general, the functional API is mostly used for quick prototyping and when using a small amount of data. For projects with large amounts of data, it's recommended to use `TMPreproc`, especially because of the parallel computation support.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "A note on the use of datatable Frames\n",
    "\n",
    "If you have installed the [datatable package](https://pypi.org/project/datatable/), many functions and methods in tmtoolkit return or accept [datatable Frames](https://github.com/h2oai/datatable/) instead of (the more commonly known) [pandas DataFrames](https://pandas.pydata.org/). This is because the former is much faster and memory efficient in most cases. You can always convert between the both like this:\n",
    "\n",
    "    import datatable as dt\n",
    "    import pandas as pd\n",
    "    \n",
    "    # a pandas DataFrame:\n",
    "    df = pd.DataFrame({'a': [1, 2, 3], 'b': list('xyz')})\n",
    "    \n",
    "    # DataFrame to datatable:\n",
    "    dtable = dt.Frame(df)\n",
    "    \n",
    "    # and vice versa datatable to DataFrame:\n",
    "    df == dtable.to_pandas()\n",
    "    \n",
    "    # Out:\n",
    "    #       a     b\n",
    "    # 0  True  True\n",
    "    # 1  True  True\n",
    "    # 2  True  True\n",
    "\n",
    "Even first creating a datatable and then converting to a DataFrame is often faster than directly creating a DataFrame.\n",
    "    \n",
    "</div>\n",
    "\n",
    "This chapter starts with the functional API and then turns to `TMPreproc`.\n",
    "\n",
    "## Functional API\n",
    "\n",
    "The functions in the preprocessing module make up the [functional API](api.rst#module-tmtoolkit.preprocess) for text preprocessing. We will explore some of the available functions. Most of them require at least passing a list of tokenized documents. In order to tokenize raw text documents (for example from a [Corpus](text_corpora.ipynb) object), we can use [tokenize()](api.rst#tmtoolkit.preprocess.tokenize). \n",
    "\n",
    "### Loading example data\n",
    "\n",
    "Let's load a sample of three documents from the built-in *NewsArticles* dataset. We'll save the document labels in `doc_labels` since the functional API works with lists of documents (not with dicts): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .boolean { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .integer { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .string  { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['NewsArticles-1880', 'NewsArticles-3350', 'NewsArticles-99']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(20191018)   # to make the sampling reproducible\n",
    "\n",
    "from tmtoolkit.corpus import Corpus\n",
    "from tmtoolkit.preprocess import tokenize\n",
    "\n",
    "corpus = Corpus.from_builtin_corpus('english-NewsArticles').sample(3)\n",
    "doc_labels = list(corpus.keys())\n",
    "doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "We can now tokenize these documents. We use `corpus.values()` to pass a list of documents. We get a list of tokenized documents back (i.e. a list of lists). We peak into the documents by only showing the first 10 words at maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['White',\n",
       "  'House',\n",
       "  'aides',\n",
       "  'told',\n",
       "  'to',\n",
       "  'keep',\n",
       "  'Russia-related',\n",
       "  'materials',\n",
       "  'Lawyers',\n",
       "  'for'],\n",
       " ['Frustration',\n",
       "  'as',\n",
       "  'cabin',\n",
       "  'electronics',\n",
       "  'ban',\n",
       "  'comes',\n",
       "  'into',\n",
       "  'force',\n",
       "  'Passengers',\n",
       "  'decry'],\n",
       " ['Should',\n",
       "  'you',\n",
       "  'have',\n",
       "  'two',\n",
       "  'bins',\n",
       "  'in',\n",
       "  'your',\n",
       "  'bathroom',\n",
       "  '?',\n",
       "  'Our']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = tokenize(corpus.values())\n",
    "[doc[:10] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Corpus language\n",
    "\n",
    "Some preprocessing steps are language-dependent, i.e. they're trained for different languages and hence you have to tell in which language your documents are written. At the moment, tmtoolkit only supports two languages off the shelf: English and German. \n",
    "\n",
    "In the functional API, all functions that are language-dependent have a `language` argument. Examples of such functions are [tokenize()](api.rst#tmtoolkit.preprocess.tokenize), [pos_tag()](api.rst#tmtoolkit.preprocess.pos_tag), [stem()](api.rst#tmtoolkit.preprocess.stem) and [lemmatize()](api.rst#tmtoolkit.preprocess.lemmatize). The default language for the `language` parameter of the preprocessing functions is set in [tmtoolkit.defaults.language](api.rst#tmtoolkit.defaults.language). If you don't change it, it's set to `\"english\"`. So you have two options when you use the functional API and work with a corpus that is not in English: you either pass the `language` parameter each time you use a language-dependent function; or you set `tmtoolkit.defaults.language` right at the beginning which will be used as default for all further language-dependent preprocessing functions. Let's try both options with a German sample corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tmtoolkit.preprocess import stem\n",
    "\n",
    "docs_de = [\n",
    "    'Von der Wiege bis zur Bahre, Formulare, Formulare.',\n",
    "    'Fischers Fritz fischt frische Fische.',\n",
    "    'Viel schon ist getan, mehr noch ist zu tun, sagt der Wasserhahn zum Wasserhuhn.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Option 1, passing the `language` parameter each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['von',\n",
       "  'der',\n",
       "  'wieg',\n",
       "  'bis',\n",
       "  'zur',\n",
       "  'bahr',\n",
       "  ',',\n",
       "  'formular',\n",
       "  ',',\n",
       "  'formular',\n",
       "  '.'],\n",
       " ['fisch', 'fritz', 'fischt', 'frisch', 'fisch', '.'],\n",
       " ['viel',\n",
       "  'schon',\n",
       "  'ist',\n",
       "  'getan',\n",
       "  ',',\n",
       "  'mehr',\n",
       "  'noch',\n",
       "  'ist',\n",
       "  'zu',\n",
       "  'tun',\n",
       "  ',',\n",
       "  'sagt',\n",
       "  'der',\n",
       "  'wasserhahn',\n",
       "  'zum',\n",
       "  'wasserhuhn',\n",
       "  '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_de = tokenize(docs_de, language='german')\n",
    "stemmed_de = stem(tokens_de, language='german')\n",
    "stemmed_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Option 2, setting `tmtoolkit.defaults.language` provides the same output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tmtoolkit.defaults\n",
    "tmtoolkit.defaults.language = 'german'\n",
    "\n",
    "tokens_de = tokenize(docs_de)\n",
    "stemmed_de == stem(tokens_de) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will return to the English corpus hence we can reset the default language and clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmtoolkit.defaults.language = 'english'\n",
    "\n",
    "del docs_de, tokens_de, stemmed_de "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### A small tour around the functional preprocessing API\n",
    "\n",
    "We will continue with the most important functions in the preprocessing API and apply them to our English sample corpus.\n",
    "\n",
    "#### Document length\n",
    "\n",
    "The document length is the number of tokens per document and can be obtained with [doc_lengths()](api.rst#tmtoolkit.preprocess.doc_lengths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[227, 646, 1052]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import doc_lengths\n",
    "\n",
    "doc_lengths(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Vocabulary and document frequencies\n",
    "\n",
    "The vocabulary is the set of unique tokens in the corpus, i.e. all tokens that occur at least once in at least one of the documents. You can use [vocabulary()](api.rst#tmtoolkit.preprocess.vocabulary) for that and [vocabulary_counts()](api.rst#tmtoolkit.preprocess.vocabulary_counts) to additionally get the number of times each token appears in the corpus. \n",
    "\n",
    "The document frequency of a token is the number of documents in which this token occurs at least once. The function [doc_frequencies()](api.rst#tmtoolkit.preprocess.doc_frequencies) returns this measure for all tokens in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%', \"'\", \"''\", \"'s\", '(', ')', ',', '-', '-Al', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import vocabulary, vocabulary_counts, doc_frequencies\n",
    "\n",
    "# first 10 entries from the sorted vocab\n",
    "vocabulary(docs, sort=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 82),\n",
       " (',', 70),\n",
       " ('.', 60),\n",
       " ('to', 53),\n",
       " ('and', 45),\n",
       " ('in', 38),\n",
       " ('a', 31),\n",
       " ('``', 28),\n",
       " ('of', 25),\n",
       " (\"''\", 23)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unsorted vocabulary counts as Counter object\n",
    "vocab_counts = vocabulary_counts(docs)\n",
    "# get top 10 tokens by occurrence\n",
    "vocab_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_freq = doc_frequencies(docs)\n",
    "\n",
    "# \"the\" occurs in all three documents, \"Lawyers\" only in one\n",
    "doc_freq['the'], doc_freq['Lawyers']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part-of-speech (POS) tagging\n",
    "\n",
    "Part-of-speech (POS) tagging finds the grammatical word-category for each token in a document. The function [pos_tag()](api.rst#tmtoolkit.preprocess.pos_tag) employs this for the whole corpus. It returns a list of tags for each document. These tags conform to a specific *tagset*. For English this is the [Penn Treebank tagset](https://ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and for German this is the [STTS tagset](http://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/TagSets/stts-table.html).\n",
    "\n",
    "These tags can be used to filter, annotate or lemmatize the documents.\n",
    "\n",
    "Remember that this is a language-dependent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('White', 'NNP'),\n",
       " ('House', 'NNP'),\n",
       " ('aides', 'NNS'),\n",
       " ('told', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('keep', 'VB'),\n",
       " ('Russia-related', 'JJ'),\n",
       " ('materials', 'NNS'),\n",
       " ('Lawyers', 'NNS'),\n",
       " ('for', 'IN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import pos_tag\n",
    "\n",
    "docs_pos = pos_tag(docs)\n",
    "\n",
    "# show pairs of tokens and POS tags for the first 10 tokens in the first document\n",
    "list(zip(docs[0][:10], docs_pos[0][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and lemmatization\n",
    "\n",
    "Stemming and lemmatization bring a token, if it is a word, to a base form. The former method is rule-based and creates base forms by chopping off common pre- and suffixes. The resulting token may not be a lexicographically correct word any more. We've already used [stem()](api.rst#tmtoolkit.preprocess.stem) in an example above.\n",
    "\n",
    "Lemmatization is a more sophisticated process that tries to find lexicographically correct base form of a given word by also considering its POS tag and possibly its context (tokens and POS tags nearby). It's usually not rule-based but a trained model that predicts the base form from the mentioned parameters. Lemmatization can be applied with [lemmatize()](api.rst#tmtoolkit.preprocess.lemmatize).\n",
    "\n",
    "Remember that both functions are language-dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('White', 'White'),\n",
       " ('House', 'House'),\n",
       " ('aides', 'aide'),\n",
       " ('told', 'tell'),\n",
       " ('to', 'to'),\n",
       " ('keep', 'keep'),\n",
       " ('Russia-related', 'Russia-related'),\n",
       " ('materials', 'material'),\n",
       " ('Lawyers', 'Lawyers'),\n",
       " ('for', 'for')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import lemmatize\n",
    "\n",
    "docs_lem = lemmatize(docs, docs_pos)\n",
    "# show pairs of original tokens and lemmata for the first 10 tokens of first document\n",
    "list(zip(docs[0][:10], docs_lem[0][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token normalization\n",
    "\n",
    "Depending on your methodology, it may be necessary to \"clean\" or \"normalize\" your tokens in different ways in order to remove noise from the corpus, such as punctuation tokens or numbers, upper/lowercase forms of the same word, etc. Note that this is usually not necessary when you work with more modern approaches such as word embeddings (word vectors).   \n",
    "\n",
    "If you want to remove certain characters in *all* tokens in your corpus, you can use [remove_chars()](api.rst#tmtoolkit.preprocess.remove_chars) and pass it a sequence of characters to remove.\n",
    "\n",
    "Note that for the following examples we continue working with the lemmatized documents `docs_lem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wht', 'Hs', 'd', 'tll', 't', 'kp', 'Rss-rltd', 'mtrl', 'Lwyrs', 'fr']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import remove_chars\n",
    "\n",
    "# remove all vowels from the documents, show first 10 tokens from first document\n",
    "remove_chars(docs_lem, 'aeiou')[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can for example use this to remove all punctuation characters from all tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Should', 'Should'),\n",
       " ('you', 'you'),\n",
       " ('have', 'have'),\n",
       " ('two', 'two'),\n",
       " ('bin', 'bin'),\n",
       " ('in', 'in'),\n",
       " ('your', 'your'),\n",
       " ('bathroom', 'bathroom'),\n",
       " ('?', ''),\n",
       " ('Our', 'Our')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "docs_clean = remove_chars(docs_lem, string.punctuation)\n",
    "# show pairs of original tokens and cleaned tokens for the first 10 tokens of 2nd doc.\n",
    "list(zip(docs_lem[2][:10], docs_clean[2][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice how the token `'?'` was transformed to an empty string `''`, because \"?\" is a punctuation character.\n",
    "\n",
    "A common (but harsh) practice is to transform all tokens to lowercase forms, which can be done with [to_lowercase()](api.rst#tmtoolkit.preprocess.to_lowercase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['should', 'you', 'have', 'two', 'bin', 'in', 'your', 'bathroom', '', 'our']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import to_lowercase\n",
    "\n",
    "docs_clean = to_lowercase(docs_clean)\n",
    "docs_clean[2][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The function [clean_tokens()](api.rst#tmtoolkit.preprocess.clean_tokens) finally applies several steps that remove tokens that meet certain criteria. This includes removing:\n",
    "\n",
    "- punctuation tokens\n",
    "- stopwords (very common words for the given language)\n",
    "- empty tokens (i.e. `''`)\n",
    "- tokens that are longer or shorter than a certain number of characters\n",
    "- numbers  \n",
    "\n",
    "Note that this is a language-dependent function, because the default stopword list is determined per language. This function has many parameters to tweak, so it's recommended to check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two',\n",
       " 'bin',\n",
       " 'bathroom',\n",
       " 'bathroom',\n",
       " 'fill',\n",
       " 'shampoo',\n",
       " 'bottle',\n",
       " 'toilet',\n",
       " 'roll',\n",
       " 'cleaning']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import clean_tokens\n",
    "\n",
    "# remove punct., stopwords, empty tokens (this is the default)\n",
    "# plus tokens shorter than 2 characters and numeric tokens like \"2019\"\n",
    "docs_final = clean_tokens(docs_clean, remove_shorter_than=2, remove_numbers=True)\n",
    "\n",
    "# first 10 tokens of doc. #2\n",
    "docs_final[2][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Due to the removal of several tokens in the previous step, the document lengths for the processed corpus are much smaller than for the original corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([227, 646, 1052], [129, 310, 504])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths(docs), doc_lengths(docs_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also observe that the vocabulary got smaller after the processing steps, which, for large corpora, is also important in terms of computation time and memory consumption for later analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681, 478)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary(docs)), len(vocabulary(docs_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also apply custom token transform functions by using [transform()](api.rst#tmtoolkit.preprocess.transform) and passing it a function that should be applied to each token in each document (hence it must accept one string argument).\n",
    "\n",
    "First let's define such a function. Here we create a simple function that should return a token's \"shape\" in terms of the case of its characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('XXX', 'XxxxxXxxx', 'xxxxx')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_shape(t):\n",
    "    return ''.join(['X' if str.isupper(c) else 'x' for c in t])\n",
    "\n",
    "token_shape('USA'), token_shape('CamelCase'), token_shape('lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now apply this function to our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('White', 'Xxxxx'),\n",
       " ('House', 'Xxxxx'),\n",
       " ('aides', 'xxxxx'),\n",
       " ('told', 'xxxx'),\n",
       " ('to', 'xx'),\n",
       " ('keep', 'xxxx'),\n",
       " ('Russia-related', 'Xxxxxxxxxxxxxx'),\n",
       " ('materials', 'xxxxxxxxx'),\n",
       " ('Lawyers', 'Xxxxxxx'),\n",
       " ('for', 'xxx')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import transform\n",
    "\n",
    "doc_shapes = transform(docs, token_shape)\n",
    "\n",
    "# show pairs of tokens and POS tags for the first 10 tokens in the first document\n",
    "list(zip(docs[0][:10], doc_shapes[0][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keywords-in-context (KWIC)\n",
    "\n",
    "*Keywords-in-context (KWIC)* allow you to quickly investigate certain keywords and their neighborhood of tokens, i.e. the tokens that appear right before and after this keyword.\n",
    "\n",
    "tmtoolkit provides three functions for this purpose:\n",
    "\n",
    "- [kwic()](api.rst#tmtoolkit.preprocess.kwic) is the base function accepting the input documents, a search pattern and several options that control how the search pattern is matched (more on that below); use this function when you want to further process the output of a KWIC search;\n",
    "- [kwic_table()](api.rst#tmtoolkit.preprocess.kwic_table) is the more \"user friendly\" version of the above function as it produces a datatable with the highlighted keyword by default\n",
    "- [filter_tokens_with_kwic()](api.rst#tmtoolkit.preprocess.filter_tokens_with_kwic) works similar to the above functions but returns the result as list of tokenized documents again; it is explained in the [section on filtering](#Filtering-tokens-and-documents)\n",
    "\n",
    "Let's see both functions in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [['told', 'Reuters', 'news', 'agency', '.'],\n",
       "  ['Jazeera', 'and', 'news', 'agencies']],\n",
       " []]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import kwic, kwic_table\n",
    "\n",
    "kwic(docs, 'news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that the first and last document do not contain any keyword that matches `\"news\"`, hence we get empty results for these documents. In the second document, we get two result contexts for the requested keyword. This keyword stands in the middle and is surrounded by its \"context tokens\", which by default means two tokens to the left and two tokens to the right. Notice that in the second result context only one token to the right is shown since the document ends after \"agencies\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>1</td><td>0</td><td>told Reuters *news* agency .</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>1</td><td>1</td><td>Jazeera and *news* agencies</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwic_table(docs, 'news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With `kwic_table()`, we get back a datatable which provides a better formatting for quick investigation. See how the matched tokens are highlighted as `*news*` and empty results are removed (only document \"1\" contains the keyword which is the *second* document – remember that Python indexing starts with 0).\n",
    "\n",
    "We can also pass the document labels via `doc_labels` to get proper labels in the `doc` column instead of document indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-3350</td><td>0</td><td>told Reuters *news* agency .</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-3350</td><td>1</td><td>Jazeera and *news* agencies</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwic_table(docs, 'news', doc_labels=doc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important parameter is `context_size`. It determines the number of tokens to display left and right to the found keyword. You can either pass a single integer for a symmetric context or a tuple with integers `(<left>, <right>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-3350</td><td>0</td><td>a traveler , told Reuters *news* agency . Al Jazee&#133;</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-3350</td><td>1</td><td>Source : -Al Jazeera and *news* agencies</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a symmetric context of size (5, 5)\n",
    "kwic_table(docs, 'news', context_size=5, doc_labels=doc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-3350</td><td>0</td><td>a traveler , told Reuters *news* agency</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-3350</td><td>1</td><td>Source : -Al Jazeera and *news* agencies</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an asymmetric context of size (5, 1)\n",
    "kwic_table(docs, 'news', context_size=(5, 1), doc_labels=doc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The KWIC functions become really powerful when using the pattern matching options. So far, we were looking for *exact* matches between the corpus tokens and our keyword `\"news\"`. However, it is also possible to match patterns like `\"new*\"` (matches any word starting with \"new\") or `\"agenc(y|ies)\"` (a regular expression matching \"agency\" and \"agencies\"). The next section gives an introduction on the different options for pattern matching.\n",
    "\n",
    "#### Common parameters for pattern matching functions\n",
    "\n",
    "Several functions and methods in tmtoolkit support pattern matching, including the already mentioned KWIC functions but also functions for filtering tokens or documents as you will see later. They all share similar function signatures, i.e. similar parameters:\n",
    "\n",
    "- `search_token` or `search_tokens`: allows to specify one or more patterns as strings\n",
    "- `match_type`: sets the matching type and can be one of the following options:\n",
    "  - `'exact'` (default): exact string matching (optionally ignoring character case), i.e. no pattern matching\n",
    "  - `'regex'` uses [regular expression](https://docs.python.org/3/library/re.html) matching\n",
    "  - `'glob'` uses \"glob patterns\" like `\"politic*\"` which matches for example \"politic\", \"politics\" or \"politician\" (see [globre package](https://pypi.org/project/globre/))\n",
    "- `ignore_case`: ignore character case (applies to all three match types)\n",
    "- `glob_method`: if `match_type` is 'glob', use this glob method. Must be `'match'` or `'search'` (similar behavior as Python's [re.match](https://docs.python.org/3/library/re.html#re.match) or [re.search](https://docs.python.org/3/library/re.html#re.search))\n",
    "- `inverse`: inverse the match results, i.e. if matching for \"hello\", return all results that do *not* match \"hello\"\n",
    "\n",
    "Let's try out some of these options with `kwic_table()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1880</td><td>0</td><td>law enforcement *agencies* to keep</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1880</td><td>1</td><td>organizations , *agencies* and individuals</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-3350</td><td>0</td><td>Reuters news *agency* . Al</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-3350</td><td>1</td><td>and news *agencies*</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>4 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a regular expression, ignoring case\n",
    "kwic_table(docs, r'agenc(y|ies)', match_type='regex', ignore_case=True,\n",
    "           doc_labels=doc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1880</td><td>0</td><td>false and *politically* motivated attacks</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-99</td><td>0</td><td>, senior *policy* adviser for</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a glob, ignoring case\n",
    "kwic_table(docs, 'pol*', match_type='glob', ignore_case=True,\n",
    "           doc_labels=doc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-99</td><td>0</td><td>potential simple *solution* that could</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-99</td><td>1</td><td>confused by *aerosols* . ''</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-99</td><td>2</td><td>bottles , *aerosols* for deodorant</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>3 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a glob, ignoring case\n",
    "kwic_table(docs, '*sol*', match_type='glob', ignore_case=True,\n",
    "           doc_labels=doc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1880</td><td>0</td><td>in the *2016* presidential election</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1880</td><td>1</td><td>related investigations *,* ABC News</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1880</td><td>2</td><td>has confirmed *.* `` The</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1880</td><td>3</td><td>confirmed . *``* The White</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1880</td><td>4</td><td>motivated attacks *,* '' an</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1880</td><td>5</td><td>attacks , *''* an administration</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1880</td><td>6</td><td>News Wednesday *.* The directive</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1880</td><td>7</td><td>last week *by* Senate Democrats</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1880</td><td>8</td><td>between Trump *'s* administration ,</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1880</td><td>9</td><td>'s administration *,* campaign and</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1880</td><td>10</td><td>transition teams *``* ? or</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1880</td><td>11</td><td>teams `` *?* or anyone</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1880</td><td>12</td><td>their behalf *``* ? and</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1880</td><td>13</td><td>behalf `` *?* and Russian</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1880</td><td>14</td><td>their associates *.* Similarly ,</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>252</td><td>NewsArticles-99</td><td>142</td><td>you do *n't* have the</td></tr>\n",
       "    <tr><td class='row_index'>253</td><td>NewsArticles-99</td><td>143</td><td>two bins *?* There are</td></tr>\n",
       "    <tr><td class='row_index'>254</td><td>NewsArticles-99</td><td>144</td><td>other options *.* Hang a</td></tr>\n",
       "    <tr><td class='row_index'>255</td><td>NewsArticles-99</td><td>145</td><td>recycling bin *.* Or opt</td></tr>\n",
       "    <tr><td class='row_index'>256</td><td>NewsArticles-99</td><td>146</td><td>non-recyclable items *.*</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>257 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a regex that matches all tokens with at least one vowel and\n",
    "# inverting these matches, i.e. all tokens *without* any vowels\n",
    "kwic_table(docs, r'[AEIOUaeiou]', match_type='regex', inverse=True,\n",
    "           doc_labels=doc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Filtering tokens and documents\n",
    "\n",
    "We can use the pattern matching parameters in numerous filtering functions and methods. The heart of many of these functions is [token_match()](api.rst#tmtoolkit.preprocess.token_match). Given a search pattern, a list of tokens and optionally some pattern matching parameters, it returns a binary NumPy array of the same length as the input tokens. Each occurrence of `True` in this binary array signals a match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White : False\n",
      "House : False\n",
      "aides : False\n",
      "told : True\n",
      "to : True\n",
      "keep : False\n",
      "Russia-related : False\n",
      "materials : False\n",
      "Lawyers : False\n",
      "for : False\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import token_match\n",
    "\n",
    "doc0_snippet = docs[0][:10]   # first 10 tokens of first doc.\n",
    "# get all tokens that match \"to*\"\n",
    "matches = token_match('to*', doc0_snippet, match_type='glob')\n",
    "\n",
    "# iterate through tokens and matches, show pair-wise results\n",
    "for tok, match in zip(doc0_snippet, matches):\n",
    "    print(tok, ':', match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `token_match()` function is a rather low-level function that you may use for pattern matching against any list/array of strings, e.g. a list of tokens, file names, etc.\n",
    "\n",
    "The following functions cover common use-cases for filtering during text preprocessing. Many of these functions start either with `filter_...()` or `remove_...()` and these pairs of filter and remove functions are complements. A filter function will always *retain* the matched elements whereas a remove function will always *drop* the matched elements. We can observe that with the first pair of functions, [filter_tokens()](api.rst#tmtoolkit.preprocess.filter_tokens) and [remove_tokens()](api.rst#tmtoolkit.preprocess.remove_tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['House', 'House', 'House', 'House'],\n",
       " [],\n",
       " ['house', 'greenhouse', 'household']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import filter_tokens, remove_tokens\n",
    "\n",
    "# retain only the tokens that match the pattern in each document\n",
    "filter_tokens(docs, '*house*', match_type='glob', ignore_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White',\n",
       " 'aides',\n",
       " 'told',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'Russia-related',\n",
       " 'materials',\n",
       " 'Lawyers',\n",
       " 'for',\n",
       " 'the']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retain only the tokens that DON'T match the pattern in each document\n",
    "# will only show the first 10 tokens from the first document here, b/c\n",
    "# the resulting documents are too long; you can see that \"House\" was\n",
    "# removed from [\"White\", \"House\", ...]\n",
    "remove_tokens(docs, '*house*', match_type='glob', ignore_case=True)[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pair [filter_documents()](api.rst#tmtoolkit.preprocess.filter_documents) and [remove_documents()](api.rst#tmtoolkit.preprocess.remove_documents) works similarily, but filters or drops whole documents regarding the supplied match criteria. Both accept the standard pattern matching parameters but also a parameter `matches_threshold` with default value `1`. When this number of matching tokens is hit, the document will be part of the result set (`filter_documents()`) or removed from the result set (`remove_documents()`). By this, we can for example retain only those documents that contain certain token patterns.\n",
    "\n",
    "Let's try these functions out in practice. This time we will also pass the `doc_labels` so that the filtering also applies to our list of document labels. If `doc_labels` is also passed, the functions return two results – the filtered list of documents and the filtered list of document labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsArticles-1880', 'NewsArticles-99']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import filter_documents, remove_documents\n",
    "\n",
    "filtered_docs, filtered_doc_labels = filter_documents(docs, '*house*',\n",
    "                                                      doc_labels=doc_labels,\n",
    "                                                      match_type='glob',\n",
    "                                                      ignore_case=True)\n",
    "filtered_doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that two out of three documents contained the pattern `'*house*'` and hence were retained. The list `filtered_docs` represents these two documents (we don't print them here because they are too long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also adjust `matches_threshold` to set the minimum number of token matches for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsArticles-1880']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs, filtered_doc_labels = filter_documents(docs, '*house*',\n",
    "                                                      doc_labels=doc_labels,\n",
    "                                                      match_type='glob',\n",
    "                                                      ignore_case=True,\n",
    "                                                      matches_threshold=4)\n",
    "filtered_doc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsArticles-3350']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs, filtered_doc_labels = remove_documents(docs, '*house*',\n",
    "                                                      doc_labels=doc_labels,\n",
    "                                                      match_type='glob',\n",
    "                                                      ignore_case=True)\n",
    "filtered_doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `remove_documents()` we get only the documents that did *not* contain the specified pattern.\n",
    "\n",
    "Another useful pair of functions is [filter_documents_by_name()](api.rst#tmtoolkit.preprocess.filter_documents_by_name) and [remove_documents_by_name()](api.rst#tmtoolkit.preprocess.remove_documents_by_name). Both functions again accept the same pattern matching parameters but they only apply them to the document names, i.e. document *labels*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsArticles-1880', 'NewsArticles-3350']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import filter_documents_by_name\n",
    "\n",
    "filtered_docs, filtered_doc_labels = filter_documents_by_name(docs, doc_labels,\n",
    "                                                              r'-\\d{4}$',\n",
    "                                                              match_type='regex')\n",
    "filtered_doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we wanted to retain only the documents whose document labels ended with exactly 4 digits, like \"...-1234\". Hence, we only get \"NewsArticles-1880\" and \"NewsArticles-3350\" but not \"NewsArticles-99\". Again, `remove_documents_by_name()` will do the exact opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also use [Keywords-in-context (KWIC)](#Keywords-in-context-(KWIC)) to filter your tokens in the neighborhood around certain keyword pattern(s). The function for that is called [filter_tokens_with_kwic()](api.rst#tmtoolkit.preprocess.filter_tokens_with_kwic) and works very similar to [kwic()](api.rst#tmtoolkit.preprocess.kwic) but returns the result as a list of tokenized documents (whereas `kwic()` returns a list of KWIC results per document) with which you can continue working as usual. Here, we filter the tokens in each document to get the tokens directly in front and after the glob pattern `'*house*'` (`context_size=1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['White',\n",
       "  'House',\n",
       "  'aides',\n",
       "  'White',\n",
       "  'House',\n",
       "  'aides',\n",
       "  'White',\n",
       "  'House',\n",
       "  'is',\n",
       "  'White',\n",
       "  'House',\n",
       "  'and'],\n",
       " [],\n",
       " ['the',\n",
       "  'house',\n",
       "  ',',\n",
       "  'of',\n",
       "  'greenhouse',\n",
       "  'gases',\n",
       "  'UK',\n",
       "  'household',\n",
       "  'threw']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import filter_tokens_with_kwic\n",
    "\n",
    "filter_tokens_with_kwic(docs, '*house*', context_size=1,\n",
    "                        match_type='glob', ignore_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you annotated your documents' tokens with Part-of-Speech (POS) tags, you can also filter them using [filter_for_pos()](api.rst#tmtoolkit.preprocess.filter_for_pos). You need to pass the documents, their POS tags and the POS tag(s) to be used for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White',\n",
       " 'House',\n",
       " 'aides',\n",
       " 'materials',\n",
       " 'Lawyers',\n",
       " 'Trump',\n",
       " 'administration',\n",
       " 'White',\n",
       " 'House',\n",
       " 'aides']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import filter_for_pos\n",
    "\n",
    "filtered_docs, filtered_docs_pos = filter_for_pos(docs, docs_pos, 'N')\n",
    "# displaying only the first 10 filtered tokens from the first document\n",
    "filtered_docs[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we filtered for tokens that were identified as nouns by passing the *simplified POS tag* `'N'` (for more on simplified tags, see the function documentation). We can also filter for more than one tag, e.g. nouns or verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White',\n",
       " 'House',\n",
       " 'aides',\n",
       " 'told',\n",
       " 'keep',\n",
       " 'materials',\n",
       " 'Lawyers',\n",
       " 'Trump',\n",
       " 'administration',\n",
       " 'have']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs, filtered_docs_pos = filter_for_pos(docs, docs_pos, ['N', 'V'])\n",
    "# displaying only the first 10 filtered tokens from the first document\n",
    "filtered_docs[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter_for_pos()` has no `remove_...()` counterpart, but you can set the `inverse` parameter to `True` to achieve the same effect.\n",
    "\n",
    "Finally there are two functions for removing tokens based on their [document frequency](#Vocabulary-and-document-frequencies): [remove_common_tokens()](api.rst#tmtoolkit.preprocess.remove_common_tokens) and [remove_uncommon_tokens()](api.rst#tmtoolkit.preprocess.remove_uncommon_tokens). The former removes all tokens that have a document frequency greater or equal a certain threshold defined by parameter `df_threshold`. The latter does the same for all tokens that have a document frequency lower or equal `df_threshold`. This parameter can either be a relative frequency (default) or absolute count (by setting parameter `absolute=True`).\n",
    "\n",
    "Before applying the function, let's have a look at the number of tokens per document again, to later see how many we will remove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[227, 646, 1052]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[143, 413, 699]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import remove_common_tokens\n",
    "\n",
    "doc_lengths(remove_common_tokens(docs, df_threshold=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing all tokens with a document threshold of at least 0.9, we would remove quite a number of tokens in each document. Let's investigate the vocabulary in order to see which tokens are removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\",\n",
       " \"'s\",\n",
       " ',',\n",
       " '.',\n",
       " '?',\n",
       " 'The',\n",
       " '``',\n",
       " 'a',\n",
       " 'all',\n",
       " 'also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'be',\n",
       " 'for',\n",
       " 'has',\n",
       " 'have',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'more',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'other',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'to',\n",
       " 'which',\n",
       " 'with'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_vocab = vocabulary(docs)  # vocabulary of unfiltered documents\n",
    "\n",
    "filtered_docs = remove_common_tokens(docs, df_threshold=0.9)\n",
    "filtered_vocab = vocabulary(filtered_docs)\n",
    "orig_vocab - filtered_vocab   # set difference gives removed vocabulary tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remove_uncommon_tokens` works similarily. This time, let's use an absolute number as threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%', '(', ')', '-Al', '.-', '10', '12', '135,000', '2016', '38']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import remove_uncommon_tokens\n",
    "\n",
    "filtered_docs = remove_uncommon_tokens(docs, df_threshold=1, absolute=True)\n",
    "filtered_vocab = vocabulary(filtered_docs)\n",
    "# set difference gives removed vocabulary tokens\n",
    "# this time, show only the first 10 tokens that were removed\n",
    "sorted(orig_vocab - filtered_vocab)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above means that we remove all tokens that appear only in exactly one document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Expanding compound words and joining tokens\n",
    "\n",
    "Compound words like \"US-Student\" or \"non-recyclable\" can be expanded to separate tokens like \"US\", \"Student\" and \"non\", \"recyclable\" using [expand_compounds()](api.rst#tmtoolkit.preprocess.expand_compounds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['US', 'Student', 'on', 'Berlin', 'bound', 'train', '.']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import expand_compounds\n",
    "\n",
    "# trying it out with a single *tokenized* document:\n",
    "expand_compounds([['US-Student', 'on', 'Berlin-bound', 'train', '.']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-Al',\n",
       " '.-',\n",
       " 'Britain-bound',\n",
       " 'Lagoas-and',\n",
       " 'Russia-related',\n",
       " 'ban.-',\n",
       " 'carry-on',\n",
       " 'editor-in-chief',\n",
       " 'experts-perplexed',\n",
       " 'non-recyclable',\n",
       " 'off-putting',\n",
       " 're-use'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying this to our documents\n",
    "\n",
    "docs_expanded = expand_compounds(docs)\n",
    "orig_vocab - vocabulary(docs_expanded)    # vocabulary tokens that were expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to join together certain *subsequent* occurrences of tokens or token patterns. This means you can for example transform all of the subsequent tokens \"White\" and \"House\" to single tokens \"White_House\". In case you don't use n-grams (see next section), this is very helpful when you want to capture a named entity that is made up by several tokens, such as persons, institutions or concepts like \"Climate Change\", as a single token. The function to use for this is [glue_tokens()](api.rst#tmtoolkit.preprocess.glue_tokens). You can pass this function:\n",
    "\n",
    "- documents `docs` to operate on;\n",
    "- a `patterns` sequence of length *N* that is used to match the subsequent *N* tokens;\n",
    "- a `glue` string that is used to join the matched subsequent tokens (by default: `\"_\"`).\n",
    "\n",
    "Along with that, you can adjust the token matching with the well-known [common token matching parameters](#Common-parameters-for-pattern-matching-functions).\n",
    "\n",
    "Let's \"glue\" all subsequent occurrences of \"White\" and \"House\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White_House',\n",
       " 'aides',\n",
       " 'told',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'Russia-related',\n",
       " 'materials',\n",
       " 'Lawyers',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Trump',\n",
       " 'administration',\n",
       " 'have',\n",
       " 'instructed',\n",
       " 'White_House',\n",
       " 'aides',\n",
       " 'to',\n",
       " 'preserve',\n",
       " 'any',\n",
       " 'material']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import glue_tokens\n",
    "\n",
    "# showing only first 20 tokens in document 1\n",
    "glue_tokens(docs, ['White', 'House'])[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of exact matches, we can also specify a sequence of regular expressions (or \"glob\" expressions) that must be matched by subsequent tokens. Here, we want to join all token pairs where the first token starts with a captial letter, and the second token is \"Trump\". We also set `return_glued_tokens` to True so that a second return value is created: a list of all matched and \"glued\" tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'President_Trump'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_glued, glued = glue_tokens(docs, [r'^[A-Z]', 'Trump'], match_type='regex',\n",
    "                                return_glued_tokens=True)\n",
    "glued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick view at the context using [kwic_table()](#Keywords-in-context-(KWIC)). We can see that only one such pattern was matched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>0</td><td>0</td><td>contact between *President_Trump* 's advisers</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>1 row &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwic_table(docs_glued, 'President_Trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating n-grams\n",
    "\n",
    "So far, we worked with *unigrams*, i.e. each document consisted of a sequence of discrete tokens. We can also generate *n-grams* from our corpus where each document consists of a sequence of *n* subsequent tokens. An example would be:\n",
    "\n",
    "Document: \"This is a simple example.\"\n",
    "\n",
    "**n=1 (unigrams):**\n",
    "\n",
    "    ['This', 'is', 'a', 'simple', 'example', '.']\n",
    "\n",
    "**n=2 (bigrams):**\n",
    "\n",
    "    ['This is', 'is a', 'a simple', 'simple example', 'example .']\n",
    "\n",
    "**n=3 (trigrams):**\n",
    "\n",
    "    ['This is a', 'is a simple', 'a simple example', 'simple example .']\n",
    "\n",
    "The function [ngrams()](api.rst#tmtoolkit.preprocess.ngrams) allows us to generate n-grams from tokenized documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White House',\n",
       " 'House aides',\n",
       " 'aides told',\n",
       " 'told to',\n",
       " 'to keep',\n",
       " 'keep Russia-related',\n",
       " 'Russia-related materials',\n",
       " 'materials Lawyers',\n",
       " 'Lawyers for',\n",
       " 'for the']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import ngrams\n",
    "\n",
    "# showing the first 10 bigrams from the first document:\n",
    "ngrams(docs, n=2)[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string used to join the tokens in each n-gram can be specified via `join_str`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White_House_aides',\n",
       " 'House_aides_told',\n",
       " 'aides_told_to',\n",
       " 'told_to_keep',\n",
       " 'to_keep_Russia-related',\n",
       " 'keep_Russia-related_materials',\n",
       " 'Russia-related_materials_Lawyers',\n",
       " 'materials_Lawyers_for',\n",
       " 'Lawyers_for_the',\n",
       " 'for_the_Trump']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the first 10 trigrams from the first document:\n",
    "ngrams(docs, n=3, join_str='_')[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-grams don't have to be joined. You can use `join=False` to generate n-grams as string lists of size *n*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['White', 'House'],\n",
       " ['House', 'aides'],\n",
       " ['aides', 'told'],\n",
       " ['told', 'to'],\n",
       " ['to', 'keep'],\n",
       " ['keep', 'Russia-related'],\n",
       " ['Russia-related', 'materials'],\n",
       " ['materials', 'Lawyers'],\n",
       " ['Lawyers', 'for'],\n",
       " ['for', 'the']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the first 10 bigrams from the first document:\n",
    "ngrams(docs, n=2, join=False)[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a sparse document-term matrix (DTM)\n",
    "\n",
    "If you're working with a bag-of-words representation of your data, you usually convert the preprocessed documents to a document-term matrix (DTM), which represents of the number of occurrences of each term (i.e. vocabulary token) in each document. This is a *N* rows by *M* columns matrix, where *N* is the number of documents and *M* is the vocabulary size (i.e. the number of unique tokens in the corpus).\n",
    "\n",
    "Not all tokens from the vocabulary occur in all documents. In fact, many tokens will occur only in a small subset of the documents if you're dealing with a \"real world\" dataset. This means that most entries in such a DTM will be zero. Almost all functions in tmtoolkit therefore generate and work with *sparse* matrices, where only non-zero values are stored in computer memory.\n",
    "\n",
    "For this example, we'll use the preprocessed documents `docs_final` from above. First, let's check the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary(docs_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use [sparse_dtm()](api.rst#tmtoolkit.preprocess.sparse_dtm) to generate a sparse DTM. We can either pass an already computed *sorted* vocabulary or let the function itself generate a vocabulary which is necessary to construct the DTM. In the latter case, the generated vocabulary is also returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x478 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 529 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import sparse_dtm\n",
    "\n",
    "dtm, vocab_final = sparse_dtm(docs_final)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a sparse matrix with 3 rows (which corresponds with the number of documents) and 478 columns was generated (which corresponds with the vocabulary size). 529 elements in this matrix are non-zero.\n",
    "\n",
    "We can convert this matrix to a non-sparse, i.e. *dense*, representation and see parts of its elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2, 1, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 2, 1]], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that you should only convert a sparse matrix to a dense representation when you're either dealing with a small amount of data (which is what we're doing in this example), or use only a part of the full matrix. Converting a sparse matrix to a dense representation can otherwise easily exceed the available computer memory.\n",
    "\n",
    "There exist different \"formats\" for sparse matrices, which have different advantages and disadvantes (see for example the [SciPy \"sparse\" module documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html#usage-information). **Not all formats support all operations that you can usually apply to an ordinary, dense matrix.** By default, the generated DTM is in \"coo\" format, which is a good intermediate format that you can use to convert to a different sparse matrix format quickly, but that doesn't offer many matrix operations. For example, the \"coo\" format doesn't support indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not running the following here:\n",
    "# dtm[0, 0]\n",
    "\n",
    "# it creates the following exception:\n",
    "# TypeError: 'coo_matrix' object is not subscriptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you have to convert the sparse DTM to another format first. For example, the CSR format allows indexing and is especially optimized for fast row access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.tocsr()[0, 443]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the number of times the token at vocabulary index 443 occurs in the first document. Which token and document does this exactly refer to? We can find out using `doc_labels`, which corresponds with the rows in `dtm` and `vocab_final` that was returned by `sparse_dtm()` and corresponds with the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NewsArticles-1880', 'trump')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_labels[0], vocab_final[443]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the index 443 come from? It's the position of the token \"trump\" in the `vocab_final` list. These indices are important when working with DTMs so you should know [Python's methods of the *list* data type](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_final.index('trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the following example of finding out the index for \"administration\" and then getting an array that represents the number of occurrences of this token across all three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_admin_ix = vocab_final.index('administration')\n",
    "dtm.tocsc()[:, vocab_admin_ix].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel processing with the `TMPreproc` class\n",
    "\n",
    "As mentioned in the beginning of this chapter, the [TMPreproc class](api.rst#tmpreproc-class-for-parallel-text-preprocessing) employs parallel computation for text preprocessing. All functions that are available in the functional API are also available in the `TMPreproc` class as *properties* or *methods*. So you can do exactly the same things, only with a slightly different syntax and with the power of parallel processing in your back.\n",
    "\n",
    "### Optional: enabling logging output\n",
    "\n",
    "At first let's have a look on how to display the logging output from tmtoolkit. By default, tmtoolkit does not expose any internal logging messages. Sometimes, for example for diagnostic output during debugging or in order to see progress for long running operations, it's helpful to enable logging output display which can be done as follows:\n",
    "\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    tmtoolkit_log = logging.getLogger('tmtoolkit')\n",
    "    # set the minimum log level to display, for instance also logging.DEBUG\n",
    "    tmtoolkit_log.setLevel(logging.INFO)\n",
    "    tmtoolkit_log.propagate = True\n",
    "\n",
    "\n",
    "### Creating a `TMPreproc` object\n",
    "\n",
    "You can create a `TMPreproc` object (also known as \"instance\") by passing a dict that maps document labels to (untokenized) documents. Since a tmtoolkit [Corpus](text_corpora.ipynb) behaves like a dict, we can pass a `Corpus` object. This time we will not use a sample but the full English news articles corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Corpus [3824 documents]>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus.from_builtin_corpus('english-NewsArticles')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass this directly to `TMPreproc`. Doing so will at first distribute all documents to several sub-processes which will later be used to run the computations in parallel. The number of sub-processes can be controlled via `n_max_processes`. It defaults to the number of CPU cores in your machine. The distribution of documents to the processes happens according to the document size. E.g. when you have two CPU cores, one very large document and three small documents, CPU 1 will take care about the large document alone and CPU 2 will take the other three small documents. After distribution of the documents, they will directly be tokenized (in parallel). Hence when you have a large corpus, the creation of a `TMPreproc` object may take some time because of the tokenization process.\n",
    "\n",
    "Let's create a `TMPreproc` object from `corpus`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TMPreproc [3824 documents]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import TMPreproc\n",
    "\n",
    "preproc = TMPreproc(corpus)\n",
    "preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important parameter is `language`, which defaults to `'english'`. So when you're working with a German corpus, you would create the object as:\n",
    "\n",
    "    preproc = TMPreproc(corpus, language='german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `TMPreproc` object `preproc` is now set up to work with the documents passed in `corpus` and the language `'english'`. All further operations with this object will use the specified documents and language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing tokens, vocabulary and other important properties\n",
    "\n",
    "`TMPreproc` provides several properties to access its data and some summary statistics. See for example the number of documents and the sum of the number of tokens in all documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3824"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2452726"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the document labels and the number of tokens in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsArticles-1',\n",
       " 'NewsArticles-10',\n",
       " 'NewsArticles-100',\n",
       " 'NewsArticles-1000',\n",
       " 'NewsArticles-1001',\n",
       " 'NewsArticles-1002',\n",
       " 'NewsArticles-1003',\n",
       " 'NewsArticles-1004',\n",
       " 'NewsArticles-1005',\n",
       " 'NewsArticles-1006']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.doc_labels[:10]  # displaying only the first 10 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying only a single document's length here\n",
    "preproc.doc_lengths['NewsArticles-1880']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are properties for vocabulary and vocabulary counts, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '#', '$', '%', '&', \"'\", \"''\", \"''We\", \"'-\", \"'-and\"]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.vocabulary[:10]  # displaying only the first 10 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115385"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how often the word \"the\" occurs in the whole corpus\n",
    "preproc.vocabulary_counts['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the document frequency for each token in the vocabulary as absolute numbers (`.vocabulary_abs_doc_frequency`) or proportions (`.vocabulary_rel_doc_frequency`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1096, 0.28661087866108786)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preproc.vocabulary_abs_doc_frequency['Trump'],\n",
    " preproc.vocabulary_rel_doc_frequency['Trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 0.043410041841004186)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preproc.vocabulary_abs_doc_frequency['Putin'],\n",
    " preproc.vocabulary_rel_doc_frequency['Putin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing document tokens\n",
    "\n",
    "The most important properties are those that start with `.tokens...`. They give access to the tokenized documents in the `TMPreproc` object in different formats.\n",
    "\n",
    "The `.tokens` property simply returns a dict mapping document labels to their tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White',\n",
       " 'House',\n",
       " 'aides',\n",
       " 'told',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'Russia-related',\n",
       " 'materials',\n",
       " 'Lawyers',\n",
       " 'for']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only showing the first ten tokens of a specific doc.\n",
    "preproc.tokens['NewsArticles-1880'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.tokens_datatable` and `.tokens_dataframe` properties return a [datatable Frame](https://github.com/h2oai/datatable/) or [pandas DataFrame](https://pandas.pydata.org/), respectively. The datatable Frame consists of at least three columns: The document label, the position of the token in the document (zero-indexed) and the token itself. Please note that for large amounts of data, `.tokens_datatable` is usually quicker than using `.tokens_dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>0</td><td>Betsy</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1</td><td>1</td><td>DeVos</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1</td><td>2</td><td>Confirmed</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1</td><td>3</td><td>as</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1</td><td>4</td><td>Education</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1</td><td>5</td><td>Secretary</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1</td><td>6</td><td>,</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1</td><td>7</td><td>With</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1</td><td>8</td><td>Pence</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1</td><td>9</td><td>Casting</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1</td><td>10</td><td>Historic</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1</td><td>11</td><td>Tie-Breaking</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1</td><td>12</td><td>Vote</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1</td><td>13</td><td>Michigan</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1</td><td>14</td><td>billionaire</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>2,452,721</td><td>NewsArticles-999</td><td>589</td><td>article</td></tr>\n",
       "    <tr><td class='row_index'>2,452,722</td><td>NewsArticles-999</td><td>590</td><td>was</td></tr>\n",
       "    <tr><td class='row_index'>2,452,723</td><td>NewsArticles-999</td><td>591</td><td>n't</td></tr>\n",
       "    <tr><td class='row_index'>2,452,724</td><td>NewsArticles-999</td><td>592</td><td>funny</td></tr>\n",
       "    <tr><td class='row_index'>2,452,725</td><td>NewsArticles-999</td><td>593</td><td>?</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2,452,726 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned pandas DataFrame from `.tokens_dataframe` has as similar layout (not shown here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More columns may be shown when you add token metadata (more on that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `TMPreproc` as a *state machine*\n",
    "\n",
    "Before we proceed with the methods that `TMPreproc` provides, we should understand how a `TMPreproc` object represents a *state* which can be changed by calling its methods. This state also determines the behavior of the object. For example, when you want to lemmatize your documents, you can call the [TMPreproc.lemmatize()](api.rst#tmtoolkit.preprocess.TMPreproc.lemmatize) method (more on that later). However, you can only use this method if you performed POS tagging via [TMPreproc.pos_tag()](api.rst#tmtoolkit.preprocess.TMPreproc.pos_tag) before, i.e. if your `TMPreproc` object's state is \"ready\" for lemmatization.\n",
    "\n",
    "A `TMPreproc` object is a complex data structure that encapsulates the data you work with (i.e. your corpus), several \"state\" variables (e.g. a variable that records whether the tokens have POS tag information), a bunch of methods that transform your data or compute something from it and, as already introduced, some properties that provide access to your data and some summary statistics.\n",
    "\n",
    "We can see how calling methods may change the data and the state of the object. For example, we can see how transforming all tokens to lowercase changes also the vocabulary and hence the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78290"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original vocabulary size\n",
    "len(preproc.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69086"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.tokens_to_lowercase()\n",
    "len(preproc.vocabulary)  # vocabulary size is now smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying `TMPreproc` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that after calling the method `tokens_to_lowercase()`, the tokens in `preproc` were transformed and the original tokens from before calling this method are not available anymore. In Python, assigning a *mutable* object to a variable binds the same object only to a different name, it doesn't copy it. Since a `TMPreproc` object is a mutable object (you can change its state by calling its methods), when we simply assign such an object to a different variable (say `preproc_upper`) we essentially only have two names for the same object and by calling a method on one of these variable names, the values will be changed for *both* names.\n",
    "\n",
    "Let's see this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_upper = preproc  # simply assignment, no copy!\n",
    "\n",
    "# we didn't change anything, so this should be true:\n",
    "preproc.vocabulary == preproc_upper.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TMPreproc [3824 documents]>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's transform the tokens to uppercase\n",
    "# we might expect that this only applies to the tokens in \"preproc_upper\"\n",
    "preproc_upper.transform_tokens(str.upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but the vocabulary is the same for both!\n",
    "preproc.vocabulary == preproc_upper.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARTICHOKES',\n",
       " 'ARTICLE',\n",
       " 'ARTICLE-IN',\n",
       " 'ARTICLE50',\n",
       " 'ARTICLES',\n",
       " 'ARTICULATE',\n",
       " 'ARTICULATED',\n",
       " 'ARTIFACTS',\n",
       " 'ARTIFICIAL',\n",
       " 'ARTIFICIALLY']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.vocabulary[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARTICHOKES',\n",
       " 'ARTICLE',\n",
       " 'ARTICLE-IN',\n",
       " 'ARTICLE50',\n",
       " 'ARTICLES',\n",
       " 'ARTICULATE',\n",
       " 'ARTICULATED',\n",
       " 'ARTIFACTS',\n",
       " 'ARTIFICIAL',\n",
       " 'ARTIFICIALLY']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_upper.vocabulary[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? As explained, by the assignment `preproc_upper = preproc` we only assigned a new name to the object behind `preproc`. Calling methods on either `preproc_upper` or `preproc` will essentially modify the same object. We can confirm that both variables point to the same object, by comparing the Python object ID via [id()](https://docs.python.org/3/library/functions.html#id):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139932303304072, 139932303304072)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(preproc), id(preproc_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is true when you assign the result of a method that returns the `TMPreproc` \"self\" object, so you have to watch out here, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, we only create another name for the same object:\n",
    "preproc_lower = preproc.tokens_to_lowercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *all* three names refer to the same object and hence to the same vocabulary\n",
    "preproc_lower.vocabulary == preproc_upper.vocabulary == preproc.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arthanayake',\n",
       " 'arthaud',\n",
       " 'arthena',\n",
       " 'arthenia',\n",
       " 'arthritic',\n",
       " 'arthritis',\n",
       " 'arthur',\n",
       " 'artichokes',\n",
       " 'article',\n",
       " 'article-in']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's all lowercase now\n",
    "preproc.vocabulary[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do about that? We need to *copy* the object which can be done with the [TMPreproc.copy()](api.rst#tmtoolkit.preprocess.TMPreproc.copy) method. By this, we create another variable that points to a separate `TMPreproc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_upper = preproc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139931125296264, 139932303304072)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the IDs confirm that we have two different objects\n",
    "id(preproc_upper), id(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_upper.transform_tokens(str.upper)\n",
    "\n",
    "# the transformation now only applied to \"preproc_upper\"\n",
    "preproc.vocabulary == preproc_upper.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARTICHOKES',\n",
       " 'ARTICLE',\n",
       " 'ARTICLE-IN',\n",
       " 'ARTICLE50',\n",
       " 'ARTICLES',\n",
       " 'ARTICULATE',\n",
       " 'ARTICULATED',\n",
       " 'ARTIFACTS',\n",
       " 'ARTIFICIAL',\n",
       " 'ARTIFICIALLY']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_upper.vocabulary[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this also uses up twice as much computer memory now. So you shouldn't create copies that often and also release unused memory by using `del`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the objects again\n",
    "del preproc_upper, preproc_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization: Saving and loading `TMPreproc` objects\n",
    "\n",
    "The current state of a `TMPreproc` object can also be stored to a file on disk so that you (or someone else who has tmtoolkit installed) can later restore it using that file. The methods for that are [TMPreproc.save_state()](api.rst#tmtoolkit.preprocess.TMPreproc.save_state) and [TMPreproc.load_state()](api.rst#tmtoolkit.preprocess.TMPreproc.load_state) / [TMPreproc.from_state()](api.rst#tmtoolkit.preprocess.TMPreproc.from_state).\n",
    "\n",
    "Let's store the current state of the `preproc`, which has all tokens transformed to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TMPreproc [3824 documents]>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.save_state('data/preproc_lowercase.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the object by retaining only documents that contain the token \"trump\" (see the reduced number of documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TMPreproc [1097 documents]>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.filter_documents('trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restore the saved data using [TMPreproc.from_state()](api.rst#tmtoolkit.preprocess.TMPreproc.from_state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TMPreproc [3824 documents]>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_full = TMPreproc.from_state('data/preproc_lowercase.pickle')\n",
    "preproc_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very useful especially when you have a large amount of data and run time consuming operations, e.g. POS tagging. When you're finished running these operations, you can easily store the current state to disk and later retrieve it without the need to re-run these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "All functions from the functional API are also available as `TMPreproc` methods, most carrying the same name. Additional functionality comes in the form of token metadata handling, which will be the first topic in the next section.\n",
    "\n",
    "Before starting to explore the `TMPreproc` methods, we'll re-create a fresh `TMPreproc` object from the NewsArticles corpus and make a copy of it in order to be able to revert to that state later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TMPreproc [3824 documents]>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc = TMPreproc(corpus)\n",
    "preproc_orig = preproc.copy()\n",
    "preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with token metadata / POS tagging\n",
    "\n",
    "`TMPreproc` allows to attach arbitrary metadata to each token in each document. This kind of \"annotations\" for tokens is very useful. For example, you may add metadata that records a token's length or whether it is all uppercase letters and later use that for filtering or in further analyses. One function to add such metadata is [add_metadata_per_doc()](api.rst#tmtoolkit.preprocess.TMPreproc.add_metadata_per_doc). This function requires to pass a dict that maps document labels to the respective token metadata list. The list's length must match the number of tokens in the respective document. At first we need to create such a metadata dict. Let's do that for the tokens' length first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('White', 5),\n",
       " ('House', 5),\n",
       " ('aides', 5),\n",
       " ('told', 4),\n",
       " ('to', 2),\n",
       " ('keep', 4),\n",
       " ('Russia-related', 14),\n",
       " ('materials', 9),\n",
       " ('Lawyers', 7),\n",
       " ('for', 3)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_tok_lengths = {doc_label: list(map(len, doc_tokens))\n",
    "                    for doc_label, doc_tokens in preproc.tokens.items()}\n",
    "\n",
    "# show first 5 tokens and their string length for a sample document\n",
    "list(zip(preproc.tokens['NewsArticles-1880'][:10],\n",
    "         meta_tok_lengths['NewsArticles-1880'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add these metadata via [add_metadata_per_doc()](api.rst#tmtoolkit.preprocess.TMPreproc.add_metadata_per_doc). We pass a label, the metadata key, and the previously generated metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.add_metadata_per_doc('length', meta_tok_lengths)\n",
    "del meta_tok_lengths  # we don't need that object anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property `.tokens_datatable` now shows an additional column with `meta_token` (the metadata key in always prefixed with `meta_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>0</td><td>Betsy</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1</td><td>1</td><td>DeVos</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1</td><td>2</td><td>Confirmed</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1</td><td>3</td><td>as</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1</td><td>4</td><td>Education</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1</td><td>5</td><td>Secretary</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1</td><td>6</td><td>,</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1</td><td>7</td><td>With</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1</td><td>8</td><td>Pence</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1</td><td>9</td><td>Casting</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1</td><td>10</td><td>Historic</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1</td><td>11</td><td>Tie-Breaking</td><td>12</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1</td><td>12</td><td>Vote</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1</td><td>13</td><td>Michigan</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1</td><td>14</td><td>billionaire</td><td>11</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>2,452,721</td><td>NewsArticles-999</td><td>589</td><td>article</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>2,452,722</td><td>NewsArticles-999</td><td>590</td><td>was</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2,452,723</td><td>NewsArticles-999</td><td>591</td><td>n't</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2,452,724</td><td>NewsArticles-999</td><td>592</td><td>funny</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2,452,725</td><td>NewsArticles-999</td><td>593</td><td>?</td><td>1</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2,452,726 rows &times; 4 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a boolean indicator for whether the given token is all uppercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_upper</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='boolean' title='bool8'>&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>0</td><td>Betsy</td><td>0</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1</td><td>1</td><td>DeVos</td><td>0</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1</td><td>2</td><td>Confirmed</td><td>0</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1</td><td>3</td><td>as</td><td>0</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1</td><td>4</td><td>Education</td><td>0</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1</td><td>5</td><td>Secretary</td><td>0</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1</td><td>6</td><td>,</td><td>0</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1</td><td>7</td><td>With</td><td>0</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1</td><td>8</td><td>Pence</td><td>0</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1</td><td>9</td><td>Casting</td><td>0</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1</td><td>10</td><td>Historic</td><td>0</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1</td><td>11</td><td>Tie-Breaking</td><td>0</td><td>12</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1</td><td>12</td><td>Vote</td><td>0</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1</td><td>13</td><td>Michigan</td><td>0</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1</td><td>14</td><td>billionaire</td><td>0</td><td>11</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>2,452,721</td><td>NewsArticles-999</td><td>589</td><td>article</td><td>0</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>2,452,722</td><td>NewsArticles-999</td><td>590</td><td>was</td><td>0</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2,452,723</td><td>NewsArticles-999</td><td>591</td><td>n't</td><td>0</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2,452,724</td><td>NewsArticles-999</td><td>592</td><td>funny</td><td>0</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2,452,725</td><td>NewsArticles-999</td><td>593</td><td>?</td><td>0</td><td>1</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2,452,726 rows &times; 5 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_tok_upper = {doc_label: list(map(str.isupper, doc_tokens))\n",
    "                  for doc_label, doc_tokens in preproc.tokens.items()}\n",
    "\n",
    "preproc.add_metadata_per_doc('upper', meta_tok_upper)\n",
    "del meta_tok_upper\n",
    "\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use these newly added columns now for example for filtering the datatable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_upper</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='boolean' title='bool8'>&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>466</td><td>ABC</td><td>1</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-10</td><td>10</td><td>A</td><td>1</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-10</td><td>109</td><td>U.S</td><td>1</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-10</td><td>225</td><td>ABC</td><td>1</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-10</td><td>227</td><td>WEAR</td><td>1</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-10</td><td>290</td><td>AP</td><td>1</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-10</td><td>373</td><td>9613BJ</td><td>1</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-100</td><td>97</td><td>UK</td><td>1</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-100</td><td>108</td><td>UK</td><td>1</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-100</td><td>326</td><td>C</td><td>1</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-100</td><td>559</td><td>A</td><td>1</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-100</td><td>581</td><td>UK</td><td>1</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1000</td><td>11</td><td>A</td><td>1</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1000</td><td>26</td><td>A</td><td>1</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1000</td><td>123</td><td>A</td><td>1</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>36,844</td><td>NewsArticles-999</td><td>490</td><td>U.S</td><td>1</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>36,845</td><td>NewsArticles-999</td><td>495</td><td>LTE</td><td>1</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>36,846</td><td>NewsArticles-999</td><td>515</td><td>4G</td><td>1</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>36,847</td><td>NewsArticles-999</td><td>567</td><td>22-28GB</td><td>1</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>36,848</td><td>NewsArticles-999</td><td>575</td><td>FCC</td><td>1</td><td>3</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>36,849 rows &times; 5 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datatable as dt\n",
    "\n",
    "preproc.tokens_datatable[dt.f.meta_upper == 1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[POS tagging](#Part-of-speech-(POS)-tagging) is also a way of annotating tokens in `TMPreproc`. When you run the method [pos_tag()](api.rst#tmtoolkit.preprocess.TMPreproc.pos_tag), a new metadata column `meta_pos` is added. We can try that out now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_upper</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='boolean' title='bool8'>&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>0</td><td>Betsy</td><td>0</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1</td><td>1</td><td>DeVos</td><td>0</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1</td><td>2</td><td>Confirmed</td><td>0</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1</td><td>3</td><td>as</td><td>0</td><td>IN</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1</td><td>4</td><td>Education</td><td>0</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1</td><td>5</td><td>Secretary</td><td>0</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1</td><td>6</td><td>,</td><td>0</td><td>,</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1</td><td>7</td><td>With</td><td>0</td><td>IN</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1</td><td>8</td><td>Pence</td><td>0</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1</td><td>9</td><td>Casting</td><td>0</td><td>NNP</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1</td><td>10</td><td>Historic</td><td>0</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1</td><td>11</td><td>Tie-Breaking</td><td>0</td><td>NNP</td><td>12</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1</td><td>12</td><td>Vote</td><td>0</td><td>NNP</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1</td><td>13</td><td>Michigan</td><td>0</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1</td><td>14</td><td>billionaire</td><td>0</td><td>POS</td><td>11</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>2,452,721</td><td>NewsArticles-999</td><td>589</td><td>article</td><td>0</td><td>NN</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>2,452,722</td><td>NewsArticles-999</td><td>590</td><td>was</td><td>0</td><td>VBD</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2,452,723</td><td>NewsArticles-999</td><td>591</td><td>n't</td><td>0</td><td>RB</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2,452,724</td><td>NewsArticles-999</td><td>592</td><td>funny</td><td>0</td><td>JJ</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2,452,725</td><td>NewsArticles-999</td><td>593</td><td>?</td><td>0</td><td>.</td><td>1</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2,452,726 rows &times; 6 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.pos_tag()\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a new column `meta_pos` with the POS tags for each token was introduced.\n",
    "\n",
    "To see which metadata keys are available, you can use [get_available_metadata_keys()](api.rst#tmtoolkit.preprocess.TMPreproc.get_available_metadata_keys):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length', 'pos', 'upper'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.get_available_metadata_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token metadata can be removed with [remove_metadata()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_metadata):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length', 'pos'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.remove_metadata('upper')\n",
    "preproc.get_available_metadata_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section on [filtering](#Filtering) will later show how to use metadata to filter tokens and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token transformations\n",
    "\n",
    "As already said, `TMPreproc` provides the same functionality as the functional API. Token transformations like stemming, lemmatization, lowercase transformation, etc. can be applied step-by-step. We will show a typical pre-processing pipeline consisting of:\n",
    "\n",
    "1. lemmatization (which we can apply because we already POS-tagged our tokens)\n",
    "2. lowercase transformation\n",
    "3. token cleaning\n",
    "4. removal of very common and very uncommon tokens\n",
    "\n",
    "Let's start with the [lemmatize()](api.rst#tmtoolkit.preprocess.TMPreproc.lemmatize) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>0</td><td>Betsy</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1</td><td>1</td><td>DeVos</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1</td><td>2</td><td>Confirmed</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1</td><td>3</td><td>as</td><td>IN</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1</td><td>4</td><td>Education</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1</td><td>5</td><td>Secretary</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1</td><td>6</td><td>,</td><td>,</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1</td><td>7</td><td>With</td><td>IN</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1</td><td>8</td><td>Pence</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1</td><td>9</td><td>Casting</td><td>NNP</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1</td><td>10</td><td>Historic</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1</td><td>11</td><td>Tie-Breaking</td><td>NNP</td><td>12</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1</td><td>12</td><td>Vote</td><td>NNP</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1</td><td>13</td><td>Michigan</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1</td><td>14</td><td>billionaire</td><td>POS</td><td>11</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>2,452,721</td><td>NewsArticles-999</td><td>589</td><td>article</td><td>NN</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>2,452,722</td><td>NewsArticles-999</td><td>590</td><td>be</td><td>VBD</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2,452,723</td><td>NewsArticles-999</td><td>591</td><td>n't</td><td>RB</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2,452,724</td><td>NewsArticles-999</td><td>592</td><td>funny</td><td>JJ</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2,452,725</td><td>NewsArticles-999</td><td>593</td><td>?</td><td>.</td><td>1</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2,452,726 rows &times; 5 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.lemmatize()\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with the pipeline and employ \"method chaining\": You can apply several methods one after another by chaining them with a `.` as long as this method returns a `TMPreproc` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>0</td><td>betsy</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1</td><td>1</td><td>devos</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1</td><td>2</td><td>confirmed</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1</td><td>3</td><td>education</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1</td><td>4</td><td>secretary</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1</td><td>5</td><td>pence</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1</td><td>6</td><td>casting</td><td>NNP</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1</td><td>7</td><td>historic</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1</td><td>8</td><td>tie-breaking</td><td>NNP</td><td>12</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1</td><td>9</td><td>vote</td><td>NNP</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1</td><td>10</td><td>michigan</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1</td><td>11</td><td>billionaire</td><td>POS</td><td>11</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1</td><td>12</td><td>education</td><td>NN</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1</td><td>13</td><td>activist</td><td>NN</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1</td><td>14</td><td>betsy</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>1,313,679</td><td>NewsArticles-999</td><td>275</td><td>away</td><td>RB</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>1,313,680</td><td>NewsArticles-999</td><td>276</td><td>think</td><td>VBD</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>1,313,681</td><td>NewsArticles-999</td><td>277</td><td>article</td><td>NN</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>1,313,682</td><td>NewsArticles-999</td><td>278</td><td>n't</td><td>RB</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>1,313,683</td><td>NewsArticles-999</td><td>279</td><td>funny</td><td>JJ</td><td>5</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>1,313,684 rows &times; 5 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.tokens_to_lowercase().clean_tokens(remove_numbers=True)\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>0</td><td>betsy</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1</td><td>1</td><td>devos</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1</td><td>2</td><td>education</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1</td><td>3</td><td>secretary</td><td>NNP</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1</td><td>4</td><td>pence</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1</td><td>5</td><td>historic</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1</td><td>6</td><td>vote</td><td>NNP</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1</td><td>7</td><td>michigan</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1</td><td>8</td><td>billionaire</td><td>POS</td><td>11</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1</td><td>9</td><td>education</td><td>NN</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1</td><td>10</td><td>activist</td><td>NN</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1</td><td>11</td><td>betsy</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1</td><td>12</td><td>devos</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1</td><td>13</td><td>confirm</td><td>VBN</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1</td><td>14</td><td>today</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>1,183,399</td><td>NewsArticles-999</td><td>219</td><td>away</td><td>RB</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>1,183,400</td><td>NewsArticles-999</td><td>220</td><td>think</td><td>VBD</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>1,183,401</td><td>NewsArticles-999</td><td>221</td><td>article</td><td>NN</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>1,183,402</td><td>NewsArticles-999</td><td>222</td><td>n't</td><td>RB</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>1,183,403</td><td>NewsArticles-999</td><td>223</td><td>funny</td><td>JJ</td><td>5</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>1,183,404 rows &times; 5 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.remove_common_tokens(0.9).remove_uncommon_tokens(5, absolute=True)\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a look at the vocabulary size and compare it with the unprocessed data, we can see that we greatly reduced the amount of unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11250, 78290)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preproc.vocabulary), len(preproc_orig.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering\n",
    "\n",
    "Filtering also works the same as with the functional API, i.e. methods like [filter_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens) or [filter_documents()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_documents) are available. We will now focus on filtering with metadata.\n",
    "\n",
    "We can tell [filter_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens) and similar methods to use metadata instead of the tokens for matching. For example, we can use the metadata `meta_length`, which we created in the [metadata section](#Working-with-token-metadata-/-POS-tagging) to filter for tokens of a certain length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1</td><td>0</td><td>use</td><td>VB</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1</td><td>1</td><td>tie</td><td>NN</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1</td><td>2</td><td>day</td><td>NN</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1</td><td>3</td><td>one</td><td>CD</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1</td><td>4</td><td>sen</td><td>NNP</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1</td><td>5</td><td>law</td><td>NN</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1</td><td>6</td><td>van</td><td>NNP</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1</td><td>7</td><td>two</td><td>CD</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1</td><td>8</td><td>abc</td><td>NNP</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-10</td><td>0</td><td>run</td><td>NN</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-10</td><td>1</td><td>may</td><td>MD</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-10</td><td>2</td><td>u.s</td><td>NNP</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-10</td><td>3</td><td>abc</td><td>NNP</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-10</td><td>4</td><td>say</td><td>VBP</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-10</td><td>5</td><td>duo</td><td>NN</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>70,182</td><td>NewsArticles-999</td><td>19</td><td>n't</td><td>RB</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>70,183</td><td>NewsArticles-999</td><td>20</td><td>new</td><td>JJ</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>70,184</td><td>NewsArticles-999</td><td>21</td><td>n't</td><td>RB</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>70,185</td><td>NewsArticles-999</td><td>22</td><td>new</td><td>JJ</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>70,186</td><td>NewsArticles-999</td><td>23</td><td>n't</td><td>RB</td><td>3</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>70,187 rows &times; 5 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.filter_tokens(3, by_meta='length')\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all matching options then apply to the metadata column, in this case to the `meta_length` column which contains integers. Since `filter_tokens()` by default employs exact matching, we get all tokens where `meta_length` equals the first argument, `3`. If we used regular expression or glob matching instead, this method would fail because you can only use that for string data.\n",
    "\n",
    "If you want to use more complex filter queries, you should create a \"filter mask\" and pass it to [filter_tokens_by_mask()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens_by_mask). A filter mask is a dictionary that maps a document label to a sequence of booleans. For all occurrences of `True`, the respective token in the document will be retained, all others will be removed. Let's try that out with a small sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1728</td><td>0</td><td>Trump</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1728</td><td>1</td><td>:</td><td>:</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1728</td><td>2</td><td>Agency</td><td>NN</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1728</td><td>3</td><td>to</td><td>TO</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1728</td><td>4</td><td>support</td><td>VB</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1728</td><td>5</td><td>'victims</td><td>NNS</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1728</td><td>6</td><td>of</td><td>IN</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1728</td><td>7</td><td>immigrant</td><td>JJ</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1728</td><td>8</td><td>crimes'</td><td>NN</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1728</td><td>9</td><td>In</td><td>IN</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1728</td><td>10</td><td>first</td><td>JJ</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1728</td><td>11</td><td>speech</td><td>NN</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1728</td><td>12</td><td>to</td><td>TO</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1728</td><td>13</td><td>Congress</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1728</td><td>14</td><td>,</td><td>,</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>2570</td><td>NewsArticles-948</td><td>332</td><td>.</td><td>.</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>2571</td><td>NewsArticles-948</td><td>333</td><td>Source</td><td>NN</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>2572</td><td>NewsArticles-948</td><td>334</td><td>:</td><td>:</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>2573</td><td>NewsArticles-948</td><td>335</td><td>-News</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2574</td><td>NewsArticles-948</td><td>336</td><td>agencies</td><td>NNS</td><td>8</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2575 rows &times; 5 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_small = TMPreproc(corpus.sample(5))\n",
    "meta_tok_lengths = {doc_label: list(map(len, doc_tokens))\n",
    "                    for doc_label, doc_tokens in preproc_small.tokens.items()}\n",
    "preproc_small.pos_tag().add_metadata_per_doc('length', meta_tok_lengths)\n",
    "preproc_small.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate the filter mask, which means for each document we create a boolean list or array that for each token in that document indicates whether that token should be kept or removed.\n",
    "\n",
    "We will iterate through the [.tokens_with_metadata](api.rst#tmtoolkit.preprocess.TMPreproc.tokens_with_metadata) property which is a dict that for each document contains a datatable with its tokens and metadata. Let's have a look at the first document's datatable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>token</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>Ex-footballer</td><td>NNP</td><td>13</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>Adam</td><td>NNP</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>Johnson</td><td>NNP</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>loses</td><td>VBZ</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>appeal</td><td>JJ</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>Ex-England</td><td>NNP</td><td>10</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>footballer</td><td>NN</td><td>10</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>Adam</td><td>NNP</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>Johnson</td><td>NNP</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>has</td><td>VBZ</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>lost</td><td>VBN</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>a</td><td>DT</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>Court</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>of</td><td>IN</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>Appeal</td><td>NNP</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>134</td><td>to</td><td>TO</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>135</td><td>another</td><td>DT</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>136</td><td>sexual</td><td>JJ</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>137</td><td>act</td><td>NN</td><td>3</td></tr>\n",
       "    <tr><td class='row_index'>138</td><td>.</td><td>.</td><td>1</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>139 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(preproc_small.tokens_with_metadata.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the filter mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_small_nouns</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='boolean' title='bool8'>&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1728</td><td>0</td><td>Trump</td><td>1</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1728</td><td>1</td><td>:</td><td>0</td><td>:</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1728</td><td>2</td><td>Agency</td><td>0</td><td>NN</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1728</td><td>3</td><td>to</td><td>0</td><td>TO</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1728</td><td>4</td><td>support</td><td>0</td><td>VB</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1728</td><td>5</td><td>'victims</td><td>0</td><td>NNS</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1728</td><td>6</td><td>of</td><td>0</td><td>IN</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1728</td><td>7</td><td>immigrant</td><td>0</td><td>JJ</td><td>9</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1728</td><td>8</td><td>crimes'</td><td>0</td><td>NN</td><td>7</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1728</td><td>9</td><td>In</td><td>0</td><td>IN</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1728</td><td>10</td><td>first</td><td>0</td><td>JJ</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1728</td><td>11</td><td>speech</td><td>0</td><td>NN</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1728</td><td>12</td><td>to</td><td>0</td><td>TO</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1728</td><td>13</td><td>Congress</td><td>0</td><td>NNP</td><td>8</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1728</td><td>14</td><td>,</td><td>0</td><td>,</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>2570</td><td>NewsArticles-948</td><td>332</td><td>.</td><td>0</td><td>.</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>2571</td><td>NewsArticles-948</td><td>333</td><td>Source</td><td>0</td><td>NN</td><td>6</td></tr>\n",
       "    <tr><td class='row_index'>2572</td><td>NewsArticles-948</td><td>334</td><td>:</td><td>0</td><td>:</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>2573</td><td>NewsArticles-948</td><td>335</td><td>-News</td><td>1</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>2574</td><td>NewsArticles-948</td><td>336</td><td>agencies</td><td>0</td><td>NNS</td><td>8</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>2575 rows &times; 6 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filter_mask = {}\n",
    "for doc_label, doc_data in preproc_small.tokens_with_metadata.items():\n",
    "    # extract the columns \"meta_length\" and \"meta_pos\"\n",
    "    # and convert them to NumPy arrays\n",
    "    doc_data_subset = doc_data[:, [dt.f.meta_length, dt.f.meta_pos]]\n",
    "    tok_lengths, tok_pos = map(np.array, doc_data_subset.to_list())\n",
    "    \n",
    "    # create a boolean array for nouns with token length less or equal 5\n",
    "    filter_mask[doc_label] = (tok_lengths <= 5) & np.char.startswith(tok_pos, 'N')\n",
    "\n",
    "# it's not necessary to add the filter mask as metadata\n",
    "# but it's a good way to check the mask\n",
    "preproc_small.add_metadata_per_doc('small_nouns', filter_mask)\n",
    "preproc_small.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can pass the mask dict to [filter_tokens_by_mask()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens_by_mask):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>position</th><th>token</th><th>meta_small_nouns</th><th>meta_pos</th><th>meta_length</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='boolean' title='bool8'>&#x25AA;</td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1728</td><td>0</td><td>Trump</td><td>1</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-1728</td><td>1</td><td>Path</td><td>1</td><td>NN</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-1728</td><td>2</td><td>wall'</td><td>1</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-1728</td><td>3</td><td>Trump</td><td>1</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-1728</td><td>4</td><td>crime</td><td>1</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>NewsArticles-1728</td><td>5</td><td>VOICE</td><td>1</td><td>NN</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>NewsArticles-1728</td><td>6</td><td>Crime</td><td>1</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>NewsArticles-1728</td><td>7</td><td>VOICE</td><td>1</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>NewsArticles-1728</td><td>8</td><td>list</td><td>1</td><td>NN</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>NewsArticles-1728</td><td>9</td><td>US</td><td>1</td><td>NNP</td><td>2</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>NewsArticles-1728</td><td>10</td><td>name</td><td>1</td><td>NN</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>NewsArticles-1728</td><td>11</td><td>Trump</td><td>1</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>NewsArticles-1728</td><td>12</td><td>name</td><td>1</td><td>NN</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>NewsArticles-1728</td><td>13</td><td>READ</td><td>1</td><td>NNP</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>NewsArticles-1728</td><td>14</td><td>Trump</td><td>1</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>254</td><td>NewsArticles-948</td><td>40</td><td>flow</td><td>1</td><td>NN</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>255</td><td>NewsArticles-948</td><td>41</td><td>goods</td><td>1</td><td>NNS</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>256</td><td>NewsArticles-948</td><td>42</td><td>Egypt</td><td>1</td><td>NNP</td><td>5</td></tr>\n",
       "    <tr><td class='row_index'>257</td><td>NewsArticles-948</td><td>43</td><td>Gaza</td><td>1</td><td>NNP</td><td>4</td></tr>\n",
       "    <tr><td class='row_index'>258</td><td>NewsArticles-948</td><td>44</td><td>-News</td><td>1</td><td>NN</td><td>5</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>259 rows &times; 6 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_small.filter_tokens_by_mask(filter_mask)\n",
    "preproc_small.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all the functions that you know from the functional API are also available for `TMPreproc` and they work exactly the same, so we won't replicate that here. Make sure to have a look at the [API](api.rst#tmpreproc-class-for-parallel-text-preprocessing) to get an overview about `TMPreproc`'s methods and properties. For the final section, we only want to focus on generating a sparse document-term matrix (DTM). There is a property [.dtm](api.rst#tmtoolkit.preprocess.TMPreproc.dtm) that generates and returns a sparse DTM from the tokens of a `TMPreproc` object. First, let's check the number of documents and vocabulary size which should determine the shape of the DTM that we will create afterwards. We will continue working with `preproc_small`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 137)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preproc_small.n_docs, len(preproc_small.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x137 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 158 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_small = preproc_small.dtm\n",
    "dtm_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the DTM has the correct shape. The method [get_dtm()](api.rst#tmtoolkit.preprocess.TMPreproc.get_dtm) also allows to return the result as datatable or pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>_doc</th><th>'We</th><th>-Al</th><th>-News</th><th>A</th><th>A-321</th><th>Adam</th><th>Air</th><th>Al</th><th>Bays</th><th class='vellipsis'>&hellip;</th><th>wages</th><th>wall</th><th>wall'</th><th>years</th><th>zone</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='string' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td></td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='integer' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>NewsArticles-1728</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>2</td><td class=vellipsis>&hellip;</td><td>3</td><td>1</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>NewsArticles-2162</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>NewsArticles-2616</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>NewsArticles-2902</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>NewsArticles-948</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>5 rows &times; 138 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": []
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_small.get_dtm(as_datatable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [bow module](api.rst#tmtoolkit-bow) contains several functions to work with DTMs, e.g. apply transformations such as *tf-idf* or compute some important summary statistics. The [next chapter](bow.ipynb) will introduce some of these functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
