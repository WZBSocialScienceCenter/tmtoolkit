{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing and basic text mining\n",
    "\n",
    "During text preprocessing, a corpus of documents is tokenized (i.e. the document strings are split into individual words, punctuation, numbers, etc.) and then these tokens can be transformed, filtered or annotated. The goal is to prepare the raw texts in a way that makes it easier to perform eventual analysis methods in a later stage, e.g. by reducing noise in the dataset. tmtoolkit provides a rich set of tools for this purpose implemented as *corpus functions* in the [tmtoolkit.corpus](api.rst#tmtoolkit-corpus) module.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Reminder: Corpus functions\n",
    "\n",
    "All *corpus functions* accept a [`Corpus`](api.rst#TODO) object as first argument and operate on it. A corpus function may retrieve information from a corpus and/or modify the corpus object.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: enabling logging output\n",
    "\n",
    "By default, tmtoolkit does not expose any internal logging messages. Sometimes, for example for diagnostic output during debugging or in order to see progress for long running operations, it's helpful to enable logging output display. For that, you can use the [`enable_logging`](api.rst#TODO) function. By default, it enables logging to console for the `INFO` level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.utils import enable_logging, disable_logging\n",
    "\n",
    "enable_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading example data\n",
    "\n",
    "Let's load a sample of ten documents from the built-in *NewsArticles* dataset. We'll use only a small number of documents here to have a better overview at the beginning. We can later use a larger sample. To apply sampling right at the beginning when loading the data, we pass the `sample=100` parameter to the [`from_builtin_corpus`](api.rst#TODO) class method. We also use [`print_summary`](api.rst#TODO) like shown in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 16:55:45,184:INFO:tmtoolkit:creating Corpus instance with no documents\n",
      "2022-01-31 16:55:45,184:INFO:tmtoolkit:using serial processing\n",
      "2022-01-31 16:55:45,665:INFO:tmtoolkit:sampling 100 documents(s) out of 3824\n",
      "2022-01-31 16:55:45,669:INFO:tmtoolkit:adding text from 100 documents(s)\n",
      "2022-01-31 16:55:51,618:INFO:tmtoolkit:generating document texts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): DOJ : 2 Russian spies indicted in Yahoo hack    Wa...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in Dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): Dutch election : High turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): Trump vs. America : The fight for democracy    Fri...\n",
      "> NewsArticles-469 (398 tokens): Warning of tight times ahead for insurers    Analy...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode releases ' Spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): Grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seeks more time on Trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): In Breakthrough Discovery , Scientists Mass - Prod...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 9223\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(20220119)   # to make the sampling reproducible\n",
    "\n",
    "from tmtoolkit.corpus import Corpus, print_summary\n",
    "\n",
    "corpus_small = Corpus.from_builtin_corpus('en-NewsArticles', sample=100)\n",
    "print_summary(corpus_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing tokens and token attributes\n",
    "\n",
    "We start with accessing the documents' tokens and their *token attributes* using [`doc_tokens`](api.rst#TODO) and [`tokens_table`](api.rst#TODO). Token attributes are meta information attached to each token. These can be linguistic features, such as the Part of Speech (POS) tag, indicators for stopwords or punctuation, etc. The default attributes are a subset of [SpaCy's token attributes](https://spacy.io/api/token#attributes). You can configure which of these attributes are stored using the `spacy_token_attrs` parameter of the [`Corpus`](api.rst#TODO) constructor. You can also add your own token attributes. This will be shown later on.\n",
    "\n",
    "At first we load the tokens along with their attributes via `doc_tokens`, which gives us a dictionary mapping document labels to document data. Each document data is another dictionary that contains the tokens and their attributes. We start by checking which token attributes are loaded by default in any document (here, we use `'NewsArticles-2433'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token', 'is_punct', 'is_stop', 'like_num', 'tag', 'pos', 'lemma'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import doc_tokens, tokens_table\n",
    "\n",
    "# with_attr=True adds default set of token attributes\n",
    "tok = doc_tokens(corpus_small, with_attr=True)\n",
    "tok['NewsArticles-2433'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each document's data can be accessed like in the example above and it will contain the seven data entries listed above. The `'token'` entry gives the actual tokens of the document. Let's show the first five tokens for a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOJ', ':', '2', 'Russian', 'spies']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok['NewsArticles-2433']['token'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other entries are the attributes corresponding to each token. Here, we display the first five lemmata for the same document and the first five punctuation indicator values. The colon is correctly identified as punctuation character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doj', ':', '2', 'russian', 'spy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok['NewsArticles-2433']['lemma'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False, False, False]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok['NewsArticles-2433']['is_punct'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your NLP pipeline performs sentence recognition, you can pass the parameter `sentences=True` which will add another level to the output representing sentences. This means that for each item like `'token'`, `'lemma'`, etc. we will get a list of sentences. For example, the following will print the tokens of the eighth sentences (index 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Justice',\n",
       " 'Department',\n",
       " 'official',\n",
       " 'said',\n",
       " 'the',\n",
       " 'agency',\n",
       " 'has',\n",
       " 'not',\n",
       " 'confirmed',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'same',\n",
       " 'person',\n",
       " 'and',\n",
       " 'declined',\n",
       " 'further',\n",
       " 'comment',\n",
       " 'to',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sents = doc_tokens(corpus_small, sentences=True, with_attr=True)\n",
    "tok_sents['NewsArticles-2433']['token'][7]   # index 7 means 8th sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more compact overview, it's better to use the [`tokens_table`](api.rst#TODO) function. This will generate a [pandas DataFrame](https://pandas.pydata.org/) from the documents in the corpus and it will be default include all token attributes, along with a column for the document label (`doc`) and the token position inside the document (`position`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>President</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>President</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>says</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>say</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>has</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>have</td>\n",
       "      <td>False</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59593</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>282</td>\n",
       "      <td>priorities</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>priority</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59594</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>283</td>\n",
       "      <td>for</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>for</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59595</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>284</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59596</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>285</td>\n",
       "      <td>nation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>nation</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59597</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>286</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59598 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position       token  is_punct  is_stop      lemma  \\\n",
       "0      NewsArticles-1100         0   President     False    False  President   \n",
       "1      NewsArticles-1100         1       Trump     False    False      Trump   \n",
       "2      NewsArticles-1100         2        says     False    False        say   \n",
       "3      NewsArticles-1100         3          he     False     True         he   \n",
       "4      NewsArticles-1100         4         has     False     True       have   \n",
       "...                  ...       ...         ...       ...      ...        ...   \n",
       "59593   NewsArticles-960       282  priorities     False    False   priority   \n",
       "59594   NewsArticles-960       283         for     False     True        for   \n",
       "59595   NewsArticles-960       284         the     False     True        the   \n",
       "59596   NewsArticles-960       285      nation     False    False     nation   \n",
       "59597   NewsArticles-960       286           .      True    False          .   \n",
       "\n",
       "       like_num    pos  tag  \n",
       "0         False  PROPN  NNP  \n",
       "1         False  PROPN  NNP  \n",
       "2         False   VERB  VBZ  \n",
       "3         False   PRON  PRP  \n",
       "4         False    AUX  VBZ  \n",
       "...         ...    ...  ...  \n",
       "59593     False   NOUN  NNS  \n",
       "59594     False    ADP   IN  \n",
       "59595     False    DET   DT  \n",
       "59596     False   NOUN   NN  \n",
       "59597     False  PUNCT    .  \n",
       "\n",
       "[59598 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = tokens_table(corpus_small)\n",
    "tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use all sorts of filtering operations on this dataframe. See the [pandas documentation](https://pandas.pydata.org/docs/user_guide/indexing.html) for details. Here, we select all tokens that were identified as \"number-like\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>64</td>\n",
       "      <td>fifteen</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>fifteen</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>96</td>\n",
       "      <td>one</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>one</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>104</td>\n",
       "      <td>four</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>four</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>535</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>563</td>\n",
       "      <td>four</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>four</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59253</th>\n",
       "      <td>NewsArticles-901</td>\n",
       "      <td>856</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59256</th>\n",
       "      <td>NewsArticles-901</td>\n",
       "      <td>859</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59374</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>63</td>\n",
       "      <td>2021</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59400</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>89</td>\n",
       "      <td>2010</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2010</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59413</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>102</td>\n",
       "      <td>1,550</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1,550</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position    token  is_punct  is_stop    lemma  \\\n",
       "288    NewsArticles-1119        64  fifteen     False     True  fifteen   \n",
       "320    NewsArticles-1119        96      one     False     True      one   \n",
       "328    NewsArticles-1119       104     four     False     True     four   \n",
       "759    NewsArticles-1119       535      100     False    False      100   \n",
       "787    NewsArticles-1119       563     four     False     True     four   \n",
       "...                  ...       ...      ...       ...      ...      ...   \n",
       "59253   NewsArticles-901       856       85     False    False       85   \n",
       "59256   NewsArticles-901       859        9     False    False        9   \n",
       "59374   NewsArticles-960        63     2021     False    False     2021   \n",
       "59400   NewsArticles-960        89     2010     False    False     2010   \n",
       "59413   NewsArticles-960       102    1,550     False    False    1,550   \n",
       "\n",
       "       like_num  pos tag  \n",
       "288        True  NUM  CD  \n",
       "320        True  NUM  CD  \n",
       "328        True  NUM  CD  \n",
       "759        True  NUM  CD  \n",
       "787        True  NUM  CD  \n",
       "...         ...  ...  ..  \n",
       "59253      True  NUM  CD  \n",
       "59256      True  NUM  CD  \n",
       "59374      True  NUM  CD  \n",
       "59400      True  NUM  CD  \n",
       "59413      True  NUM  CD  \n",
       "\n",
       "[1139 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl[tbl.like_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This however only filters the table output. We will later see how to filter corpus documents and tokens.\n",
    "\n",
    "If you want to generate the table only for a subset of documents, you can use the `select` parameter and provide one or more document labels. Similar to that, you can use the `with_attr` parameter to list only a subset of the token attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sent</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Russian</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>spies</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>837</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>838</td>\n",
       "      <td>reflect</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>839</td>\n",
       "      <td>new</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>840</td>\n",
       "      <td>developments</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>841</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>842 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   doc  sent  position         token    pos\n",
       "0    NewsArticles-2433     0         0           DOJ   NOUN\n",
       "1    NewsArticles-2433     0         1             :  PUNCT\n",
       "2    NewsArticles-2433     0         2             2    NUM\n",
       "3    NewsArticles-2433     0         3       Russian    ADJ\n",
       "4    NewsArticles-2433     0         4         spies   NOUN\n",
       "..                 ...   ...       ...           ...    ...\n",
       "837  NewsArticles-2433    27       837            to   PART\n",
       "838  NewsArticles-2433    27       838       reflect   VERB\n",
       "839  NewsArticles-2433    27       839           new    ADJ\n",
       "840  NewsArticles-2433    27       840  developments   NOUN\n",
       "841  NewsArticles-2433    27       841             .  PUNCT\n",
       "\n",
       "[842 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a single document and only show the \"pos\" attribute (coarse POS tag)\n",
    "tokens_table(corpus_small, select='NewsArticles-2433', sentences=True, with_attr='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>3</td>\n",
       "      <td>Russian</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>4</td>\n",
       "      <td>spies</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1107</td>\n",
       "      <td>fight</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1108</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1109</td>\n",
       "      <td>defend</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1110</td>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1111</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1954 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc  position    token    pos  tag\n",
       "0     NewsArticles-2433         0      DOJ   NOUN   NN\n",
       "1     NewsArticles-2433         1        :  PUNCT    :\n",
       "2     NewsArticles-2433         2        2    NUM   CD\n",
       "3     NewsArticles-2433         3  Russian    ADJ   JJ\n",
       "4     NewsArticles-2433         4    spies   NOUN  NNS\n",
       "...                 ...       ...      ...    ...  ...\n",
       "1949    NewsArticles-49      1107    fight   VERB   VB\n",
       "1950    NewsArticles-49      1108       to   PART   TO\n",
       "1951    NewsArticles-49      1109   defend   VERB   VB\n",
       "1952    NewsArticles-49      1110       it   PRON  PRP\n",
       "1953    NewsArticles-49      1111        .  PUNCT    .\n",
       "\n",
       "[1954 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select two documents and only show the \"pos\" and \"tag\" attributes (coarse and detailed POS tags)\n",
    "tokens_table(corpus_small, select=['NewsArticles-2433', 'NewsArticles-49'], with_attr=['pos', 'tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Side note: Common corpus function parameters\n",
    "    \n",
    "Many corpus functions share the same parameter names and when they do, they implicate the same behavior. As already explained, all corpus functions accept a `Corpus` object as first parameter. But next to that, many corpus functions also accept a `select` parameter, which can always be used to specify a subset of the documents to which the respective function is applied. We also already got to know the `sentences` parameter that some corpus functions accept in order to also represent the sentence structure of a document in their output.\n",
    "    \n",
    "To know which functions accept which parameter, check their documentation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus vocabulary\n",
    "\n",
    "The corpus *vocabulary* is the set of unique tokens (usually refered to as *token types*) in a corpus. We can get that set via [`vocabulary`](api.rst#TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Syrian',\n",
       " 'Adding',\n",
       " 'fluid',\n",
       " 'Swinburne',\n",
       " 'survey',\n",
       " 'responding',\n",
       " 'Man',\n",
       " 'execution',\n",
       " 'eastern',\n",
       " 'Finance',\n",
       " 'nonetheless',\n",
       " 'Dorries',\n",
       " 'services',\n",
       " '\"?Wright',\n",
       " 'juror',\n",
       " 'chancellor',\n",
       " 'distinctive',\n",
       " 'motor',\n",
       " 'eliminated',\n",
       " 'clone',\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import vocabulary\n",
    "\n",
    "vocabulary(corpus_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus function also accepts a `select` parameter. We can also sort the vocabulary via `sort=True`, which returns a list instead of a set. To get the sorted vocabulary for document \"NewsArticles-2433\", we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n',\n",
       " '\"',\n",
       " \"'s\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '2',\n",
       " '2014',\n",
       " '22',\n",
       " '29',\n",
       " '33',\n",
       " '43',\n",
       " '500',\n",
       " ':',\n",
       " 'A',\n",
       " 'Akehmet',\n",
       " 'Aleksandrovich',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary(corpus_small, select='NewsArticles-2433', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the number of unique tokens in the corpus, i.e. the vocabulary size, we can use [`vocabulary_size`](api.rst#vocabulary_size), which is basically a shortcut for `len(vocabulary(<Corpus object>))`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import vocabulary_size\n",
    "\n",
    "vocabulary_size(corpus_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus function [`vocabulary_counts`](api.rst#vocabulary_size) is useful to find out how often each token in the vocabulary occurs in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 1,\n",
       " 'agent': 6,\n",
       " 'cop': 1,\n",
       " 'mark': 1,\n",
       " 'number': 18,\n",
       " 'passion': 1,\n",
       " 'unable': 1,\n",
       " 'Where': 2,\n",
       " 'types.-': 1,\n",
       " 'Closer': 1,\n",
       " 'Reflection': 1,\n",
       " 'approach': 8,\n",
       " 'users': 10,\n",
       " 'average': 11,\n",
       " 'designed': 2,\n",
       " 'geared': 1,\n",
       " 'stream': 3,\n",
       " 'sites': 2,\n",
       " 'deportation': 4,\n",
       " 'discourse': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import vocabulary_counts\n",
    "\n",
    "vocabulary_counts(corpus_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to obtain absolute counts, you can use the `proportions` parameter. Setting it to `1` gives you ordinary proportions (i.e. $\\frac{x_i}{\\sum_j x_j}$) and `2` gives you proportions on a log10 scale ($\\log_{10} \\frac{x_i}{\\sum_j x_j}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 1.6779086546528407e-05,\n",
       " 'agent': 0.00010067451927917044,\n",
       " 'cop': 1.6779086546528407e-05,\n",
       " 'mark': 1.6779086546528407e-05,\n",
       " 'number': 0.0003020235578375113,\n",
       " 'passion': 1.6779086546528407e-05,\n",
       " 'unable': 1.6779086546528407e-05,\n",
       " 'Where': 3.3558173093056814e-05,\n",
       " 'types.-': 1.6779086546528407e-05,\n",
       " 'Closer': 1.6779086546528407e-05,\n",
       " 'Reflection': 1.6779086546528407e-05,\n",
       " 'approach': 0.00013423269237222726,\n",
       " 'users': 0.00016779086546528407,\n",
       " 'average': 0.00018456995201181247,\n",
       " 'designed': 3.3558173093056814e-05,\n",
       " 'geared': 1.6779086546528407e-05,\n",
       " 'stream': 5.033725963958522e-05,\n",
       " 'sites': 3.3558173093056814e-05,\n",
       " 'deportation': 6.711634618611363e-05,\n",
       " 'discourse': 1.6779086546528407e-05,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_proportions = vocabulary_counts(corpus_small, proportions=1)\n",
    "vocab_proportions   # will reuse that later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular output is often more convenient for displaying results. You can set the `as_table` parameter to `True` to get a dataframe of tokens and their frequency. You can also specify to sort the dataframe by specifying the column to sort by in the `as_table` parameter. By default, this will sort in ascending order, but if you prefix the column name by \"-\", you obtain a descending sort order. Here, we will get a table of tokens with their frequencies in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>the</td>\n",
       "      <td>2670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>,</td>\n",
       "      <td>2426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6325</th>\n",
       "      <td>.</td>\n",
       "      <td>2175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>\"</td>\n",
       "      <td>1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>of</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>colours</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>foolish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4053</th>\n",
       "      <td>shift</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222</th>\n",
       "      <td>ageing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9223 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  freq\n",
       "3742      the  2670\n",
       "1339        ,  2426\n",
       "6325        .  2175\n",
       "7933        \"  1417\n",
       "457        of  1387\n",
       "...       ...   ...\n",
       "4062  colours     1\n",
       "4059  foolish     1\n",
       "4056       59     1\n",
       "4053    shift     1\n",
       "9222   ageing     1\n",
       "\n",
       "[9223 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_counts(corpus_small, as_table='-freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Common parameter `as_table`\n",
    "    \n",
    "Just like `select` or `sentences`, the `as_table` parameter is also a common parameter available for many corpus functions, e.g. [`doc_lengths`](api.rst#TODO), [`doc_num_sents`](api.rst#TODO) or [`doc_texts`](api.rst#TODO).\n",
    "\n",
    "</div>\n",
    "\n",
    "We can see that \"the\" and \"to\" are top-ranking tokens, along with some punctuation characters. We can check the share of tokens for \"the\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04480016107923085"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_proportions['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the token \"the\" occurs more the 4% of the time in the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing corpus summary statistics\n",
    "\n",
    "There are several functions for visualizing summary statistics of corpora which are implemented in the [`corpus.visualize`](api.rst#tmtoolkit.corpus.visualize) module. These are especially useful to see how certain processing steps influence summary statistics like token distributions and document length in a corpus. We will start with a few visualizations for the current corpus and can later compare these with plots generated after text processing.\n",
    "\n",
    "First, we disable logging, because it's not useful for the next examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the plotting functions that we'll use. We also need to import `matplotlib.pyplot` in order to generate a Figure and an Axes object on which the actual plot is drawn. Most plotting functions in tmtoolkit work this way that you need to pass these two objects. This allows for full flexibility since you can adjust the plot before and after applying the plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tmtoolkit.corpus.visualize import plot_doc_lengths_hist, plot_doc_frequencies_hist, plot_ranked_vocab_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Side Note: So much `from tmtoolkit.corpus import ...`\n",
    "    \n",
    "You'll see a lot of import statements from the [`corpus` module](api.rst#TODO) in this chapter, because all corpus functions are defined in this module. In this manual, I like to explicitly point out from where to import an object (like a function) and only import those objects that I actually need. However, it's completely fine to make a wildcard import `from tmtoolkit.corpus import *` at the beginning of your own code so that all objects in that module are directly available. An alternative approach is to import the corpus module with a short alias name, e.g. `import tmtoolkit.corpus as crp`. Then, you can access all objects in that module via `crp.<...>`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use [`plot_doc_lengths_hist`](api.rst#TODO) to show the distribution of document lengths (i.e. number of tokens in each document) in our corpus. By default, the y-axis uses a log10 scale which is useful for medium and large scaled corpora, but since our corpus is so small we'll use a linear scale instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_doc_lengths_hist(fig, ax, corpus_small, y_log=False)  # use linear scale\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve this plot, e.g. to better see the distribution of small documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgJUlEQVR4nO3de5hkdX3n8fdHRpABBHTGC5exAZENGoMycUETY8AoERXXJT64kICanUSfmGhIyCgxMcm6wcsaUZPoRBGMLEqIEpS4ggY1JoACAjKAgjACAg6K3LwAo9/945zWop2erhm66lc98349Tz9d5/r7nlNVXZ/+nXPqpKqQJEnSeD2kdQGSJElbIkOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkxaAJKuTPKt1HS0l+W9JbkxyT5KnDDH/Z5P89jhqmwRJTk7yvxq1vSbJs1u0LS1khjCpsfV9gCU5JskXpoer6olV9dk51jOVpJIsGlGprb0N+L2q2r6qvty6mHFK8qwkN7WuA9qGPWlzYwiTNJQJCHePA1Y3rkGS5o0hTFoABnvLkjwtyUVJ7kryrSRv72f7fP/7jv6Q3YFJHpLkT5N8I8naJB9MsuPAen+rn/adJG+Y0c4bk5yR5ENJ7gKO6ds+P8kdSW5J8u4kWw+sr5K8Ksk1Se5O8ldJ9kryn329pw/OP2Mb11trkm2S3ANsBVyW5OuzLP9rSa5OcmeSdwOZa90D03+pr/GO/pDnMf34BxzSnNlDubHbm+T5SS7t2/nPJE+e8Rz/UZLL+234SJKHJdkO+CSwS/+83pNkl1leKoP7Y6PbGph+XP/83pzkt/vtfHySFcCRwHF9HR8faHK/9a0vyZIkn+jruD3Jvyfxs0fCECYtRCcCJ1bVw4G9gNP78c/sf+/UH7I7Hzim//lVYE9ge+DdAEn2Bf6O7kP1scCOwK4z2joMOAPYCTgV+BHwWmAJcCBwMPCqGcs8F9gfOAA4DlgFHAXsDjwJeOks27XeWqvq3qravp/nF6pqr5kLJlkCfBT40762rwPPmGvd/bKPows57wKWAvsBl85S4/oMtb3pzmM7Cfgd4JHAe4GzkmwzsK6XAIcAewBPBo6pqu8Bvw7c3D+v21fVzRsqaFPb6pc9BPhD4NnA44FnTS9QVavoXgdv6et4wVzrA44FbqLbt48GXg94vzwJQ5g0Kc7sewruSHIHXTiazf3A45Msqap7quqCDcx7JPD2qrququ4BXgccke7Q4uHAx6vqC1V1H/Bn/OyH4/lVdWZV/biqflBVF1fVBVW1rqrW0H24/8qMZd5SVXdV1WrgCuCcvv076cLObCfVb6jWuTwPWF1VZ1TV/cA7gFuHXPf/AD5dVadV1f1V9Z2qunSINjd2e1cA762qC6vqR1V1CnAvXXib9s6qurmqbgc+ThcIN8WDaeslwAeqanVVfR9445Btzra+++lC/uP6/fvv5U2LJcAQJk2KF1XVTtM//Gzv0qBXAE8Ark7ypSTP38C8uwDfGBj+BrCIrkdiF+DG6Qn9B+53Zix/4+BAkif0h5Zu7Q9R/m+6nqdB3xp4/IP1DG/P+m2o1rnM3JaaUfuG1r07Xc/Zphp2ex8HHDsjbO/e1zZtMDh+n9n31VweTFsP2JczHm/IbOt7K3AtcE6S65KsHHJ90mbPECYtMFV1TVW9FHgU8GbgjP68ofX1LtxM94E8bRmwji4o3ALsNj0hybZ0h64e0NyM4b8Hrgb27g+Hvp6Bc68epA3VOpdb6EIGAEkyODzHum+kO6y7Pt8DFg8MP2aIWmZzI/CmwbBdVYur6rQhlt3YnqMH09YDXhc8cD9udC1VdXdVHVtVewIvBP4wycEbsw5pc2UIkxaYJEclWVpVPwbu6Ef/GLit/73nwOynAa9NskeS7el6rj5SVevozvV6QZKn9yePv5G5A9UOwF3APUn+C/DKedqsuWqdy9nAE5O8uD/E+Ps8MDBtaN2nAs9O8pIki5I8Msl+/XKXAi9OsjjJ4+l6ITfVPwC/m+S/prNdkkOT7DDEst8CHpmBiwlG2NbpwMuS/FySxcAb1lPLnj+72Pr1Fwg8vg/Gd9KdV/jjYZeXNmeGMGnhOQRYne6KwROBI/rztb4PvAn4j/4Q1AF0J2f/I92Vk9cDPwReDdCfw/Rq4MN0vR/3AGvpzh2azR/RnUN1N90H/UfmcbtmrXUuVfVt4DeAE+gOqe4N/Mcw666qG+jOKTsWuJ0ueP1Cv9zfAPfRBY9T6ALbJqmqi4D/SXdBwHfpDtEdM+SyV9MFyev653aDV0c+yLY+CbwTOK9fbvqcw+nXxfuBffs6zhxilXsDn6Z7fZ0P/F1VnTdMLdLmLp4fKQmg7yG6g+5Q4/WNy9GESPJzdBccbDNkr6SkIdkTJm3BkrygP9S2Hd030n8FWNO2KrWW7hZR2yTZme68w48bwKT5ZwiTtmyH0Z20fjPdYaMj/PoA0X2/2Fq6q0Z/xPye+yep5+FISZKkBuwJkyRJasAQJkmS1MAwtwNpbsmSJTU1NdW6DEmSpDldfPHF366qpXPNtyBC2NTUFBdddFHrMiRJkuaU5Btzz+XhSEmSpCYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktSAIUySJKkBQ5gkSVIDhjBJkqQGDGGSJEkNjCyEJTkpydokV6xn2rFJKsmSUbUvSZI0yUbZE3YycMjMkUl2B54D3DDCtiVJkibayEJYVX0euH09k/4GOA6oUbUtSZI06RaNs7EkhwHfrKrLksw17wpgBcCyZcvGUJ0m0dTKs0fexpoTDh15G5IkzTS2E/OTLAZeD/zZMPNX1aqqWl5Vy5cuXTra4iRJksZsnFdH7gXsAVyWZA2wG3BJkseMsQZJkqSJMLbDkVX1FeBR08N9EFteVd8eVw2SJEmTYpRfUXEacD6wT5KbkrxiVG1JkiQtNCPrCauql84xfWpUbUuSJE06vzFfkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktTAotYFaP5NrTx75G2sOeHQkbchSdLmzJ4wSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktSAIUySJKkBQ5gkSVIDhjBJkqQGDGGSJEkNjCyEJTkpydokVwyMe2uSq5NcnuRjSXYaVfuSJEmTbJQ9YScDh8wYdy7wpKp6MvA14HUjbF+SJGlijSyEVdXngdtnjDunqtb1gxcAu42qfUmSpEnW8pywlwOfbNi+JElSM4taNJrkeGAdcOoG5lkBrABYtmzZmCobvamVZ7cuQZIkTYCx94QlOQZ4PnBkVdVs81XVqqpaXlXLly5dOrb6JEmSxmGsPWFJDgGOA36lqr4/zrYlSZImySi/ouI04HxgnyQ3JXkF8G5gB+DcJJcmec+o2pckSZpkI+sJq6qXrmf0+0fVniRJ0kLiN+ZLkiQ1YAiTJElqwBAmSZLUgCFMkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktSAIUySJKkBQ5gkSVIDhjBJkqQGDGGSJEkNGMIkSZIaMIRJkiQ1YAiTJElqwBAmSZLUgCFMkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1MDIQliSk5KsTXLFwLhHJDk3yTX9751H1b4kSdIkG2VP2MnAITPGrQQ+U1V7A5/phyVJkrY4IwthVfV54PYZow8DTukfnwK8aFTtS5IkTbJxnxP26Kq6pX98K/DoMbcvSZI0EZqdmF9VBdRs05OsSHJRkotuu+22MVYmSZI0euMOYd9K8liA/vfa2WasqlVVtbyqli9dunRsBUqSJI3DuEPYWcDR/eOjgX8Zc/uSJEkTYZRfUXEacD6wT5KbkrwCOAH4tSTXAM/uhyVJkrY4i0a14qp66SyTDh5Vm5IkSQuF35gvSZLUgCFMkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktTAotYFaGGaWnl26xLUyLie+zUnHDqWdiSpFXvCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktSAIUySJKkBQ5gkSVIDhjBJkqQGDGGSJEkNGMIkSZIaMIRJkiQ10CSEJXltktVJrkhyWpKHtahDkiSplbGHsCS7Ar8PLK+qJwFbAUeMuw5JkqSWWh2OXARsm2QRsBi4uVEdkiRJTSwad4NV9c0kbwNuAH4AnFNV58ycL8kKYAXAsmXLxluktihTK88eeRtrTjh05G1IkhaWFocjdwYOA/YAdgG2S3LUzPmqalVVLa+q5UuXLh13mZIkSSPV4nDks4Hrq+q2qrof+Cjw9AZ1SJIkNTNUCEvymWHGDekG4IAki5MEOBi4ahPXJUmStCBt8Jyw/qsjFgNL+sOI6Sc9HNh1UxqsqguTnAFcAqwDvgys2pR1SZIkLVRznZj/O8Br6M7dupifhrC7gHdvaqNV9efAn2/q8pIkSQvdBkNYVZ0InJjk1VX1rjHVJEmStNkb6isqqupdSZ4OTA0uU1UfHFFdkiRJm7WhQliSfwT2Ai4FftSPLsAQJkmStAmG/bLW5cC+VVWjLEaSJGlLMez3hF0BPGaUhUiSJG1Jhu0JWwJcmeSLwL3TI6vqhSOpSpIkaTM3bAh74yiLkCRJ2tIMe3Xk50ZdiCRJ0pZk2Ksj76a7GhJga+ChwPeq6uGjKkySJGlzNmxP2A7Tj/v7PR4GHDCqoiRJkjZ3w14d+RPVORN47vyXI0mStGUY9nDkiwcGH0L3vWE/HElFkiRJW4Bhr458wcDjdcAaukOSkiRJ2gTDnhP2slEXIkmStCUZ6pywJLsl+ViStf3PPyfZbdTFSZIkba6GPTH/A8BZwC79z8f7cZIkSdoEw54TtrSqBkPXyUleM4J6mplaeXbrEiQNGMd7cs0Jh468DUmazbA9Yd9JclSSrfqfo4DvjLIwSZKkzdmwIezlwEuAW4FbgMOBY0ZUkyRJ0mZv2MORfwkcXVXfBUjyCOBtdOFMkiRJG2nYnrAnTwcwgKq6HXjKaEqSJEna/A0bwh6SZOfpgb4nbNheNEmSJM0wbJD6P8D5Sf6pH/4N4E2jKUmSJGnzN+w35n8wyUXAQf2oF1fVlaMrS5IkafM29CHFPnQZvCRJkubBsOeESZIkaR4ZwiRJkhowhEmSJDVgCJMkSWrAECZJktSAIUySJKkBQ5gkSVIDhjBJkqQGDGGSJEkNGMIkSZIaaBLCkuyU5IwkVye5KsmBLeqQJElqZeh7R86zE4H/V1WHJ9kaWNyoDkmSpCbGHsKS7Ag8EzgGoKruA+4bdx2SJEkttTgcuQdwG/CBJF9O8r4k2zWoQ5IkqZkWhyMXAU8FXl1VFyY5EVgJvGFwpiQrgBUAy5YtG3uR0nyaWnn2WNpZc8KhY2lHkvTgtegJuwm4qaou7IfPoAtlD1BVq6pqeVUtX7p06VgLlCRJGrWxh7CquhW4Mck+/aiDgSvHXYckSVJLra6OfDVwan9l5HXAyxrVIUmS1ESTEFZVlwLLW7QtSZI0CfzGfEmSpAYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktSAIUySJKkBQ5gkSVIDhjBJkqQGDGGSJEkNGMIkSZIaMIRJkiQ1YAiTJElqwBAmSZLUgCFMkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktSAIUySJKkBQ5gkSVIDhjBJkqQGDGGSJEkNGMIkSZIaMIRJkiQ10CyEJdkqyZeTfKJVDZIkSa207An7A+Cqhu1LkiQ10ySEJdkNOBR4X4v2JUmSWlvUqN13AMcBO8w2Q5IVwAqAZcuWjacqaYGbWnl26xIkSUMae09YkucDa6vq4g3NV1Wrqmp5VS1funTpmKqTJEkajxaHI58BvDDJGuDDwEFJPtSgDkmSpGbGHsKq6nVVtVtVTQFHAP9WVUeNuw5JkqSW/J4wSZKkBlqdmA9AVX0W+GzLGiRJklqwJ0ySJKkBQ5gkSVIDhjBJkqQGDGGSJEkNGMIkSZIaMIRJkiQ1YAiTJElqwBAmSZLUgCFMkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1MCi1gVIkh68qZVnj7yNNSccOvI2pC2JPWGSJEkNGMIkSZIaMIRJkiQ1YAiTJElqwBAmSZLUgCFMkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhoYewhLsnuS85JcmWR1kj8Ydw2SJEmtLWrQ5jrg2Kq6JMkOwMVJzq2qKxvUIkmS1MTYe8Kq6paquqR/fDdwFbDruOuQJElqqUVP2E8kmQKeAly4nmkrgBUAy5YtG29hkrYIUyvPbl3CgjKO/bXmhENH3oY0KZqdmJ9ke+CfgddU1V0zp1fVqqpaXlXLly5dOv4CJUmSRqhJCEvyULoAdmpVfbRFDZIkSS21uDoywPuBq6rq7eNuX5IkaRK06Al7BvCbwEFJLu1/ntegDkmSpGbGfmJ+VX0ByLjblSRJmiR+Y74kSVIDhjBJkqQGDGGSJEkNGMIkSZIaMIRJkiQ1YAiTJElqwBAmSZLUgCFMkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOLWhcgSdK4Ta08e+RtrDnh0JG3sTkZx3MCk/W82BMmSZLUgCFMkiSpAUOYJElSA4YwSZKkBgxhkiRJDRjCJEmSGjCESZIkNWAIkyRJasAQJkmS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhowhEmSJDVgCJMkSWrAECZJktRAkxCW5JAkX01ybZKVLWqQJElqaewhLMlWwN8Cvw7sC7w0yb7jrkOSJKmlFj1hTwOurarrquo+4MPAYQ3qkCRJaqZFCNsVuHFg+KZ+nCRJ0hZjUesCZpNkBbCiH7wnyVfnuYklwLfneZ3jZP1tWX9b1t/WyOrPm0ex1p8xlv0/wm3x9fMgzMPzMkz9jxtmRS1C2DeB3QeGd+vHPUBVrQJWjaqIJBdV1fJRrX/UrL8t62/L+tuy/rasv635rL/F4cgvAXsn2SPJ1sARwFkN6pAkSWpm7D1hVbUuye8BnwK2Ak6qqtXjrkOSJKmlJueEVdW/Av/aou0BIzvUOSbW35b1t2X9bVl/W9bf1rzVn6qar3VJkiRpSN62SJIkqYEtLoQthFsmJdk9yXlJrkyyOskf9OMfkeTcJNf0v3fuxyfJO/ttujzJU9tuQSfJVkm+nOQT/fAeSS7s6/xIf2EGSbbph6/tp081LbyraackZyS5OslVSQ5cSPs/yWv7184VSU5L8rBJ3v9JTkqyNskVA+M2en8nObqf/5okRzeu/6396+fyJB9LstPAtNf19X81yXMHxjf5+7S++gemHZukkizphxfE/u/Hv7p/DlYnecvA+Inf/0n2S3JBkkuTXJTkaf34Sdz/8/aZ1WIbNlD/6N/DVbXF/NBdCPB1YE9ga+AyYN/Wda2nzscCT+0f7wB8je4WT28BVvbjVwJv7h8/D/gkEOAA4MLW29DX9YfA/wU+0Q+fDhzRP34P8Mr+8auA9/SPjwA+MgG1nwL8dv94a2CnhbL/6b78+Hpg24H9fswk73/gmcBTgSsGxm3U/gYeAVzX/965f7xzw/qfAyzqH795oP59+7892wB79H+Ttmr592l99ffjd6e7iOobwJIFtv9/Ffg0sE0//KiFtP+Bc4BfH9jnn53g/T8vn1mttmED9Y/8Pbyl9YQtiFsmVdUtVXVJ//hu4Cq6D9bD6MIB/e8X9Y8PAz5YnQuAnZI8drxVP1CS3YBDgff1wwEOAs7oZ5lZ//R2nQEc3M/fRJId6f4ovh+gqu6rqjtYQPuf7qKbbZMsAhYDtzDB+7+qPg/cPmP0xu7v5wLnVtXtVfVd4FzgkJEXz/rrr6pzqmpdP3gB3XciTtf/4aq6t6quB66l+9vU7O/TLPsf4G+A44DBk4cXxP4HXgmcUFX39vOsHah/Iez/Ah7eP94RuLl/PIn7f74+s5psw2z1j+M9vKWFsAV3y6R0h4aeAlwIPLqqbukn3Qo8un88idv1Dro/3j/uhx8J3DHwgh6s8Sf199Pv7OdvZQ/gNuAD6Q6nvi/JdiyQ/V9V3wTeBtxAF77uBC5m4ez/aRu7vyfqeZjh5XT/+cMCqT/JYcA3q+qyGZMWRP3AE4BfTneI/XNJfrEfv1Dqfw3w1iQ30r2fX9ePn+j6H+RnVvNtmFH/oJG8h7e0ELagJNke+GfgNVV11+C06vpEJ/LS1iTPB9ZW1cWta9lEi+gODfx9VT0F+B5dV/pPTPj+35nuv689gF2A7RjTf8SjMsn7ey5JjgfWAae2rmVYSRYDrwf+rHUtD8IiusNaBwB/DJzesod9E7wSeG1V7Q68lr5nfpIt1M+sabPVP8r38JYWwoa6ZdIkSPJQuhfDqVX10X70t6YPc/W/p7vXJ227ngG8MMkauu7Yg4AT6bqcp7+bbrDGn9TfT98R+M44C57hJuCmqpr+T+gMulC2UPb/s4Hrq+q2qrof+Cjdc7JQ9v+0jd3fk/Y8kOQY4PnAkf2HECyM+veiC/GX9e/j3YBLkjyGhVE/dO/jj/aHvL5I1yu/hIVT/9F0712Af6I71AUTWv88fWY124ZZ6h/5e3hLC2EL4pZJ/X9r7weuqqq3D0w6i+6NSf/7XwbG/1Z/xckBwJ0DXcBjV1Wvq6rdqmqKbh//W1UdCZwHHN7PNrP+6e06vJ+/2X9MVXUrcGOSffpRBwNXskD2P91hyAOSLO5fS9P1L4j9P2Bj9/engOck2bnvDXxOP66JJIfQHZJ/YVV9f2DSWcAR6a5K3QPYG/giE/T3qaq+UlWPqqqp/n18E92Jy7eyQPY/cCbdyfkkeQLdidLfZgHs/97NwK/0jw8CrukfT9z+n8fPrCbbMFv9Y3kP1xiunJikH7qrMr5GdwXD8a3rmaXGX6Lrtr0cuLT/eR7deTqfoXszfhp4RD9/gL/tt+krwPLW2zCwLc/ip1dH7tm/UK+l+89u+qqlh/XD1/bT95yAuvcDLuqfgzPprtRZMPsf+AvgauAK4B/pruKZ2P0PnEZ3/tr9dB/4r9iU/U133sa1/c/LGtd/Ld35IdPv4fcMzH98X/9X6a+A68c3+fu0vvpnTF/DT6+OXCj7f2vgQ/174BLgoIW0/+k+By6mu8LuQmD/Cd7/8/aZ1WIbNlD/yN/DfmO+JElSA1va4UhJkqSJYAiTJElqwBAmSZLUgCFMkiSpAUOYJElSA4YwSSOR5I1J/qh1HcNIckySXWaZdnKSw9c37UG2+fqBx1NJrpjvNiRNNkOYJMExdLd4GqfXzz2LpM2ZIUzSvElyfJKvJfkCsM/A+P2SXJDk8iQf678NmySPT/LpJJcluSTJXkmeleQTA8u+u791CEnWJPnrJJcmuSjJU5N8KsnXk/zuwDJ/nORLfXt/0Y+bSnJVkn9IsjrJOUm27Xu5lgOn9uvddgPbt3+6m0Ff3Lc7fUuWzyZ5c5Iv9tv/y/34xUlOT3Jlv90XJlme5ARg27696fvRbTWztn4dv98vf3mSD8/H8yRpMhjCJM2LJPvT3aZjP7pvjf7FgckfBP6kqp5M9w3Zf96PPxX426r6BeDpdN8aPpcbqmo/4N+Bk+lutXQA3V0CSPIcutuIPK2vZf8kz+yX3btv74nAHcB/r6oz6O6OcGRV7VdVP5hl+x4KvAs4vKr2B04C3jQwy6KqehrwmoHtexXw3araF3gDsD9AVa0EftC3d+RstfXjVwJP6ffdT4KmpIVv0dyzSNJQfhn4WPX3WEtyVv97R2CnqvpcP98pwD8l2QHYtao+BlBVP+znn6ud6XuxfQXYvqruBu5Ocm+SnejuN/cc4Mv9fNvTBZwb6G5sfmk//mJgaiO2bx/gScC5fY1b8cDQOH3T38H1/hLdzeupqiuSXL6B9c9W2+V0vXRn0t1CS9JmwhAmadKs44G99A+bMf3e/vePBx5PDy+iuy/dX1fVewcXSjI1Y/4fAbMeelyPAKur6sBZpk+v+0ds2t/W2Wo7FHgm8ALg+CQ/X1XrNmH9kiaMhyMlzZfPAy/qz7PagS40UFV3At+dPk8K+E3gc30P1k1JXgSQZJski4FvAPv2wzsBB29kHZ8CXp5k+369uyZ51BzL3A3sMMc8XwWWJjmwX+9DkzxxjmX+A3hJP/++wM8PTLu/P8Q5qyQPAXavqvOAPwF2pOvZk7QZsCdM0ryoqkuSfAS4DFgLfGlg8tHAe/qQdR3wsn78bwLvTfKXwP3Ab1TVdUlOB64AruenhxWHreOcJD8HnN8fNrwHOIqud2k2J/f1/QA4cH3nhVXVff1J/O/sD7EuAt4BrN7Aev8OOCXJlcDV/bx39tNWAZcnuQQ4fpbltwI+1LcX4J1VdccG2pO0gKSqWtcgSZulJFsBD62qHybZC/g0sE9V3de4NEkTwJ4wSRqdxcB5/WHHAK8ygEmaZk+YJElSA56YL0mS1IAhTJIkqQFDmCRJUgOGMEmSpAYMYZIkSQ0YwiRJkhr4/94bOND355EnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))   # make the plot larger\n",
    "plot_doc_lengths_hist(fig, ax, corpus_small, y_log=False, bins=20)  # use 20 bins\n",
    "ax.set_xticks(range(0, 2201, 200))    # set x axis ticks and range\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this is a right skewed distribution with a few quite large documents with more than 1500 tokens, but also several documents that are very small (less than 100 tokens).\n",
    "\n",
    "The function [`plot_doc_frequencies_hist`](api.rst#TODO) lets us plot the distribution of document frequencies of each token type. This time, we stick with the log10 scale on the y-axis, because otherwise the token types with high document frequency would be hardly visible in the plot, since there are so few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_doc_frequencies_hist(fig, ax, corpus_small)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that the large majority of token types has a low document frequency, i.e. they occur only in a few documents. There are only very few token types that occur in almost every document. Words like \"the\" or \"a\" are usually among these.\n",
    "\n",
    "Another common type of plot is a *rank-frequency distribution* plot for token frequencies. This means the tokens are ordered in descending order from the most frequent token to the least frequent token. This forms the x-axis. On the y-axis the frequency of the respective tokens is plotted. Rank and frequency of tokens in text corpora usually have an inverse relationship, i.e. the second most frequent token occurs only half as often as the most frequent token, the token on rank 100 only has 1/100 of the frequency of the most frequent token, etc. This is a power law distribution which appears as nearly straight line when plotted on a log-log scale (i.e. a log scale on both axes).\n",
    "\n",
    "We can also observe that in our small corpus using the [`plot_ranked_vocab_counts`](api.rst#TODO) function, which by default uses a log-log scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_ranked_vocab_counts(fig, ax, corpus_small, zipf=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I additionally passed the argument `zipf=True` which compares the corpus' distribution to the distribution predicted by [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing: transforming documents and tokens\n",
    "\n",
    "So far we haven't modified anything in our corpus, we only investigated its contents. This will change now as we will apply several text processing methods to the contents of our corpus. But before we do that, I want you pay attention to an important detail about how a `Corpus` object behaves when it is modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: A `Corpus` object as \"state machine\"\n",
    "\n",
    "A `Corpus` object is implemented as a \"state machine\", i.e. its contents (the documents) and behavior can change when you apply a function to it. For instance, let's suppose we want to turn all tokens in a corpus to lowercase tokens. We do that by applying [`to_lowercase`](api.rst#TODO):\n",
    "\n",
    "\n",
    "```python\n",
    "corp = Corpus({\n",
    "    \"doc1\": \"Hello world!\",\n",
    "    \"doc2\": \"Another example\"\n",
    "}, language='en')\n",
    "\n",
    "doc_tokens(corp)\n",
    "\n",
    "# Out:\n",
    "# {\n",
    "#   'doc1': ['Hello', 'world', '!'],\n",
    "#   'doc2': ['Another', 'example']\n",
    "# }\n",
    "\n",
    "to_lowercase(corp)\n",
    "doc_tokens(corp)\n",
    "\n",
    "# Out:\n",
    "# {\n",
    "#   'doc1': ['hello', 'world', '!'],\n",
    "#   'doc2': ['another', 'example']\n",
    "# }\n",
    "```\n",
    "\n",
    "As you can see, the tokens \"inside\" `corp` are changed *in place*. It's important to see that after applying the function `to_lowercase()`, the tokens in `corp` were transformed and **the original tokens from before calling this method are not available anymore.** In Python, assigning a *mutable* object to a variable binds the same object only to a different name, it doesn't copy it. Since a `Corpus` object is a mutable object (you can change its contents), when we simply assign such an object to a different variable (say `corp_original`) we essentially only have two names for the same object and by calling a method on one of these variable names, the values will be changed for *both* names.\n",
    "\n",
    "#### Copying `Corpus` objects\n",
    "\n",
    "What can we do about that? There are two ways: The first is to *copy* the object which can be done with the Python [`copy`](https://docs.python.org/3/library/copy.html#copy.copy) method. By this, we create another variable `corpus_orig` that points to a separate `Corpus` object. The second way is to apply the corpus transformation function, e.g. `to_lowercase`, but set the parameter `inplace=False`. This will then return a *modified copy* and retain the original input corpus. The `inplace` parameter is a common corpus function parameter that is available for all functions that modify a `Corpus` object in some way. By default, it is set to `True`.\n",
    "\n",
    "We start with the first way, copying a `Corpus` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): DOJ : 2 Russian spies indicted in Yahoo hack    Wa...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in Dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): Dutch election : High turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): Trump vs. America : The fight for democracy    Fri...\n",
      "> NewsArticles-469 (398 tokens): Warning of tight times ahead for insurers    Analy...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode releases ' Spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): Grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seeks more time on Trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): In Breakthrough Discovery , Scientists Mass - Prod...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 9223\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "\n",
    "corpus_orig = copy(corpus_small)\n",
    "print_summary(corpus_orig)   # same content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139914070486464, 139912893465840)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but the different IDs confirm that we have two different objects\n",
    "id(corpus_small), id(corpus_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply `to_lowercase` to `corpus_small`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): doj : 2 russian spies indicted in yahoo hack    wa...\n",
      "> NewsArticles-2225 (539 tokens): rutte and wilders face - off in dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): dutch election : high turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): trump vs. america : the fight for democracy    fri...\n",
      "> NewsArticles-469 (398 tokens): warning of tight times ahead for insurers    analy...\n",
      "> NewsArticles-2766 (700 tokens): depeche mode releases ' spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (464 tokens): doj seeks more time on trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): in breakthrough discovery , scientists mass - prod...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 8369\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import to_lowercase\n",
    "\n",
    "to_lowercase(corpus_small)\n",
    "print_summary(corpus_small)  # all tokens are lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens of the copied original corpus remain unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): DOJ : 2 Russian spies indicted in Yahoo hack    Wa...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in Dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): Dutch election : High turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): Trump vs. America : The fight for democracy    Fri...\n",
      "> NewsArticles-469 (398 tokens): Warning of tight times ahead for insurers    Analy...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode releases ' Spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): Grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seeks more time on Trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): In Breakthrough Discovery , Scientists Mass - Prod...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 9223\n"
     ]
    }
   ],
   "source": [
    "print_summary(corpus_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this also uses up almost twice as much computer memory now. So you shouldn't create copies that often and also release unused memory by using `del`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the objects again\n",
    "del corpus_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the second approach. We pass the `inplace=False` parameter and get back a transformed copy of `corpus_orig` as return value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): doj : 2 russian spies indicted in yahoo hack    wa...\n",
      "> NewsArticles-2225 (539 tokens): rutte and wilders face - off in dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): dutch election : high turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): trump vs. america : the fight for democracy    fri...\n",
      "> NewsArticles-469 (398 tokens): warning of tight times ahead for insurers    analy...\n",
      "> NewsArticles-2766 (700 tokens): depeche mode releases ' spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (464 tokens): doj seeks more time on trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): in breakthrough discovery , scientists mass - prod...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 8369\n"
     ]
    }
   ],
   "source": [
    "corpus_lowercase = to_lowercase(corpus_orig, inplace=False)\n",
    "print_summary(corpus_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the original corpus stays unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): DOJ : 2 Russian spies indicted in Yahoo hack    Wa...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in Dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): Dutch election : High turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): Trump vs. America : The fight for democracy    Fri...\n",
      "> NewsArticles-469 (398 tokens): Warning of tight times ahead for insurers    Analy...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode releases ' Spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): Grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seeks more time on Trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): In Breakthrough Discovery , Scientists Mass - Prod...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 9223\n"
     ]
    }
   ],
   "source": [
    "print_summary(corpus_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to apply some common text processing steps to our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and token normalization\n",
    "\n",
    "Lemmatization brings a token, if it is a word, to its base form. The lemma is already found out during the tokenization process and is available in the `lemma` token attribute. However, when you want to further process the tokens on the base of the lemmata, you should use the [lemmatize](api.rst#tmtoolkit.corpus.lemmatize) corpus function. This function sets the lemmata as tokens and all further processing will happen using these lemmatized tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>President</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>President</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>say</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>say</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>have</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>have</td>\n",
       "      <td>False</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59593</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>282</td>\n",
       "      <td>priority</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>priority</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59594</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>283</td>\n",
       "      <td>for</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>for</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59595</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>284</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59596</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>285</td>\n",
       "      <td>nation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>nation</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59597</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>286</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59598 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position      token  is_punct  is_stop      lemma  \\\n",
       "0      NewsArticles-1100         0  President     False    False  President   \n",
       "1      NewsArticles-1100         1      Trump     False    False      Trump   \n",
       "2      NewsArticles-1100         2        say     False    False        say   \n",
       "3      NewsArticles-1100         3         he     False     True         he   \n",
       "4      NewsArticles-1100         4       have     False     True       have   \n",
       "...                  ...       ...        ...       ...      ...        ...   \n",
       "59593   NewsArticles-960       282   priority     False    False   priority   \n",
       "59594   NewsArticles-960       283        for     False     True        for   \n",
       "59595   NewsArticles-960       284        the     False     True        the   \n",
       "59596   NewsArticles-960       285     nation     False    False     nation   \n",
       "59597   NewsArticles-960       286          .      True    False          .   \n",
       "\n",
       "       like_num    pos  tag  \n",
       "0         False  PROPN  NNP  \n",
       "1         False  PROPN  NNP  \n",
       "2         False   VERB  VBZ  \n",
       "3         False   PRON  PRP  \n",
       "4         False    AUX  VBZ  \n",
       "...         ...    ...  ...  \n",
       "59593     False   NOUN  NNS  \n",
       "59594     False    ADP   IN  \n",
       "59595     False    DET   DT  \n",
       "59596     False   NOUN   NN  \n",
       "59597     False  PUNCT    .  \n",
       "\n",
       "[59598 rows x 9 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import lemmatize\n",
    "\n",
    "# we use `inplace=False` to generate a lemmatized copy `corpus_norm`\n",
    "# of the original data; all further steps will be applied to `corpus_norm`\n",
    "corpus_norm = lemmatize(corpus_orig, inplace=False)\n",
    "tokens_table(corpus_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `lemma` column was copied over to the `token` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Stemming\n",
    "    \n",
    "tmtoolkit doesn't support stemming directly, since lemmatization is generally accepted as a better approach to bring different word forms of one word to a common base form. However, you may install [NLTK](https://www.nltk.org/) and apply stemming by using the [stem](api.rst#tmtoolkit.corpus.stem) function.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how you further want to analyze the data, it may be necessary to \"clean\" or \"normalize\" your tokens in different ways in order to remove noise from the corpus, such as punctuation tokens or numbers, upper/lowercase forms of the same word, etc. Note that this is usually not necessary when you work with some approaches such as word embeddings (word vectors).   \n",
    "\n",
    "If you want to remove certain characters in *all* tokens in your documents, you can use [remove_chars](api.rst#tmtoolkit.corpus.remove_chars) and pass it a sequence of characters to remove. There is also a shortcut [remove_punctuation](api.rst#tmtoolkit.corpus.remove_punctuation) which will remove all punctuation characters (all characters in [string.punction](https://docs.python.org/3/library/string.html#string.punctuation) by default) *in* tokens. This means, a token like \"vs.\" will be transformed to \"vs\" and a token \",\" will be transformed to an empty token \"\". It's useful to also remove empty tokens and we will do that in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): doj : 2 russian spy indict in Yahoo hack    Washin...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): dutch election : high turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): Trump vs America : the fight for democracy    Frid...\n",
      "> NewsArticles-469 (398 tokens): warning of tight time ahead for insurer    analyst...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode release ' Spirit , ' an unusually pol...\n",
      "> NewsArticles-2712 (571 tokens): grieve family speak out as police hunt for killer ...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seek more time on Trump wiretappe inquiry    J...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - back rebel in ' near full control ' of Al...\n",
      "> NewsArticles-3428 (776 tokens): in Breakthrough Discovery , Scientists Mass - Prod...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 7287\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import remove_chars\n",
    "\n",
    "# remove only full stops \".\"\n",
    "remove_chars(corpus_norm, ['.'])\n",
    "print_summary(corpus_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): doj  2 russian spy indict in Yahoo hack  Washingto...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face  off in dutch general elect...\n",
      "> NewsArticles-2487 (1015 tokens): dutch election  high turnout in key national vote ...\n",
      "> NewsArticles-49 (1112 tokens): Trump vs America  the fight for democracy  Frida G...\n",
      "> NewsArticles-469 (398 tokens): warning of tight time ahead for insurer  analyst w...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode release  Spirit   an unusually politi...\n",
      "> NewsArticles-2712 (571 tokens): grieve family speak out as police hunt for killer ...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seek more time on Trump wiretappe inquiry  Jus...\n",
      "> NewsArticles-1377 (774 tokens): Turkey  back rebel in  near full control  of Al Ba...\n",
      "> NewsArticles-3428 (776 tokens): in Breakthrough Discovery  Scientists Mass  Produc...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 7203\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import remove_punctuation\n",
    "\n",
    "# remove all punctuation\n",
    "remove_punctuation(corpus_norm)\n",
    "print_summary(corpus_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the vocabulary size (the number of unique tokens in a corpus) was also reduced with each step. We can also confirm that our functions worked by comparing the set of characters used in the original corpus to the set of characters used at the current normalization step via [`corpus_unique_chars`](api.rst#TODO). We can see that there are no more punctuation characters in the latter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['\\n',\n",
       "  ' ',\n",
       "  '!',\n",
       "  '\"',\n",
       "  '#',\n",
       "  '$',\n",
       "  '%',\n",
       "  '&',\n",
       "  \"'\",\n",
       "  '(',\n",
       "  ')',\n",
       "  '*',\n",
       "  '+',\n",
       "  ',',\n",
       "  '-',\n",
       "  '.',\n",
       "  '/',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2',\n",
       "  ...],\n",
       " ['0',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '4',\n",
       "  '5',\n",
       "  '6',\n",
       "  '7',\n",
       "  '8',\n",
       "  '9',\n",
       "  'A',\n",
       "  'B',\n",
       "  'C',\n",
       "  'D',\n",
       "  'E',\n",
       "  'F',\n",
       "  'G',\n",
       "  'H',\n",
       "  'I',\n",
       "  'J',\n",
       "  ...])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import corpus_unique_chars\n",
    "\n",
    "sorted(corpus_unique_chars(corpus_orig)), sorted(corpus_unique_chars(corpus_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common (but harsh) practice is to transform all tokens to lowercase forms, which can be done with [to_lowercase](api.rst#tmtoolkit.corpus.to_lowercase), as already shown before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): doj  2 russian spy indict in yahoo hack  washingto...\n",
      "> NewsArticles-2225 (539 tokens): rutte and wilders face  off in dutch general elect...\n",
      "> NewsArticles-2487 (1015 tokens): dutch election  high turnout in key national vote ...\n",
      "> NewsArticles-49 (1112 tokens): trump vs america  the fight for democracy  frida g...\n",
      "> NewsArticles-469 (398 tokens): warning of tight time ahead for insurer  analyst w...\n",
      "> NewsArticles-2766 (700 tokens): depeche mode release  spirit   an unusually politi...\n",
      "> NewsArticles-2712 (571 tokens): grieve family speak out as police hunt for killer ...\n",
      "> NewsArticles-2301 (464 tokens): doj seek more time on trump wiretappe inquiry  jus...\n",
      "> NewsArticles-1377 (774 tokens): turkey  back rebel in  near full control  of al ba...\n",
      "> NewsArticles-3428 (776 tokens): in breakthrough discovery  scientists mass  produc...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 6692\n"
     ]
    }
   ],
   "source": [
    "to_lowercase(corpus_norm)\n",
    "print_summary(corpus_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways on how to treat numbers in text. You may choose to leave them untreated, remove them completely or transform them to placeholders that only encode their magnitude. Number removal can be applied via [`filter_clean_tokens`](api.rst#TODO) which I will present later. Number transformation to magnitudes can be done via [`numbers_to_magnitudes`](api.rst#TODO) which I will show now. But first, let's get an overview about the numbers used in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>like_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>64</td>\n",
       "      <td>fifteen</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>96</td>\n",
       "      <td>one</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>104</td>\n",
       "      <td>four</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>535</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>563</td>\n",
       "      <td>four</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59253</th>\n",
       "      <td>NewsArticles-901</td>\n",
       "      <td>856</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59256</th>\n",
       "      <td>NewsArticles-901</td>\n",
       "      <td>859</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59374</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>63</td>\n",
       "      <td>2021</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59400</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>89</td>\n",
       "      <td>2010</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59413</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>102</td>\n",
       "      <td>1550</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position    token  like_num\n",
       "288    NewsArticles-1119        64  fifteen      True\n",
       "320    NewsArticles-1119        96      one      True\n",
       "328    NewsArticles-1119       104     four      True\n",
       "759    NewsArticles-1119       535      100      True\n",
       "787    NewsArticles-1119       563     four      True\n",
       "...                  ...       ...      ...       ...\n",
       "59253   NewsArticles-901       856       85      True\n",
       "59256   NewsArticles-901       859        9      True\n",
       "59374   NewsArticles-960        63     2021      True\n",
       "59400   NewsArticles-960        89     2010      True\n",
       "59413   NewsArticles-960       102     1550      True\n",
       "\n",
       "[1139 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_norm_num = tokens_table(corpus_norm, with_attr='like_num')\n",
    "table_norm_num[table_norm_num.like_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all tokens that were detected as \"number-like\" by SpaCy. Those that consist of digits can be converted to their respective magnitudes using the mentioned `numbers_to_magnitudes` function. This function has many options for customization, but by default a two digits number will be converted to \"10\", a three digits number to \"100\", a ten digits number to \"1,000,000,000\", etc. You may customize this output, e.g. so that all numbers are converted to the form \"NNN...\". You can further drop or keep signs, use thousands separators, etc. Depending on your research context, it may or may not make sense to reduce numbers in such a way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>like_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>64</td>\n",
       "      <td>fifteen</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>96</td>\n",
       "      <td>one</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>104</td>\n",
       "      <td>four</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>535</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>563</td>\n",
       "      <td>four</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59253</th>\n",
       "      <td>NewsArticles-901</td>\n",
       "      <td>856</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59256</th>\n",
       "      <td>NewsArticles-901</td>\n",
       "      <td>859</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59374</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>63</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59400</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>89</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59413</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>102</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position    token  like_num\n",
       "288    NewsArticles-1119        64  fifteen      True\n",
       "320    NewsArticles-1119        96      one      True\n",
       "328    NewsArticles-1119       104     four      True\n",
       "759    NewsArticles-1119       535      100      True\n",
       "787    NewsArticles-1119       563     four      True\n",
       "...                  ...       ...      ...       ...\n",
       "59253   NewsArticles-901       856       10      True\n",
       "59256   NewsArticles-901       859        1      True\n",
       "59374   NewsArticles-960        63     1000      True\n",
       "59400   NewsArticles-960        89     1000      True\n",
       "59413   NewsArticles-960       102     1000      True\n",
       "\n",
       "[1139 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import numbers_to_magnitudes\n",
    "\n",
    "numbers_to_magnitudes(corpus_norm)\n",
    "table_norm_num = tokens_table(corpus_norm, with_attr='like_num')\n",
    "table_norm_num[table_norm_num.like_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all numbers with digits were converted to their respective magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [filter_clean_tokens](api.rst#tmtoolkit.preprocess.TMPreproc.clean_tokens) finally applies several steps that remove tokens that meet certain criteria. This includes removing:\n",
    "\n",
    "- punctuation tokens (i.e. all tokens with attribute `is_punct` set to `True`)\n",
    "- stopwords (very common words for the given language, i.e. all tokens with attribute `is_stop` set to `True`)\n",
    "- empty tokens (i.e. `''`)\n",
    "- tokens that are longer or shorter than a certain number of characters\n",
    "- numbers  \n",
    "\n",
    "This method has many parameters to tweak, so it's recommended to check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (413 tokens): doj 1 russian spy indict yahoo hack washington cnn...\n",
      "> NewsArticles-2225 (258 tokens): rutte wilders face dutch general election 10 milli...\n",
      "> NewsArticles-2487 (471 tokens): dutch election high turnout key national vote cruc...\n",
      "> NewsArticles-49 (523 tokens): trump vs america fight democracy frida ghitis worl...\n",
      "> NewsArticles-469 (227 tokens): warning tight time ahead insurer analyst warn jan ...\n",
      "> NewsArticles-2766 (309 tokens): depeche mode release spirit unusually political al...\n",
      "> NewsArticles-2712 (215 tokens): grieve family speak police hunt killer 1 colorado ...\n",
      "> NewsArticles-2301 (216 tokens): doj seek time trump wiretappe inquiry justice depa...\n",
      "> NewsArticles-1377 (398 tokens): turkey back rebel near control al bab turkey defen...\n",
      "> NewsArticles-3428 (400 tokens): breakthrough discovery scientists mass produce art...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 28042 / vocabulary size: 6268\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import filter_clean_tokens\n",
    "\n",
    "# remove punct., stopwords, empty tokens (this is the default)\n",
    "# plus tokens shorter than 2 characters\n",
    "filter_clean_tokens(corpus_norm, remove_shorter_than=2)\n",
    "print_summary(corpus_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the removal of several tokens in the previous steps, the overall number of tokens was almost halved as compared to the original corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59598, 28042)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import corpus_num_tokens\n",
    "\n",
    "corpus_num_tokens(corpus_orig), corpus_num_tokens(corpus_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe that the vocabulary got smaller after the processing steps, which, for large corpora, is also important in terms of computation time and memory consumption for later analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9223, 6268)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size(corpus_orig), vocabulary_size(corpus_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also apply custom token transform functions by using [transform_tokens](api.rst#tmtoolkit.corpus.transform_tokens) and passing it a function that should be applied to each token in each document (hence it must accept one string argument).\n",
    "\n",
    "First let's define such a function. Here we create a simple function that should return a token's \"shape\" in terms of the case of its characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('XX', 'XxxxxXxxx', 'xxxxx')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_shape(t):\n",
    "    return ''.join(['X' if str.isupper(c) else 'x' for c in t])\n",
    "\n",
    "token_shape('EU'), token_shape('CamelCase'), token_shape('lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply this function to our documents (we will use the original documents here, because they were not transformed to lower case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): XXX x x Xxxxxxx xxxxx xxxxxxxx xx Xxxxx xxxx xx Xx...\n",
      "> NewsArticles-2225 (539 tokens): Xxxxx xxx Xxxxxxx xxxx x xxx xx Xxxxx xxxxxxx xxxx...\n",
      "> NewsArticles-2487 (1015 tokens): Xxxxx xxxxxxxx x Xxxx xxxxxxx xx xxx xxxxxxxx xxxx...\n",
      "> NewsArticles-49 (1112 tokens): Xxxxx xxx Xxxxxxx x Xxx xxxxx xxx xxxxxxxxx xx Xxx...\n",
      "> NewsArticles-469 (398 tokens): Xxxxxxx xx xxxxx xxxxx xxxxx xxx xxxxxxxx xx Xxxxx...\n",
      "> NewsArticles-2766 (700 tokens): Xxxxxxx Xxxx xxxxxxxx x Xxxxxx x x xx xxxxxxxxx xx...\n",
      "> NewsArticles-2712 (571 tokens): Xxxxxxxx xxxxxxxx xxxxx xxx xx xxxxxx xxxx xxx xxx...\n",
      "> NewsArticles-2301 (464 tokens): XXX xxxxx xxxx xxxx xx Xxxxx xxxxxxxxxxx xxxxxxx x...\n",
      "> NewsArticles-1377 (774 tokens): Xxxxxx x xxxxxx xxxxxx xx x xxxx xxxx xxxxxxx x xx...\n",
      "> NewsArticles-3428 (776 tokens): Xx Xxxxxxxxxxxx Xxxxxxxxx x Xxxxxxxxxx Xxxx x Xxxx...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59598 / vocabulary size: 176\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import transform_tokens\n",
    "\n",
    "corpus_shapes = transform_tokens(corpus_orig, func=token_shape, inplace=False)\n",
    "print_summary(corpus_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several more token transforming functions available in tmtoolkit. These are listed in the [corpus module API](api.rst#TODO). There are for example functions to simplify or normalize unicode characters in tokens.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Retokenization\n",
    "    \n",
    "One important point to note is that although you may change a token's text via a transformation, its token attributes such as POS tag, lemma, etc. stay the same. This is because the SpaCy NLP pipeline is only run initially and in most of the cases this is fine. However, if you want to re-run the NLP pipeline after you've applied some transformations in order to re-tokenize and re-analyze the text so that token attributes are also updated, you should run the [`retokenize`](api.rst#TODO) function.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and joining token collocations\n",
    "\n",
    "*Collocations* are tokens that occur together in a series frequently (i.e. more than would be expected by chance). Examples could be the collocations \"United\", \"States\" or \"Bank\", \"of\", \"America\". The tmtoolkit package provides functions for identifying and joining such series of tokens.\n",
    "\n",
    "For identifying collocations, you can use [`corpus_collocations`](api.rst#TODO). By default, it will produce a dataframe ranked by a collocation statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collocation</th>\n",
       "      <th>statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capsule falcon</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manchester victoria</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consulate deluge</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dominique ingres1814</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>carl jr</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>petroleum fundwill</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>russians consume</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v8 fords</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cristie clare</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>solder alloy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            collocation  statistic\n",
       "0        capsule falcon        1.0\n",
       "1   manchester victoria        1.0\n",
       "2      consulate deluge        1.0\n",
       "3  dominique ingres1814        1.0\n",
       "4               carl jr        1.0\n",
       "5    petroleum fundwill        1.0\n",
       "6      russians consume        1.0\n",
       "7              v8 fords        1.0\n",
       "8         cristie clare        1.0\n",
       "9          solder alloy        1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import corpus_collocations\n",
    "\n",
    "corpus_collocations(corpus_norm).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default statistic is the normalized variant of the *pointwise mutual information (NPMI)* measure implemented in the [`pmi`](api.rst#TODO) function. You can use a different statistic via the `statistic` argument. Here, we use the PMI³ statistic from the [`tokenseq`](api.rst#tmtoolkit.tokenseq) module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collocation</th>\n",
       "      <th>statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>le pen</td>\n",
       "      <td>-7.143448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white house</td>\n",
       "      <td>-8.012485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>real estate</td>\n",
       "      <td>-8.032940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kuala lumpur</td>\n",
       "      <td>-8.162017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>donald trump</td>\n",
       "      <td>-8.184953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>united states</td>\n",
       "      <td>-8.216084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reddit newsvine</td>\n",
       "      <td>-8.295549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>digg reddit</td>\n",
       "      <td>-8.295549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>newsvine permalink</td>\n",
       "      <td>-8.295549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tumblr linkedin</td>\n",
       "      <td>-8.295549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          collocation  statistic\n",
       "0              le pen  -7.143448\n",
       "1         white house  -8.012485\n",
       "2         real estate  -8.032940\n",
       "3        kuala lumpur  -8.162017\n",
       "4        donald trump  -8.184953\n",
       "5       united states  -8.216084\n",
       "6     reddit newsvine  -8.295549\n",
       "7         digg reddit  -8.295549\n",
       "8  newsvine permalink  -8.295549\n",
       "9     tumblr linkedin  -8.295549"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.tokenseq import pmi3\n",
    "\n",
    "corpus_collocations(corpus_norm, statistic=pmi3).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few more parameters to `corpus_collocations`. For instance, you can specify a threshold value via `threshold` and produce a list output instead of a dataframe via `as_table=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('le pen', -7.143447542606667),\n",
       " ('white house', -8.012485389630276),\n",
       " ('real estate', -8.0329400813261),\n",
       " ('kuala lumpur', -8.162017123601242),\n",
       " ('donald trump', -8.184952560000124),\n",
       " ('united states', -8.216084344871515)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_collocations(corpus_norm, statistic=pmi3, threshold=-8.25, as_table=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying and investigating collocations, you may also choose to join some of them so that they form a single token. This can be done via [`join_collocations_by_statistic`](api.rst#TODO). Here, we specify the collocation statistic to use, we set a minimum threshold and also enable returning a set of actually joint tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'donald_trump',\n",
       " 'kuala_lumpur',\n",
       " 'le_pen',\n",
       " 'real_estate',\n",
       " 'united_states',\n",
       " 'white_house'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import join_collocations_by_statistic\n",
    "\n",
    "join_collocations_by_statistic(corpus_norm, statistic=pmi3, threshold=-8.25, return_joint_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six collocations were joint to form single tokens. We can check one of them to see in which documents this joint token appears by using the `find_documents` function. This function searches the documents for matches to one or more keywords or patterns. By default, it returns all documents with at least one match as dictionary that maps document labels to number of matches. Here, we use the common parameter `as_table` again to provide a tabular output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>n_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-3353</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-2240</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-2641</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NewsArticles-1860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  n_matches\n",
       "0    NewsArticles-49          5\n",
       "1  NewsArticles-3353          2\n",
       "3    NewsArticles-72          2\n",
       "2  NewsArticles-2240          1\n",
       "4  NewsArticles-2641          1\n",
       "5  NewsArticles-1860          1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import find_documents\n",
    "\n",
    "find_documents(corpus_norm, 'united_states', as_table='-n_matches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for joining collocations is using the [`join_collocations_by_patterns`](api.rst#TODO) function, which allows you to define a pattern of subsequent tokens that should be joint. Here, we want to join all subsequent tokens where the first token is \"north\" and the second is anything that starts with \"korea*\", i.e. matching for example \"north\", \"korea\" or \"north\", \"korean\". The pattern \"korea*\" is a *glob pattern* – details on pattern matching will be given in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'north_korea', 'north_korean', 'north_koreans'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import join_collocations_by_patterns\n",
    "\n",
    "join_collocations_by_patterns(corpus_norm, ['north', 'korea*'], match_type='glob', return_joint_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that three collocation patterns were joint. Again, we can find the documents that contain these patterns using a glob pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>n_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1587</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1860</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  n_matches\n",
       "0  NewsArticles-1587         18\n",
       "1  NewsArticles-1860         11"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_documents(corpus_norm, 'north_korea*', match_type='glob' , as_table='-n_matches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords-in-context (KWIC) and general filtering methods\n",
    "\n",
    "*Keywords-in-context (KWIC)* allow you to quickly investigate certain keywords and their neighborhood of tokens, i.e. the tokens that appear right before and after this keyword.\n",
    "\n",
    "There are three corpus functions for this purpose:\n",
    "\n",
    "- [`kwic`](api.rst#TODO) is the base function accepting a search pattern and several options that control how the search pattern is matched (more on that below); use this function when you want to further process the output of a KWIC search;\n",
    "- [`kwic_table`](api.rst#TODO) is the more \"user friendly\" version of the above function as it produces a dataframe with the highlighted keyword by default;\n",
    "- [`filter_tokens_with_kwic`](api.rst#TODO) works similar to the above functions but applies the result by filtering the documents; it is explained in the [section on filtering](#Filtering-tokens-and-documents);\n",
    "\n",
    "Let's see the KWIC functions in action. We will start with `kwic` and use the original, unprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NewsArticles-2433': [],\n",
       " 'NewsArticles-2225': [],\n",
       " 'NewsArticles-2487': [['Dutch', 'lower', 'house', 'of', 'parliament']],\n",
       " 'NewsArticles-49': [['by', 'White', 'House', 'bullying', '.']],\n",
       " 'NewsArticles-469': [],\n",
       " 'NewsArticles-2766': [],\n",
       " 'NewsArticles-2712': [],\n",
       " 'NewsArticles-2301': [['on', 'the', 'House', 'of', 'Representatives'],\n",
       "  ['to', 'the', 'House', 'Intelligence', 'Committee'],\n",
       "  ['The', 'White', 'House', 'on', 'Monday']],\n",
       " 'NewsArticles-1377': [],\n",
       " 'NewsArticles-3428': [],\n",
       " 'NewsArticles-3208': [],\n",
       " 'NewsArticles-2156': [],\n",
       " 'NewsArticles-2143': [],\n",
       " 'NewsArticles-2730': [],\n",
       " 'NewsArticles-3159': [],\n",
       " 'NewsArticles-3201': [],\n",
       " 'NewsArticles-3353': [['purchased', 'a', 'house', 'outside', 'Mexico']],\n",
       " 'NewsArticles-355': [],\n",
       " 'NewsArticles-422': [],\n",
       " 'NewsArticles-2867': [['along', 'White', 'House', 'fence', '\\n\\n'],\n",
       "  ['the', 'White', 'House', ',', 'sources'],\n",
       "  ['the', 'White', 'House', 'fence', ','],\n",
       "  ['    ', 'White', 'House', 'press', 'secretary'],\n",
       "  ['the', 'White', 'House', 'fence', '.'],\n",
       "  ['a', 'White', 'House', 'fence', 'last']],\n",
       " ...}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import kwic\n",
    "\n",
    "kwic(corpus_orig, 'house', ignore_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a dictionary that maps document labels to the KWIC results. Each document contains a list of \"contexts\", i.e. a list of tokens that surround a keyword, here `\"house\"`. This keyword stands in the middle and is surrounded by its \"context tokens\", which by default means two tokens to the left and two tokens to the right (which may be less when the keyword is near the start or the end of a document). \n",
    "\n",
    "We can see that \"NewsArticles-2487\" and \"NewsArticles-49\" contain one context, \"NewsArticles-2301\" contains three contexts, etc., but most documents don't contain the search pattern and hence provide an empty list as result.\n",
    "\n",
    "With `kwic_table`, we get back a dataframe which provides a better formatting for quick investigation. See how the matched tokens are highlighted as `*house*` and empty results are removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>context</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>0</td>\n",
       "      <td>new White *House* is being</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>1</td>\n",
       "      <td>his White *House* was in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>2</td>\n",
       "      <td>his White *House* and a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1263</td>\n",
       "      <td>0</td>\n",
       "      <td>near our *house* . I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1546</td>\n",
       "      <td>0</td>\n",
       "      <td>of White *House* counselor -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1610</td>\n",
       "      <td>0</td>\n",
       "      <td>. White *House* spokesman Sean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2132</td>\n",
       "      <td>0</td>\n",
       "      <td>the White *House* . \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2132</td>\n",
       "      <td>1</td>\n",
       "      <td>a White *House* gathering of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2301</td>\n",
       "      <td>0</td>\n",
       "      <td>on the *House* of Representatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2301</td>\n",
       "      <td>1</td>\n",
       "      <td>to the *House* Intelligence Committee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-2301</td>\n",
       "      <td>2</td>\n",
       "      <td>The White *House* on Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2431</td>\n",
       "      <td>0</td>\n",
       "      <td>in the *House* of Commons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2487</td>\n",
       "      <td>0</td>\n",
       "      <td>Dutch lower *house* of parliament</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2641</td>\n",
       "      <td>0</td>\n",
       "      <td>a contemporary *house* her impromptu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2867</td>\n",
       "      <td>0</td>\n",
       "      <td>along White *House* fence \\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2867</td>\n",
       "      <td>1</td>\n",
       "      <td>the White *House* , sources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-2867</td>\n",
       "      <td>2</td>\n",
       "      <td>the White *House* fence ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-2867</td>\n",
       "      <td>3</td>\n",
       "      <td>White *House* press secretary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-2867</td>\n",
       "      <td>4</td>\n",
       "      <td>the White *House* fence .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NewsArticles-2867</td>\n",
       "      <td>5</td>\n",
       "      <td>a White *House* fence last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-3094</td>\n",
       "      <td>0</td>\n",
       "      <td>*House* Intel Chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-3094</td>\n",
       "      <td>1</td>\n",
       "      <td>of the *House* Intelligence Committee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-3094</td>\n",
       "      <td>2</td>\n",
       "      <td>both the *House* and Senate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-3094</td>\n",
       "      <td>3</td>\n",
       "      <td>, the *House* Intelligence Committee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-3116</td>\n",
       "      <td>0</td>\n",
       "      <td>of the *House* Intelligence Committee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-3156</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\n US *House* Republicans are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-3156</td>\n",
       "      <td>1</td>\n",
       "      <td>programme , *House* Speaker Paul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-3156</td>\n",
       "      <td>2</td>\n",
       "      <td>on the *House* of Representatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-3156</td>\n",
       "      <td>3</td>\n",
       "      <td>in the *House* , a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-3156</td>\n",
       "      <td>4</td>\n",
       "      <td>in the *House* . Republicans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-3353</td>\n",
       "      <td>0</td>\n",
       "      <td>purchased a *house* outside Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-3739</td>\n",
       "      <td>0</td>\n",
       "      <td>the White *House* staff is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-3739</td>\n",
       "      <td>1</td>\n",
       "      <td>the White *House* counsel to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-3739</td>\n",
       "      <td>2</td>\n",
       "      <td>former White *House* ethics czar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-3739</td>\n",
       "      <td>3</td>\n",
       "      <td>the White *House* in its</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-3739</td>\n",
       "      <td>4</td>\n",
       "      <td>the White *House* as an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NewsArticles-3739</td>\n",
       "      <td>5</td>\n",
       "      <td>the White *House* Office ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NewsArticles-3739</td>\n",
       "      <td>6</td>\n",
       "      <td>the White *House* counsel and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>0</td>\n",
       "      <td>by White *House* bullying .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-681</td>\n",
       "      <td>0</td>\n",
       "      <td>the White *House* to head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-681</td>\n",
       "      <td>1</td>\n",
       "      <td>the White *House* 's messaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-72</td>\n",
       "      <td>0</td>\n",
       "      <td>\" The *House* has always</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-72</td>\n",
       "      <td>1</td>\n",
       "      <td>to the *House* . \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-72</td>\n",
       "      <td>2</td>\n",
       "      <td>for the *House* to proceed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-72</td>\n",
       "      <td>3</td>\n",
       "      <td>by this *House* . \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-72</td>\n",
       "      <td>4</td>\n",
       "      <td>of the *House* of Lords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NewsArticles-72</td>\n",
       "      <td>5</td>\n",
       "      <td>of the *House* of Commons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  context                                  token\n",
       "0  NewsArticles-1119        0             new White *House* is being\n",
       "1  NewsArticles-1119        1               his White *House* was in\n",
       "2  NewsArticles-1119        2                his White *House* and a\n",
       "0  NewsArticles-1263        0                   near our *house* . I\n",
       "0  NewsArticles-1546        0           of White *House* counselor -\n",
       "0  NewsArticles-1610        0         . White *House* spokesman Sean\n",
       "0  NewsArticles-2132        0                  the White *House* . \"\n",
       "1  NewsArticles-2132        1           a White *House* gathering of\n",
       "0  NewsArticles-2301        0      on the *House* of Representatives\n",
       "1  NewsArticles-2301        1  to the *House* Intelligence Committee\n",
       "2  NewsArticles-2301        2            The White *House* on Monday\n",
       "0  NewsArticles-2431        0              in the *House* of Commons\n",
       "0  NewsArticles-2487        0      Dutch lower *house* of parliament\n",
       "0  NewsArticles-2641        0   a contemporary *house* her impromptu\n",
       "0  NewsArticles-2867        0         along White *House* fence \\n\\n\n",
       "1  NewsArticles-2867        1            the White *House* , sources\n",
       "2  NewsArticles-2867        2              the White *House* fence ,\n",
       "3  NewsArticles-2867        3          White *House* press secretary\n",
       "4  NewsArticles-2867        4              the White *House* fence .\n",
       "5  NewsArticles-2867        5             a White *House* fence last\n",
       "0  NewsArticles-3094        0                    *House* Intel Chair\n",
       "1  NewsArticles-3094        1  of the *House* Intelligence Committee\n",
       "2  NewsArticles-3094        2            both the *House* and Senate\n",
       "3  NewsArticles-3094        3   , the *House* Intelligence Committee\n",
       "0  NewsArticles-3116        0  of the *House* Intelligence Committee\n",
       "0  NewsArticles-3156        0        \\n\\n US *House* Republicans are\n",
       "1  NewsArticles-3156        1       programme , *House* Speaker Paul\n",
       "2  NewsArticles-3156        2      on the *House* of Representatives\n",
       "3  NewsArticles-3156        3                     in the *House* , a\n",
       "4  NewsArticles-3156        4           in the *House* . Republicans\n",
       "0  NewsArticles-3353        0     purchased a *house* outside Mexico\n",
       "0  NewsArticles-3739        0             the White *House* staff is\n",
       "1  NewsArticles-3739        1           the White *House* counsel to\n",
       "2  NewsArticles-3739        2       former White *House* ethics czar\n",
       "3  NewsArticles-3739        3               the White *House* in its\n",
       "4  NewsArticles-3739        4                the White *House* as an\n",
       "5  NewsArticles-3739        5             the White *House* Office ,\n",
       "6  NewsArticles-3739        6          the White *House* counsel and\n",
       "0    NewsArticles-49        0            by White *House* bullying .\n",
       "0   NewsArticles-681        0              the White *House* to head\n",
       "1   NewsArticles-681        1         the White *House* 's messaging\n",
       "0    NewsArticles-72        0               \" The *House* has always\n",
       "1    NewsArticles-72        1                     to the *House* . \"\n",
       "2    NewsArticles-72        2             for the *House* to proceed\n",
       "3    NewsArticles-72        3                    by this *House* . \"\n",
       "4    NewsArticles-72        4                of the *House* of Lords\n",
       "5    NewsArticles-72        5              of the *House* of Commons"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import kwic_table\n",
    "\n",
    "kwic_table(corpus_orig, 'house', ignore_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter is `context_size`. It determines the number of tokens to display left and right to the found keyword. You can either pass a single integer for a symmetric context or a tuple with integers `(<left>, <right>)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>context</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>0</td>\n",
       "      <td>way his new White *House* is being portrayed and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>1</td>\n",
       "      <td>reports that his White *House* was in chaos ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>2</td>\n",
       "      <td>coverage of his White *House* and a desire to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1263</td>\n",
       "      <td>0</td>\n",
       "      <td>militants exploded near our *house* . I was fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1546</td>\n",
       "      <td>0</td>\n",
       "      <td>bizarre image of White *House* counselor - Kel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1610</td>\n",
       "      <td>0</td>\n",
       "      <td>he said . White *House* spokesman Sean Spicer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2132</td>\n",
       "      <td>0</td>\n",
       "      <td>Cabinet at the White *House* . \" Hopefully we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2132</td>\n",
       "      <td>1</td>\n",
       "      <td>He told a White *House* gathering of Americans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2301</td>\n",
       "      <td>0</td>\n",
       "      <td>from lawmakers on the *House* of Representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2301</td>\n",
       "      <td>1</td>\n",
       "      <td>by Monday to the *House* Intelligence Committe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  context  \\\n",
       "0  NewsArticles-1119        0   \n",
       "1  NewsArticles-1119        1   \n",
       "2  NewsArticles-1119        2   \n",
       "0  NewsArticles-1263        0   \n",
       "0  NewsArticles-1546        0   \n",
       "0  NewsArticles-1610        0   \n",
       "0  NewsArticles-2132        0   \n",
       "1  NewsArticles-2132        1   \n",
       "0  NewsArticles-2301        0   \n",
       "1  NewsArticles-2301        1   \n",
       "\n",
       "                                               token  \n",
       "0   way his new White *House* is being portrayed and  \n",
       "1      reports that his White *House* was in chaos ,  \n",
       "2      coverage of his White *House* and a desire to  \n",
       "0  militants exploded near our *house* . I was fr...  \n",
       "0  bizarre image of White *House* counselor - Kel...  \n",
       "0  he said . White *House* spokesman Sean Spicer ...  \n",
       "0      Cabinet at the White *House* . \" Hopefully we  \n",
       "1  He told a White *House* gathering of Americans...  \n",
       "0  from lawmakers on the *House* of Representativ...  \n",
       "1  by Monday to the *House* Intelligence Committe...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 tokens to each side of the keyword (only display first 10 rows)\n",
    "kwic_table(corpus_orig, 'house', ignore_case=True, context_size=4).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>context</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>0</td>\n",
       "      <td>White *House* is being portrayed and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>1</td>\n",
       "      <td>White *House* was in chaos ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>2</td>\n",
       "      <td>White *House* and a desire to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1263</td>\n",
       "      <td>0</td>\n",
       "      <td>our *house* . I was frightened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1546</td>\n",
       "      <td>0</td>\n",
       "      <td>White *House* counselor - Kellyanne Conway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1610</td>\n",
       "      <td>0</td>\n",
       "      <td>White *House* spokesman Sean Spicer said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2132</td>\n",
       "      <td>0</td>\n",
       "      <td>White *House* . \" Hopefully we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2132</td>\n",
       "      <td>1</td>\n",
       "      <td>White *House* gathering of Americans who</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2301</td>\n",
       "      <td>0</td>\n",
       "      <td>the *House* of Representatives Intelligence Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2301</td>\n",
       "      <td>1</td>\n",
       "      <td>the *House* Intelligence Committee , which</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  context  \\\n",
       "0  NewsArticles-1119        0   \n",
       "1  NewsArticles-1119        1   \n",
       "2  NewsArticles-1119        2   \n",
       "0  NewsArticles-1263        0   \n",
       "0  NewsArticles-1546        0   \n",
       "0  NewsArticles-1610        0   \n",
       "0  NewsArticles-2132        0   \n",
       "1  NewsArticles-2132        1   \n",
       "0  NewsArticles-2301        0   \n",
       "1  NewsArticles-2301        1   \n",
       "\n",
       "                                               token  \n",
       "0               White *House* is being portrayed and  \n",
       "1                       White *House* was in chaos ,  \n",
       "2                      White *House* and a desire to  \n",
       "0                     our *house* . I was frightened  \n",
       "0         White *House* counselor - Kellyanne Conway  \n",
       "0           White *House* spokesman Sean Spicer said  \n",
       "0                     White *House* . \" Hopefully we  \n",
       "1           White *House* gathering of Americans who  \n",
       "0  the *House* of Representatives Intelligence Co...  \n",
       "1         the *House* Intelligence Committee , which  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 token to the left, 4 tokens to the right of the keyword (only display first 10 rows)\n",
    "kwic_table(corpus_orig, 'house', ignore_case=True, context_size=(1, 4)).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KWIC functions become really powerful when using the pattern matching options. So far, we were looking for *exact* (but case insensitive) matches between the corpus tokens and our keyword `\"house\"`. However, it is also possible to match patterns like `\"new*\"` (matches any token starting with \"new\") or `\"agenc(y|ies)\"` (a regular expression matching \"agency\" and \"agencies\"). The next section gives an introduction on the different options for pattern matching.\n",
    "\n",
    "#### Common parameters for pattern matching functions\n",
    "\n",
    "Several functions and methods in tmtoolkit support pattern matching, including the already mentioned function `find_documents` and the KWIC functions, but also functions for filtering tokens or documents as you will see later. They all share similar function signatures, i.e. similar parameters:\n",
    "\n",
    "- `search_token` or `search_tokens`: allows to specify one or more patterns as strings\n",
    "- `match_type`: sets the matching type and can be one of the following options:\n",
    "  - `'exact'` (default): exact string matching (optionally ignoring character case), i.e. no pattern matching\n",
    "  - `'regex'` uses [regular expression](https://docs.python.org/3/library/re.html) matching\n",
    "  - `'glob'` uses \"glob patterns\" like `\"politic*\"` which matches for example \"politic\", \"politics\" or \"politician\" (see [globre package](https://pypi.org/project/globre/))\n",
    "- `ignore_case`: ignore character case (applies to all three match types)\n",
    "- `glob_method`: if `match_type` is 'glob', use this glob method. Must be `'match'` or `'search'` (similar behavior as Python's [re.match](https://docs.python.org/3/library/re.html#re.match) or [re.search](https://docs.python.org/3/library/re.html#re.search))\n",
    "- `inverse`: inverse the match results, i.e. if matching for \"hello\", return all results that do *not* match \"hello\"\n",
    "\n",
    "Let's try out some of these options with `kwic_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>context</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>in various *agencies* who had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>0</td>\n",
       "      <td>Anadolu news *agency* . Earlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>1</td>\n",
       "      <td>and news *agencies*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1561</td>\n",
       "      <td>0</td>\n",
       "      <td>National Crime *Agency* every month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1561</td>\n",
       "      <td>1</td>\n",
       "      <td>National Crime *Agency* for specialist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1610</td>\n",
       "      <td>0</td>\n",
       "      <td>. Source:-News *agencies*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1860</td>\n",
       "      <td>0</td>\n",
       "      <td>state news *agency* . The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2156</td>\n",
       "      <td>0</td>\n",
       "      <td>ministries and *agencies* in all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2301</td>\n",
       "      <td>0</td>\n",
       "      <td>. Source:-News *agencies*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>said the *agency* has not</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  context                                   token\n",
       "0  NewsArticles-1100        0           in various *agencies* who had\n",
       "0  NewsArticles-1377        0         Anadolu news *agency* . Earlier\n",
       "1  NewsArticles-1377        1                     and news *agencies*\n",
       "0  NewsArticles-1561        0     National Crime *Agency* every month\n",
       "1  NewsArticles-1561        1  National Crime *Agency* for specialist\n",
       "0  NewsArticles-1610        0               . Source:-News *agencies*\n",
       "0  NewsArticles-1860        0               state news *agency* . The\n",
       "0  NewsArticles-2156        0        ministries and *agencies* in all\n",
       "0  NewsArticles-2301        0               . Source:-News *agencies*\n",
       "0  NewsArticles-2433        0               said the *agency* has not"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a regular expression, ignoring case (only display first 10 rows)\n",
    "kwic_table(corpus_orig, r'agenc(y|ies)', match_type='regex', ignore_case=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>context</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>0</td>\n",
       "      <td>modern American *political* history .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>1</td>\n",
       "      <td>than the *political* media ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>2</td>\n",
       "      <td>his own *poll* numbers ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>3</td>\n",
       "      <td>Washington 's *political* establishment and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>4</td>\n",
       "      <td>Trump among *political* elites in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1185</td>\n",
       "      <td>0</td>\n",
       "      <td>over a *police* - officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1185</td>\n",
       "      <td>1</td>\n",
       "      <td>the latest *police* violation to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1185</td>\n",
       "      <td>2</td>\n",
       "      <td>- against *police* brutality.-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1185</td>\n",
       "      <td>3</td>\n",
       "      <td>when the *police* stopped him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1185</td>\n",
       "      <td>4</td>\n",
       "      <td>by the *police* circulated on</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  context                                        token\n",
       "0  NewsArticles-1119        0        modern American *political* history .\n",
       "1  NewsArticles-1119        1                 than the *political* media ,\n",
       "2  NewsArticles-1119        2                     his own *poll* numbers ,\n",
       "3  NewsArticles-1119        3  Washington 's *political* establishment and\n",
       "4  NewsArticles-1119        4            Trump among *political* elites in\n",
       "0  NewsArticles-1185        0                    over a *police* - officer\n",
       "1  NewsArticles-1185        1             the latest *police* violation to\n",
       "2  NewsArticles-1185        2             - against *police* brutality.-  \n",
       "3  NewsArticles-1185        3                when the *police* stopped him\n",
       "4  NewsArticles-1185        4                by the *police* circulated on"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a glob, ignoring case (only display first 10 rows)\n",
    "kwic_table(corpus_orig, 'pol*', match_type='glob', ignore_case=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>context</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>0</td>\n",
       "      <td>leaks are *absolutely* real .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1185</td>\n",
       "      <td>0</td>\n",
       "      <td>the biggest *unresolved* problem is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1407</td>\n",
       "      <td>0</td>\n",
       "      <td>of Belfast *solicitor* - Pat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1587</td>\n",
       "      <td>0</td>\n",
       "      <td>North 's *isolationist* regime ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1587</td>\n",
       "      <td>1</td>\n",
       "      <td>in the *isolated* country ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1787</td>\n",
       "      <td>0</td>\n",
       "      <td>roundabout legal *solution* for an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1860</td>\n",
       "      <td>0</td>\n",
       "      <td>embraced the *isolated* state ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1860</td>\n",
       "      <td>1</td>\n",
       "      <td>have been *sold* to North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2152</td>\n",
       "      <td>0</td>\n",
       "      <td>\" rock *solid* \" support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2152</td>\n",
       "      <td>1</td>\n",
       "      <td>a negotiated *solution* and deny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  context                                token\n",
       "0  NewsArticles-1119        0        leaks are *absolutely* real .\n",
       "0  NewsArticles-1185        0  the biggest *unresolved* problem is\n",
       "0  NewsArticles-1407        0         of Belfast *solicitor* - Pat\n",
       "0  NewsArticles-1587        0     North 's *isolationist* regime ,\n",
       "1  NewsArticles-1587        1          in the *isolated* country ,\n",
       "0  NewsArticles-1787        0   roundabout legal *solution* for an\n",
       "0  NewsArticles-1860        0      embraced the *isolated* state ,\n",
       "1  NewsArticles-1860        1            have been *sold* to North\n",
       "0  NewsArticles-2152        0             \" rock *solid* \" support\n",
       "1  NewsArticles-2152        1     a negotiated *solution* and deny"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a glob, ignoring case (only display first 10 rows)\n",
    "kwic_table(corpus_orig, '*sol*', match_type='glob', ignore_case=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>context</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>investigate leaks *\\n\\n* President Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>his administration *.* The news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>leaks are *\"* very serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>very serious *.* \" \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>serious . *\"* \" I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>5</td>\n",
       "      <td>. \" *\"* I 've</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>6</td>\n",
       "      <td>the leaks *.* Those are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>7</td>\n",
       "      <td>criminal leaks *,* \" Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>8</td>\n",
       "      <td>leaks , *\"* Trump said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>9</td>\n",
       "      <td>this afternoon *.* \" We</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc  context                                     token\n",
       "0  NewsArticles-1100        0  investigate leaks *\\n\\n* President Trump\n",
       "1  NewsArticles-1100        1           his administration *.* The news\n",
       "2  NewsArticles-1100        2                leaks are *\"* very serious\n",
       "3  NewsArticles-1100        3                      very serious *.* \" \"\n",
       "4  NewsArticles-1100        4                         serious . *\"* \" I\n",
       "5  NewsArticles-1100        5                             . \" *\"* I 've\n",
       "6  NewsArticles-1100        6                   the leaks *.* Those are\n",
       "7  NewsArticles-1100        7                criminal leaks *,* \" Trump\n",
       "8  NewsArticles-1100        8                    leaks , *\"* Trump said\n",
       "9  NewsArticles-1100        9                   this afternoon *.* \" We"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a regex that matches all tokens with at least one vowel and\n",
    "# inverting these matches, i.e. all tokens *without* any vowels\n",
    "# (only display first 10 rows)\n",
    "kwic_table(corpus_orig, r'[AEIOUaeiou]', match_type='regex', inverse=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering tokens and documents\n",
    "\n",
    "We can use the pattern matching parameters in numerous filtering methods. The heart of many of these methods is [`token_match`](api.rst#tmtoolkit.tokenseq.token_match). Given a search pattern, a list of tokens and optionally some pattern matching parameters, it returns a boolean NumPy array of the same length as the input tokens. Each occurrence of `True` in this boolean array signals a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('President', False),\n",
       " ('Trump', False),\n",
       " ('says', False),\n",
       " ('he', False),\n",
       " ('has', False),\n",
       " ('asked', False),\n",
       " ('the', False),\n",
       " ('Justice', False),\n",
       " ('Department', False),\n",
       " ('to', True),\n",
       " ('investigate', False),\n",
       " ('leaks', False),\n",
       " ('\\n\\n', False),\n",
       " ('President', False),\n",
       " ('Trump', False),\n",
       " ('said', False),\n",
       " ('today', True),\n",
       " ('he', False),\n",
       " ('has', False),\n",
       " ('directed', False)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.tokenseq import token_match\n",
    "\n",
    "# first 20 tokens of document \"NewsArticles-1100\"\n",
    "doc_snippet = corpus_orig['NewsArticles-1100']['token'][:20]\n",
    "# get all tokens that match \"to*\"\n",
    "matches = token_match('to*', doc_snippet, match_type='glob')\n",
    "\n",
    "# show pair-wise results\n",
    "list(zip(doc_snippet, matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `token_match` function is a rather low-level function that you may use for pattern matching against any list/array of strings, e.g. a list of tokens, file names, etc.\n",
    "\n",
    "The following functions cover common use-cases for filtering during text preprocessing. Many of these functions start either with `filter_...()` or `remove_...()` and these pairs of filter and remove functions are complements. A *filter function* will always *retain* the matched elements whereas a *remove function* will always *drop* the matched elements. Note that a remove function is actually a shortcut for a filter function with the parameter `inverse=True`.\n",
    "\n",
    "We can observe that behavior with the first pair of functions, [`filter_tokens`](api.rst#tmtoolkit.corpus.filter_tokens) and [`remove_tokens`](api.rst#tmtoolkit.corpus.remove_tokens). Since these functions *modify* a corpus, you can again choose to make these modifications to the existing corpus object (\"in-place\") or return a modified corpus using the `inplace` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (0 tokens): \n",
      "> NewsArticles-2225 (0 tokens): \n",
      "> NewsArticles-2487 (1 tokens): house\n",
      "> NewsArticles-49 (1 tokens): House\n",
      "> NewsArticles-469 (0 tokens): \n",
      "> NewsArticles-2766 (0 tokens): \n",
      "> NewsArticles-2712 (0 tokens): \n",
      "> NewsArticles-2301 (3 tokens): House House House\n",
      "> NewsArticles-1377 (0 tokens): \n",
      "> NewsArticles-3428 (0 tokens): \n",
      "(and 90 more documents)\n",
      "total number of tokens: 53 / vocabulary size: 8\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import filter_tokens\n",
    "\n",
    "# retain only the tokens that match the pattern in each document\n",
    "corpus_filtered = filter_tokens(corpus_orig, '*house*', match_type='glob',\n",
    "                                ignore_case=True, inplace=False)\n",
    "print_summary(corpus_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (842 tokens): DOJ : 2 Russian spies indicted in Yahoo hack    Wa...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in Dutch general elec...\n",
      "> NewsArticles-2487 (1014 tokens): Dutch election : High turnout in key national vote...\n",
      "> NewsArticles-49 (1111 tokens): Trump vs. America : The fight for democracy    Fri...\n",
      "> NewsArticles-469 (398 tokens): Warning of tight times ahead for insurers    Analy...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode releases ' Spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): Grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (461 tokens): DOJ seeks more time on Trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): In Breakthrough Discovery , Scientists Mass - Prod...\n",
      "(and 90 more documents)\n",
      "total number of tokens: 59545 / vocabulary size: 9215\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import remove_tokens\n",
    "\n",
    "# remove the tokens that match the pattern in each document\n",
    "corpus_filtered = remove_tokens(corpus_orig, '*house*', match_type='glob',\n",
    "                                ignore_case=True, inplace=False)\n",
    "print_summary(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pair [`filter_documents`](api.rst#tmtoolkit.corpus.filter_documents) and [`remove_documents`](api.rst#tmtoolkit.corpus.remove_documents) works similarily, but filters or drops whole documents regarding the supplied match criteria. Both accept the standard pattern matching parameters, but also a parameter `matches_threshold` with default value `1`. When this number of matching tokens is hit, the document will be part of the result set (`filter_documents`) or removed from the result set (`remove_documents`). By this, we can for example retain only those documents that contain certain token patterns.\n",
    "\n",
    "Let's try out these functions in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 21 documents in English\n",
      "> NewsArticles-2487 (1015 tokens): Dutch election : High turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): Trump vs. America : The fight for democracy    Fri...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seeks more time on Trump wiretapping inquiry  ...\n",
      "> NewsArticles-3159 (154 tokens): Massachusetts panel opens hearings on recreational...\n",
      "> NewsArticles-3353 (1478 tokens): Undocumented migrants await Trump 's next move Jam...\n",
      "> NewsArticles-422 (986 tokens): Jackie Chan 's Indian adventure    Movie giant in ...\n",
      "> NewsArticles-2867 (170 tokens): Person detained after hopping bike - rack barrier ...\n",
      "> NewsArticles-2431 (1678 tokens): Will Europe ride the populist wave ? A visual guid...\n",
      "> NewsArticles-2132 (490 tokens): Trump on health care : ' It 's a big , fat , beaut...\n",
      "> NewsArticles-1119 (975 tokens): An amazing moment in history : Donald Trump 's pre...\n",
      "(and 11 more documents)\n",
      "total number of tokens: 16655 / vocabulary size: 3813\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import filter_documents\n",
    "\n",
    "corpus_filtered = filter_documents(corpus_orig, '*house*', match_type='glob',\n",
    "                                   ignore_case=True, inplace=False)\n",
    "print_summary(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 21 out of 100 documents contained the pattern `'*house*'` and hence were retained.\n",
    "\n",
    "We can also adjust `matches_threshold` to set the minimum number of token matches for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 5 documents in English\n",
      "> NewsArticles-2867 (170 tokens): Person detained after hopping bike - rack barrier ...\n",
      "> NewsArticles-72 (1054 tokens): Speaker John Bercow defends his comments on Donald...\n",
      "> NewsArticles-3156 (554 tokens): Republicans working on changes to healthcare overh...\n",
      "> NewsArticles-3094 (694 tokens): House Intel Chair : Trump Administration Documents...\n",
      "> NewsArticles-3739 (491 tokens): Trump ally : Ivanka Trump 's new gig is n't nepoti...\n",
      "total number of tokens: 2963 / vocabulary size: 936\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import filter_documents\n",
    "\n",
    "corpus_filtered = filter_documents(corpus_orig, '*house*', match_type='glob',\n",
    "                                   matches_threshold=4,\n",
    "                                   ignore_case=True, inplace=False)\n",
    "print_summary(corpus_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 79 documents in English\n",
      "> NewsArticles-2433 (842 tokens): DOJ : 2 Russian spies indicted in Yahoo hack    Wa...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in Dutch general elec...\n",
      "> NewsArticles-469 (398 tokens): Warning of tight times ahead for insurers    Analy...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode releases ' Spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): Grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): In Breakthrough Discovery , Scientists Mass - Prod...\n",
      "> NewsArticles-3208 (945 tokens): East Timor holds first presidential election since...\n",
      "> NewsArticles-2156 (276 tokens): Mongolian PM to visit Russian in first half of 201...\n",
      "> NewsArticles-2143 (596 tokens): Twitter Hilariously Burns Kellyanne Conway For Mic...\n",
      "(and 69 more documents)\n",
      "total number of tokens: 42943 / vocabulary size: 7573\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import remove_documents\n",
    "\n",
    "corpus_filtered = remove_documents(corpus_orig, '*house*', match_type='glob',\n",
    "                 ignore_case=True, inplace=False)\n",
    "print_summary(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `remove_documents` we get only the documents that did *not* contain the specified pattern. Since we had 21 documents that contained the \"house\" pattern, we now have the complement set with the 79 documents that don't contain this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful pair of functions is [`filter_documents_by_label`](api.rst#tmtoolkit.corpus.filter_documents_by_label) and [`remove_documents_by_label`](api.rst#tmtoolkit.corpus.remove_documents_by_label). Both functions again accept the same pattern matching parameters but they only apply them to the document names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 73 documents in English\n",
      "> NewsArticles-2433 (842 tokens): DOJ : 2 Russian spies indicted in Yahoo hack    Wa...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in Dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): Dutch election : High turnout in key national vote...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode releases ' Spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): Grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seeks more time on Trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): In Breakthrough Discovery , Scientists Mass - Prod...\n",
      "> NewsArticles-3208 (945 tokens): East Timor holds first presidential election since...\n",
      "> NewsArticles-2156 (276 tokens): Mongolian PM to visit Russian in first half of 201...\n",
      "(and 63 more documents)\n",
      "total number of tokens: 43114 / vocabulary size: 7598\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import filter_documents_by_label\n",
    "\n",
    "corpus_filtered = filter_documents_by_label(corpus_orig, r'-\\d{4}$',\n",
    "                                            match_type='regex', inplace=False)\n",
    "print_summary(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we wanted to retain only the documents whose document labels ended with exactly 4 digits, like \"...-1234\". Hence, we only get \"NewsArticles-1880\" and \"NewsArticles-3350\" but not \"NewsArticles-99\". Again, `filter_documents_by_label` will do the exact opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also use [Keywords-in-context (KWIC)](#Keywords-in-context-(KWIC)-and-general-filtering-methods) to filter your tokens in the neighborhood around certain keyword pattern(s). The method for that is called [`filter_tokens_with_kwic`](api.rst#tmtoolkit.corpus.filter_tokens_with_kwic) and works very similar to [`kwic`](api.rst#tmtoolkit.corpus.kwic), but filters the documents in the `Corpus` instance with which you can continue working as usual. Here, we filter the tokens in each document to get the tokens directly in front and after the glob pattern `'*house*'` (`context_size=1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 100 documents in English\n",
      "> NewsArticles-2433 (0 tokens): \n",
      "> NewsArticles-2225 (0 tokens): \n",
      "> NewsArticles-2487 (3 tokens): lower house of\n",
      "> NewsArticles-49 (3 tokens): White House bullying\n",
      "> NewsArticles-469 (0 tokens): \n",
      "> NewsArticles-2766 (0 tokens): \n",
      "> NewsArticles-2712 (0 tokens): \n",
      "> NewsArticles-2301 (9 tokens): the House of the House Intelligence White House on\n",
      "> NewsArticles-1377 (0 tokens): \n",
      "> NewsArticles-3428 (0 tokens): \n",
      "(and 90 more documents)\n",
      "total number of tokens: 158 / vocabulary size: 50\n"
     ]
    }
   ],
   "source": [
    "from tmtoolkit.corpus import filter_tokens_with_kwic\n",
    "\n",
    "corpus_filtered = filter_tokens_with_kwic(corpus_orig, '*house*',\n",
    "                                          context_size=1, match_type='glob',\n",
    "                                          ignore_case=True, inplace=False)\n",
    "print_summary(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your NLP pipeline annotated your documents' tokens with Part-of-Speech (POS) tags, you can also filter them using [`filter_for_pos`](api.rst#tmtoolkit.corpus.filter_for_pos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>President</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>President</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>Justice</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Justice</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>Department</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Department</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>leaks</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>leak</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>90</td>\n",
       "      <td>Putin</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Putin</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>91</td>\n",
       "      <td>Russia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Russia</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17596</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>92</td>\n",
       "      <td>capabilities</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>capability</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17597</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>93</td>\n",
       "      <td>priorities</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>priority</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>94</td>\n",
       "      <td>nation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>nation</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17599 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position         token  is_punct  is_stop  \\\n",
       "0      NewsArticles-1100         0     President     False    False   \n",
       "1      NewsArticles-1100         1         Trump     False    False   \n",
       "2      NewsArticles-1100         2       Justice     False    False   \n",
       "3      NewsArticles-1100         3    Department     False    False   \n",
       "4      NewsArticles-1100         4         leaks     False    False   \n",
       "...                  ...       ...           ...       ...      ...   \n",
       "17594   NewsArticles-960        90         Putin     False    False   \n",
       "17595   NewsArticles-960        91        Russia     False    False   \n",
       "17596   NewsArticles-960        92  capabilities     False    False   \n",
       "17597   NewsArticles-960        93    priorities     False    False   \n",
       "17598   NewsArticles-960        94        nation     False    False   \n",
       "\n",
       "            lemma  like_num    pos  tag  \n",
       "0       President     False  PROPN  NNP  \n",
       "1           Trump     False  PROPN  NNP  \n",
       "2         Justice     False  PROPN  NNP  \n",
       "3      Department     False  PROPN  NNP  \n",
       "4            leak     False   NOUN  NNS  \n",
       "...           ...       ...    ...  ...  \n",
       "17594       Putin     False  PROPN  NNP  \n",
       "17595      Russia     False  PROPN  NNP  \n",
       "17596  capability     False   NOUN  NNS  \n",
       "17597    priority     False   NOUN  NNS  \n",
       "17598      nation     False   NOUN   NN  \n",
       "\n",
       "[17599 rows x 9 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import filter_for_pos\n",
    "\n",
    "# \"N\" means filter for nouns\n",
    "corpus_filtered = filter_for_pos(corpus_orig, 'N', inplace=False)\n",
    "tokens_table(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we filtered for tokens that were identified as nouns by passing the *simplified POS tag* `'N'` (for more on simplified tags, see the function documentation). We can also filter for more than one tag, e.g. nouns or verbs by passing a list of required POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filter_for_pos` function has no `remove_...` counterpart, but you can set the `inverse` parameter to `True` to achieve the same effect.\n",
    "\n",
    "Finally there are functions for removing tokens based on their document frequency: [`filter_tokens_by_doc_frequency`](api.rst#TODO) along with the shortcut functions [`remove_common_tokens`](api.rst#tmtoolkit.corpus.remove_common_tokens) and [`remove_uncommon_tokens`](api.rst#tmtoolkit.corpus.remove_uncommon_tokens). The former removes all tokens that have a document frequency greater or equal a certain threshold defined by parameter `df_threshold`. The latter does the same for all tokens that have a document frequency lower or equal `df_threshold`. This parameter accepts a relative frequency (default) or absolute count (via parameter `proportions`).\n",
    "\n",
    "Before applying the function, let's have a look at the total number of tokens again, to later see how many we  removed. We will also store the vocabulary in `orig_vocab` for later comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59598"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import doc_lengths\n",
    "\n",
    "orig_vocab = vocabulary(corpus_orig)\n",
    "corpus_num_tokens(corpus_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44102"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import remove_common_tokens\n",
    "\n",
    "corpus_filtered = remove_common_tokens(corpus_orig, df_threshold=0.9, inplace=False)\n",
    "corpus_num_tokens(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing all tokens with a document frequency threshold of 0.9, we removed quite a number of tokens in each document. Let's investigate the vocabulary in order to see which tokens were removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n\\n',\n",
       " \"'s\",\n",
       " ',',\n",
       " '.',\n",
       " 'a',\n",
       " 'and',\n",
       " 'in',\n",
       " 'is',\n",
       " 'of',\n",
       " 'on',\n",
       " 'that',\n",
       " 'the',\n",
       " 'to'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set difference gives removed vocabulary tokens\n",
    "set(orig_vocab) - set(vocabulary(corpus_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this – as expected – removed very common token types.\n",
    "\n",
    "The `remove_uncommon_tokens` function works similarily. This time, let's use an absolute number as threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adding',\n",
       " 'fluid',\n",
       " 'Swinburne',\n",
       " 'responding',\n",
       " 'execution',\n",
       " 'nonetheless',\n",
       " 'Dorries',\n",
       " '\"?Wright',\n",
       " 'juror',\n",
       " 'distinctive',\n",
       " 'motor',\n",
       " 'eliminated',\n",
       " 'clone',\n",
       " 'Hardee',\n",
       " 'Tim',\n",
       " 'Roll',\n",
       " 'wound',\n",
       " 'rough',\n",
       " 'Lifetime',\n",
       " 'flexibility',\n",
       " ...}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import remove_uncommon_tokens\n",
    "\n",
    "corpus_filtered = remove_uncommon_tokens(corpus_orig, df_threshold=1,\n",
    "                                         proportions=0, inplace=False)\n",
    "\n",
    "# set difference gives removed vocabulary tokens\n",
    "set(orig_vocab) - set(vocabulary(corpus_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above means that we removed all tokens that appear only in exactly one document. As expected, these are rather uncommon token types.\n",
    "\n",
    "There are more filtering functions available. See the [corpus functions API](api.rst#TODO) and search for `filter_` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with document and token attributes\n",
    "\n",
    "Each document and each token in a corpus can have an arbitrary number of *attributes* attached to it. Think of these attributes as meta information or \"annotations\" at document or token level. An example for a document attribute is the document label, i.e. its name. An example for a token attribute is the POS tag.\n",
    "\n",
    "While the mentioned examples are attributes that tmtoolkit creates itself, you can also create your own attributes. For example, you may add a publication year as document attribute or a token attribute that indicates whether a token is in all caps. You can then use these attributes for example for filtering or in further analyses.\n",
    "\n",
    "#### Document attributes\n",
    "\n",
    "There are two functions for adding or updating document or token attributes, respectively: [`set_document_attr`](api.rst#TODO) and [`set_token_attr`](api.rst#TODO). We'll start with adding a new document attribute, `year`. At first we need to provide the attribute data as dict that maps document labels to document attribute values. For the purpose of this tutorial, we'll simply make up some data by drawing a random year for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NewsArticles-2433': 2020,\n",
       " 'NewsArticles-2225': 2020,\n",
       " 'NewsArticles-2487': 2016,\n",
       " 'NewsArticles-49': 2017,\n",
       " 'NewsArticles-469': 2016,\n",
       " 'NewsArticles-2766': 2017,\n",
       " 'NewsArticles-2712': 2019,\n",
       " 'NewsArticles-2301': 2019,\n",
       " 'NewsArticles-1377': 2020,\n",
       " 'NewsArticles-3428': 2016,\n",
       " 'NewsArticles-3208': 2015,\n",
       " 'NewsArticles-2156': 2018,\n",
       " 'NewsArticles-2143': 2019,\n",
       " 'NewsArticles-2730': 2019,\n",
       " 'NewsArticles-3159': 2015,\n",
       " 'NewsArticles-3201': 2018,\n",
       " 'NewsArticles-3353': 2016,\n",
       " 'NewsArticles-355': 2019,\n",
       " 'NewsArticles-422': 2019,\n",
       " 'NewsArticles-2867': 2019,\n",
       " ...}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_years = {lbl: random.randint(2015, 2020) for lbl in corpus_orig}\n",
    "doc_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `set_document_attr` to create the new document attribute and pass the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('label', 'has_sents', 'year')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import set_document_attr\n",
    "\n",
    "corpus_new = set_document_attr(corpus_orig, 'year', data=doc_years, inplace=False)\n",
    "# using the `doc_attrs` property to check that the new attribute is recorded:\n",
    "corpus_new.doc_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we investigate the token table, we can see a new column `year` which is constant for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>President</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>President</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>says</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>say</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>has</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>have</td>\n",
       "      <td>False</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59593</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>282</td>\n",
       "      <td>priorities</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>priority</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59594</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>283</td>\n",
       "      <td>for</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>for</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59595</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>284</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59596</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>285</td>\n",
       "      <td>nation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>nation</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59597</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>286</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59598 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position       token  is_punct  is_stop      lemma  \\\n",
       "0      NewsArticles-1100         0   President     False    False  President   \n",
       "1      NewsArticles-1100         1       Trump     False    False      Trump   \n",
       "2      NewsArticles-1100         2        says     False    False        say   \n",
       "3      NewsArticles-1100         3          he     False     True         he   \n",
       "4      NewsArticles-1100         4         has     False     True       have   \n",
       "...                  ...       ...         ...       ...      ...        ...   \n",
       "59593   NewsArticles-960       282  priorities     False    False   priority   \n",
       "59594   NewsArticles-960       283         for     False     True        for   \n",
       "59595   NewsArticles-960       284         the     False     True        the   \n",
       "59596   NewsArticles-960       285      nation     False    False     nation   \n",
       "59597   NewsArticles-960       286           .      True    False          .   \n",
       "\n",
       "       like_num    pos  tag  year  \n",
       "0         False  PROPN  NNP  2018  \n",
       "1         False  PROPN  NNP  2018  \n",
       "2         False   VERB  VBZ  2018  \n",
       "3         False   PRON  PRP  2018  \n",
       "4         False    AUX  VBZ  2018  \n",
       "...         ...    ...  ...   ...  \n",
       "59593     False   NOUN  NNS  2016  \n",
       "59594     False    ADP   IN  2016  \n",
       "59595     False    DET   DT  2016  \n",
       "59596     False   NOUN   NN  2016  \n",
       "59597     False  PUNCT    .  2016  \n",
       "\n",
       "[59598 rows x 10 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_table(corpus_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we set a document attribute value for each document in the corpus. However, you can also just set values for a subset of the documents. All other documents will then also contain that document attribute, but with a default value which is determined with the `default` parameter. Let's find out all documents that contain the token \"president\" (ignoring case). This is only a subset of all documents. We create an attribute dictionary that assigns `True` to all these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NewsArticles-2433': True,\n",
       " 'NewsArticles-2225': True,\n",
       " 'NewsArticles-2487': True,\n",
       " 'NewsArticles-49': True,\n",
       " 'NewsArticles-2766': True,\n",
       " 'NewsArticles-2301': True,\n",
       " 'NewsArticles-1377': True,\n",
       " 'NewsArticles-3208': True,\n",
       " 'NewsArticles-2143': True,\n",
       " 'NewsArticles-3201': True,\n",
       " 'NewsArticles-3353': True,\n",
       " 'NewsArticles-2502': True,\n",
       " 'NewsArticles-2431': True,\n",
       " 'NewsArticles-3309': True,\n",
       " 'NewsArticles-1185': True,\n",
       " 'NewsArticles-21': True,\n",
       " 'NewsArticles-2132': True,\n",
       " 'NewsArticles-1119': True,\n",
       " 'NewsArticles-549': True,\n",
       " 'NewsArticles-760': True,\n",
       " ...}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "president_docs = find_documents(corpus_new, 'president', ignore_case=True)\n",
    "president_attrs = {lbl: True for lbl in president_docs}\n",
    "president_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use this dictionary to create a new document attribute `president`. All documents not contained in `president_attrs` will get the default attribute value `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>year</th>\n",
       "      <th>president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2018</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>NewsArticles-1185</td>\n",
       "      <td>2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>NewsArticles-1263</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>NewsArticles-1353</td>\n",
       "      <td>2020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599</th>\n",
       "      <td>NewsArticles-770</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57987</th>\n",
       "      <td>NewsArticles-780</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58110</th>\n",
       "      <td>NewsArticles-836</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58397</th>\n",
       "      <td>NewsArticles-901</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59311</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  year  president\n",
       "0      NewsArticles-1100  2018       True\n",
       "224    NewsArticles-1119  2017       True\n",
       "1199   NewsArticles-1185  2017       True\n",
       "2470   NewsArticles-1263  2016      False\n",
       "2880   NewsArticles-1353  2020      False\n",
       "...                  ...   ...        ...\n",
       "57599   NewsArticles-770  2017      False\n",
       "57987   NewsArticles-780  2016      False\n",
       "58110   NewsArticles-836  2015      False\n",
       "58397   NewsArticles-901  2015      False\n",
       "59311   NewsArticles-960  2016       True\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_document_attr(corpus_new, 'president', data=president_attrs, default=False)\n",
    "toktbl = tokens_table(corpus_new, with_attr=['year', 'president'])\n",
    "\n",
    "# only show rows with document name, year and president indicator\n",
    "toktbl[['doc', 'year', 'president']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token attributes\n",
    "\n",
    "Similar to document attributes, we can use `set_token_attr` for creating or updating token attributes. However, this function has two modes of assigning attribute values to tokens. The default mode assigns each token occurrence, i.e. each token type, a certain attribute value. We will start with this mode.\n",
    "\n",
    "We set a new token attribute `obama` and simply assign a boolean value to each token which is `True` when the token is \"Obama\", else `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('is_punct', 'is_stop', 'like_num', 'tag', 'pos', 'lemma', 'obama')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import set_token_attr\n",
    "\n",
    "set_token_attr(corpus_new, 'obama', data={'Obama': True}, default=False)\n",
    "# check the token attributes property\n",
    "corpus_new.token_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it worked, but it isn't really useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>obama</th>\n",
       "      <th>pos</th>\n",
       "      <th>president</th>\n",
       "      <th>tag</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>137</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>NewsArticles-1119</td>\n",
       "      <td>848</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699</th>\n",
       "      <td>NewsArticles-1515</td>\n",
       "      <td>2</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4715</th>\n",
       "      <td>NewsArticles-1515</td>\n",
       "      <td>18</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>NewsArticles-1515</td>\n",
       "      <td>72</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4815</th>\n",
       "      <td>NewsArticles-1515</td>\n",
       "      <td>118</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>NewsArticles-1515</td>\n",
       "      <td>230</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>NewsArticles-1515</td>\n",
       "      <td>253</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>NewsArticles-1515</td>\n",
       "      <td>295</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>NewsArticles-1515</td>\n",
       "      <td>390</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc  position  token  is_punct  is_stop  lemma  like_num  \\\n",
       "137   NewsArticles-1100       137  Obama     False    False  Obama     False   \n",
       "1072  NewsArticles-1119       848  Obama     False    False  Obama     False   \n",
       "4699  NewsArticles-1515         2  Obama     False    False  Obama     False   \n",
       "4715  NewsArticles-1515        18  Obama     False    False  Obama     False   \n",
       "4769  NewsArticles-1515        72  Obama     False    False  Obama     False   \n",
       "4815  NewsArticles-1515       118  Obama     False    False  Obama     False   \n",
       "4927  NewsArticles-1515       230  Obama     False    False  Obama     False   \n",
       "4950  NewsArticles-1515       253  Obama     False    False  Obama     False   \n",
       "4992  NewsArticles-1515       295  Obama     False    False  Obama     False   \n",
       "5087  NewsArticles-1515       390  Obama     False    False  Obama     False   \n",
       "\n",
       "      obama    pos  president  tag  year  \n",
       "137    True  PROPN       True  NNP  2018  \n",
       "1072   True  PROPN       True  NNP  2017  \n",
       "4699   True  PROPN       True  NNP  2016  \n",
       "4715   True  PROPN       True  NNP  2016  \n",
       "4769   True  PROPN       True  NNP  2016  \n",
       "4815   True  PROPN       True  NNP  2016  \n",
       "4927   True  PROPN       True  NNP  2016  \n",
       "4950   True  PROPN       True  NNP  2016  \n",
       "4992   True  PROPN       True  NNP  2016  \n",
       "5087   True  PROPN       True  NNP  2016  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toktbl = tokens_table(corpus_new)\n",
    "toktbl[toktbl.obama].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second mode to assign token attribute values is much more useful. In this mode, you provide a dictionary that maps a document label to a list or array of token attribute values. The list's/array's size must match the number of tokens in the respective document. With this, you can assign an attribute value to each token in each document. We will use this to add a token attribute that records the number of characters in each token. At first, we generate that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 5, 4, 2, 3, 5, 3, 7, 10, 2, 11, 5, 2, 9, 5, 4, 5, 2, 3, 8, ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_toks = doc_tokens(corpus_new)\n",
    "\n",
    "tok_lengths = {lbl: list(map(len, tok)) for lbl, tok in doc_toks.items()}\n",
    "# show the number of characters for each token in a sample document\n",
    "tok_lengths['NewsArticles-1100']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass this data but set `per_token_occurrence=False` to indicate that the data contains attribute values per token in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>nchar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>President</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>says</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>he</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>has</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59593</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>282</td>\n",
       "      <td>priorities</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59594</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>283</td>\n",
       "      <td>for</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59595</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>284</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59596</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>285</td>\n",
       "      <td>nation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59597</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>286</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59598 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position       token  nchar\n",
       "0      NewsArticles-1100         0   President      9\n",
       "1      NewsArticles-1100         1       Trump      5\n",
       "2      NewsArticles-1100         2        says      4\n",
       "3      NewsArticles-1100         3          he      2\n",
       "4      NewsArticles-1100         4         has      3\n",
       "...                  ...       ...         ...    ...\n",
       "59593   NewsArticles-960       282  priorities     10\n",
       "59594   NewsArticles-960       283         for      3\n",
       "59595   NewsArticles-960       284         the      3\n",
       "59596   NewsArticles-960       285      nation      6\n",
       "59597   NewsArticles-960       286           .      1\n",
       "\n",
       "[59598 rows x 4 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_token_attr(corpus_new, 'nchar', data=tok_lengths, per_token_occurrence=False)\n",
    "tokens_table(corpus_new, with_attr='nchar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing attributes\n",
    "\n",
    "Document and token attributes can be removed with [`remove_document_attr`](api.rst#TODO) and [`remove_token_attr`](api.rst#TODO) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('label', 'has_sents', 'president')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import remove_document_attr\n",
    "\n",
    "remove_document_attr(corpus_new, 'year')\n",
    "corpus_new.doc_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('is_punct', 'is_stop', 'like_num', 'tag', 'pos', 'lemma', 'nchar')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import remove_token_attr\n",
    "\n",
    "remove_token_attr(corpus_new, 'obama')\n",
    "corpus_new.token_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell [`filter_tokens`](api.rst#tmtoolkit.corpus.filter_tokens) and similar functions to use document or token attributes instead of the tokens for matching. The common parameter name for this option is `by_meta`. For example, we can use the `nchar` attribute, which we created before, to filter for tokens of a certain length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>nchar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>has</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>has</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>39</td>\n",
       "      <td>its</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>40</td>\n",
       "      <td>has</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>41</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>42</td>\n",
       "      <td>for</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>43</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc  position token  nchar\n",
       "0     NewsArticles-1100         0   has      3\n",
       "1     NewsArticles-1100         1   the      3\n",
       "2     NewsArticles-1100         2   has      3\n",
       "3     NewsArticles-1100         3   the      3\n",
       "4     NewsArticles-1100         4   the      3\n",
       "...                 ...       ...   ...    ...\n",
       "9277   NewsArticles-960        39   its      3\n",
       "9278   NewsArticles-960        40   has      3\n",
       "9279   NewsArticles-960        41   the      3\n",
       "9280   NewsArticles-960        42   for      3\n",
       "9281   NewsArticles-960        43   the      3\n",
       "\n",
       "[9282 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_3chars = filter_tokens(corpus_new, 3, by_attr='nchar', inplace=False)\n",
    "tokens_table(corpus_3chars, with_attr='nchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus_3chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all matching options then apply to the token attribute column, in this case to the `nchar` column which contains integers. Since `filter_tokens` by default employs exact matching, we get all tokens where `nchar` equals the first argument, `3`. If we used regular expression or glob matching instead, this method would fail because you can only use that for string data.\n",
    "\n",
    "If you want to use more complex filter queries, you should create a \"filter mask\" and pass it to [`filter_tokens_by_mask`](api.rst#tmtoolkit.corpus.filter_tokens_by_mask). A filter mask is a dictionary that maps a document label to a sequence of booleans. For all occurrences of `True`, the respective token in the document will be retained, all others will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that out with a small example: We now generate the filter mask, which means for each document we create a boolean list or array that for each token in that document indicates whether that token should be kept or removed.\n",
    "\n",
    "We will iterate through the document tokens with attributes supplied from `doc_tokens`. We set `as_arrays=True` to obtain the `nchar` token attribute for each document as NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 1, 1, 7, 5, 8, 2, 5, 4, 2]),\n",
       " array(['NOUN', 'PUNCT', 'NUM', 'ADJ', 'NOUN', 'VERB', 'ADP', 'PROPN',\n",
       "        'NOUN', 'SPACE'], dtype='<U5'))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokattrs = doc_tokens(corpus_new, with_attr=['nchar', 'pos'], as_arrays=True)\n",
    "\n",
    "# show number of characters and POS tag of \n",
    "# first 10 tokens for a sample document\n",
    "(doc_tokattrs['NewsArticles-2433']['nchar'][:10],\n",
    " doc_tokattrs['NewsArticles-2433']['pos'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the filter mask. Since we generated the token attribute data as NumPy arrays before, we can directly and efficiently use NumPy functions such as `np.isin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>nchar</th>\n",
       "      <th>pos</th>\n",
       "      <th>small_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>President</td>\n",
       "      <td>9</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump</td>\n",
       "      <td>5</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>says</td>\n",
       "      <td>4</td>\n",
       "      <td>VERB</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>he</td>\n",
       "      <td>2</td>\n",
       "      <td>PRON</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>has</td>\n",
       "      <td>3</td>\n",
       "      <td>AUX</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59593</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>282</td>\n",
       "      <td>priorities</td>\n",
       "      <td>10</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59594</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>283</td>\n",
       "      <td>for</td>\n",
       "      <td>3</td>\n",
       "      <td>ADP</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59595</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>284</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "      <td>DET</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59596</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>285</td>\n",
       "      <td>nation</td>\n",
       "      <td>6</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59597</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>286</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59598 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc  position       token  nchar    pos  small_nouns\n",
       "0      NewsArticles-1100         0   President      9  PROPN        False\n",
       "1      NewsArticles-1100         1       Trump      5  PROPN         True\n",
       "2      NewsArticles-1100         2        says      4   VERB        False\n",
       "3      NewsArticles-1100         3          he      2   PRON        False\n",
       "4      NewsArticles-1100         4         has      3    AUX        False\n",
       "...                  ...       ...         ...    ...    ...          ...\n",
       "59593   NewsArticles-960       282  priorities     10   NOUN        False\n",
       "59594   NewsArticles-960       283         for      3    ADP        False\n",
       "59595   NewsArticles-960       284         the      3    DET        False\n",
       "59596   NewsArticles-960       285      nation      6   NOUN        False\n",
       "59597   NewsArticles-960       286           .      1  PUNCT        False\n",
       "\n",
       "[59598 rows x 6 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filter_mask = {}\n",
    "for doc_label, doc_data in doc_tokattrs.items():\n",
    "    tok_lengths = doc_data['nchar']\n",
    "    tok_pos = doc_data['pos']\n",
    "    # create a boolean array for nouns with token length less or equal 5\n",
    "    filter_mask[doc_label] = (tok_lengths <= 5) & np.isin(tok_pos, ['NOUN', 'PROPN'])\n",
    "\n",
    "# it's not necessary to add the filter mask as token attribute\n",
    "# but it's a good way to check the mask\n",
    "set_token_attr(corpus_new, 'small_nouns', data=filter_mask, per_token_occurrence=False)\n",
    "tokens_table(corpus_new, with_attr=['nchar', 'pos', 'small_nouns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can pass the mask dict to `filter_tokens_by_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>nchar</th>\n",
       "      <th>pos</th>\n",
       "      <th>small_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump</td>\n",
       "      <td>5</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>leaks</td>\n",
       "      <td>5</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>2</td>\n",
       "      <td>Trump</td>\n",
       "      <td>5</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>3</td>\n",
       "      <td>today</td>\n",
       "      <td>5</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1100</td>\n",
       "      <td>4</td>\n",
       "      <td>leaks</td>\n",
       "      <td>5</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>29</td>\n",
       "      <td>Trump</td>\n",
       "      <td>5</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>30</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>4</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>31</td>\n",
       "      <td>rest</td>\n",
       "      <td>4</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>32</td>\n",
       "      <td>world</td>\n",
       "      <td>5</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>NewsArticles-960</td>\n",
       "      <td>33</td>\n",
       "      <td>Putin</td>\n",
       "      <td>5</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc  position  token  nchar    pos  small_nouns\n",
       "0     NewsArticles-1100         0  Trump      5  PROPN         True\n",
       "1     NewsArticles-1100         1  leaks      5   NOUN         True\n",
       "2     NewsArticles-1100         2  Trump      5  PROPN         True\n",
       "3     NewsArticles-1100         3  today      5   NOUN         True\n",
       "4     NewsArticles-1100         4  leaks      5   NOUN         True\n",
       "...                 ...       ...    ...    ...    ...          ...\n",
       "6913   NewsArticles-960        29  Trump      5  PROPN         True\n",
       "6914   NewsArticles-960        30   U.S.      4  PROPN         True\n",
       "6915   NewsArticles-960        31   rest      4   NOUN         True\n",
       "6916   NewsArticles-960        32  world      5   NOUN         True\n",
       "6917   NewsArticles-960        33  Putin      5  PROPN         True\n",
       "\n",
       "[6918 rows x 6 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import filter_tokens_by_mask\n",
    "\n",
    "filter_tokens_by_mask(corpus_new, mask=filter_mask)\n",
    "tokens_table(corpus_new, with_attr=['nchar', 'pos', 'small_nouns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating n-grams\n",
    "\n",
    "So far, we worked with *unigrams*, i.e. each document consisted of a sequence of discrete tokens. We can also generate *n-grams* from our corpus where each document consists of a sequence of *n* subsequent tokens. An example would be:\n",
    "\n",
    "Document: \"This is a simple example.\"\n",
    "\n",
    "**n=1 (unigrams):**\n",
    "\n",
    "    ['This', 'is', 'a', 'simple', 'example', '.']\n",
    "\n",
    "**n=2 (bigrams):**\n",
    "\n",
    "    ['This is', 'is a', 'a simple', 'simple example', 'example .']\n",
    "\n",
    "**n=3 (trigrams):**\n",
    "\n",
    "    ['This is a', 'is a simple', 'a simple example', 'simple example .']\n",
    "\n",
    "The method [generate_ngrams()](api.rst#tmtoolkit.preprocess.TMPreproc.generate_ngrams) allows us to generate n-grams from tokenized documents. We can then get the results with the `ngrams` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc\n",
    "\n",
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.generate_ngrams(2)  # generate bigrams\n",
    "preproc.ngrams['NewsArticles-1880'][:10]  # show first 10 bigrams of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may afterwards use [join_ngrams()](api.rst#tmtoolkit.preprocess.TMPreproc.join_ngrams) to merge the generated n-grams to joint tokens and use these as new tokens in this TMPreproc instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.join_ngrams()\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a sparse document-term matrix (DTM)\n",
    "\n",
    "If you're working with a bag-of-words representation of your data, you usually convert the preprocessed documents to a document-term matrix (DTM), which represents of the number of occurrences of each term (i.e. vocabulary token) in each document. This is a *N* rows by *M* columns matrix, where *N* is the number of documents and *M* is the vocabulary size (i.e. the number of unique tokens in the corpus).\n",
    "\n",
    "Not all tokens from the vocabulary occur in all documents. In fact, many tokens will occur only in a small subset of the documents if you're dealing with a \"real world\" dataset. This means that most entries in such a DTM will be zero. Almost all functions in tmtoolkit therefore generate and work with *sparse* matrices, where only non-zero values are stored in computer memory.\n",
    "\n",
    "For this example, we'll generate a DTM from the `preproc_orig` instance. First, let's check the number of documents and the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.n_docs, preproc_orig.vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [dtm](api.rst#tmtoolkit.preprocess.TMPreproc.dtm) property to generate a sparse DTM from the current instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a sparse matrix with 3 rows (which corresponds with the number of documents) and 683 columns was generated (which corresponds to the vocabulary size). 816 elements in this matrix are non-zero.\n",
    "\n",
    "We can convert this matrix to a non-sparse, i.e. *dense*, representation and see parts of its elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.dtm.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that you should only convert a sparse matrix to a dense representation when you're either dealing with a small amount of data (which is what we're doing in this example), or use only a part of the full matrix. Converting a sparse matrix to a dense representation can otherwise easily exceed the available computer memory.\n",
    "\n",
    "There exist different \"formats\" for sparse matrices, which have different advantages and disadvantages (see for example the [SciPy \"sparse\" module documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html#usage-information)). **Not all formats support all operations that you can usually apply to an ordinary, dense matrix.** By default, the generated DTM is in *Compressed Sparse Row (CSR)* format. This format allows indexing and is especially optimized for fast row access. You may convert it to any other sparse matrix format; see the mentioned SciPy documentation for this.\n",
    "\n",
    "The rows of the DTM are aligned to the sequence of the document labels and its columns are aligned to the vocabulary. For example, let's find the frequency of the term \"House\" in the document \"NewsArticles-1880\". To do this, we find out the indices into the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.doc_labels.index('NewsArticles-1880')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.vocabulary.index('House')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the frequency of the term \"House\" in the document \"NewsArticles-1880\" is located in row 0 and column 4 of the DTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.dtm[0, 67]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the following example of finding out the index for \"administration\" and then getting an array that represents the number of occurrences of this token across all three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_admin_ix = preproc_orig.vocabulary.index('administration')\n",
    "preproc_orig.dtm[:, vocab_admin_ix].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the [dtm](api.rst#tmtoolkit.preprocess.TMPreproc.dtm) property, there's also the [get_dtm()](api.rst#tmtoolkit.preprocess.TMPreproc.get_dtm) method which allows to also return the result as datatable or pandas DataFrame. Note that these representations are not sparse and hence can consume much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.get_dtm(as_datatable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization: Saving and loading `TMPreproc` objects\n",
    "\n",
    "The current state of a `TMPreproc` object can also be stored to a file on disk so that you (or someone else who has tmtoolkit installed) can later restore it using that file. The methods for that are [save_state()](api.rst#tmtoolkit.preprocess.TMPreproc.save_state) and [load_state()](api.rst#tmtoolkit.preprocess.TMPreproc.load_state) / [from_state()](api.rst#tmtoolkit.preprocess.TMPreproc.from_state).\n",
    "\n",
    "Let's store the current state of the `preproc_orig` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.print_summary()\n",
    "preproc_orig.save_state('data/preproc_state.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the object by retaining only documents that contain the token \"house\" (see the reduced number of documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.filter_documents('*house*', match_type='glob', ignore_case=True)\n",
    "preproc_orig.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restore the saved data using [from_state()](api.rst#tmtoolkit.preprocess.TMPreproc.from_state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_restored = TMPreproc.from_state('data/preproc_state.pickle')\n",
    "preproc_restored.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the full dataset with three documents was restored.\n",
    "\n",
    "This is very useful especially when you have a large amount of data and run time consuming operations, e.g. POS tagging. When you're finished running these operations, you can easily store the current state to disk and later retrieve it without the need to re-run these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n",
    "The `TMPreproc` class provides a convenient object-oriented interface for parallel text processing and analysis. There is also a *functional API* provided in the [tmtoolkit.preprocess](api.rst#tmtoolkit-preprocess) module. Most of these functions accept a list of spaCy documents along with additional parameters. You may use these functions for quick prototyping, but it is generally much more convenient to use `TMPreproc`. The functional API does not provide parallel processing.\n",
    "\n",
    "To initialize the functional API for a certain language, you need to start with [init_for_language()](api.rst#tmtoolkit.preprocess.init_for_language) and may then tokenize your raw text documents via [tokenize()](api.rst#tmtoolkit.preprocess.tokenize), which will generate a list of spaCy documents. Most other functions in this API accept such a list of list of spaCy documents as input.\n",
    "\n",
    "```\n",
    "init_for_language('en')\n",
    "docs = tokenize(['Hello this is a test.', 'And here comes another one.'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental token string and token string sequence functions\n",
    "\n",
    "TODO: tokenseq module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result after applying preprocessing steps and hence transforming the text data is often a document-term matrix (DTM). The [bow module](api.rst#tmtoolkit-bow) contains several functions to work with DTMs, e.g. apply transformations such as *tf-idf* or compute some important summary statistics. The [next chapter](bow.ipynb) will introduce some of these functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
