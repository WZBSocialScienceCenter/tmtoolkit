{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "During text preprocessing, a corpus of documents is tokenized (i.e. the document strings are split into individual words, punctuation, numbers, etc.) and then these tokens can be transformed, filtered or annotated. The goal is to prepare the raw texts in a way that makes it easier to perform eventual analysis methods in a later stage, e.g. by reducing noise in the dataset. tmtoolkit provides a rich set of tools for this purpose in the [tmtoolkit.preprocess](api.rst#tmtoolkit-preprocess) module.   \n",
    "\n",
    "## Two approaches: functional API and `TMPreproc` class\n",
    "\n",
    "There are two ways to apply text preprocessing methods to your documents: First, there is the [functional API](api.rst#module-tmtoolkit.preprocess) which consists of a set of Python functions that accept a list of (tokenized) documents. An example might be:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Hello world!\",    # document 1\n",
    "    \"Another example\"  # document 2\n",
    "]\n",
    "\n",
    "docs = tokenize(corpus)\n",
    "to_lowercase(docs)\n",
    "# Out: [['hello', 'world', '!'],\n",
    "#       ['another', 'example']]\n",
    "```\n",
    "\n",
    "\n",
    "The advantage of this approach is that it's very straight-forward and flexible. However, you must manage any meta data associated with the documents on your own (e.g. document labels or token metadata). Furthermore, the processing is not done in parallel.\n",
    "\n",
    "Second, there is the [TMPreproc class](api.rst#tmpreproc-class-for-parallel-text-preprocessing) which addresses these limitations. You can create an instance of this class from your (labelled) documents and then apply preprocessing methods to it. This instance is a \"state-machine\", i.e. its contents (the documents) can change when you call a method. An example:\n",
    "\n",
    "```python\n",
    "corpus = {\n",
    "    \"doc1\": \"Hello world!\",\n",
    "    \"doc2\": \"Another example\"\n",
    "}\n",
    "\n",
    "preproc = TMPreproc(corpus)     # documents are directly tokenized\n",
    "preproc.tokens_to_lowercase()   # this changes the documents\n",
    "preproc.tokens                  # one of many ways to access the tokens\n",
    "\n",
    "# Out:\n",
    "# {\n",
    "#   'doc1': ['hello', 'world', '!'],\n",
    "#   'doc2': ['another', 'example']\n",
    "# }\n",
    "```\n",
    "\n",
    "The most important advantage is that `TMPreproc` employs parallel processing, i.e. it uses all available processors on your machine to do the computations necessary during preprocessing. For large text corpora, this can lead to a strong speed up. \n",
    "\n",
    "Both approaches offer mostly the same features in terms of available preprocessing methods. `TMPreproc` has some more methods to export the data to dataframes or datatables. In general, the functional API is mostly used for quick prototyping and when using a small amount of data. For projects with large amounts of data, it's recommended to use `TMPreproc`, especially because of the parallel computation support.\n",
    "\n",
    "This chapter starts with a few examples using the functional API and then turns to `TMPreproc`.\n",
    "\n",
    "## Functional API\n",
    "\n",
    "The functions in the preprocessing module make up the [functional API](api.rst#module-tmtoolkit.preprocess) for text preprocessing. We will explore some of the available functions. Most of them require at least passing a list of tokenized documents. In order to tokenize raw text documents (for example from a [Corpus](text_corpora.ipynb) object), we can use [tokenize()](api.rst#tmtoolkit.preprocess.tokenize). \n",
    "\n",
    "### Loading example data\n",
    "\n",
    "Let's load a sample of three documents from the built-in *NewsArticles* dataset. We'll save the document labels in `doc_labels` since the functional API works with lists of documents (not with dicts): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['NewsArticles-1880', 'NewsArticles-3350', 'NewsArticles-99'])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "import random\n",
    "random.seed(20191018)   # to make the sampling reproducible\n",
    "\n",
    "from tmtoolkit.corpus import Corpus\n",
    "from tmtoolkit.preprocess import tokenize\n",
    "\n",
    "corpus = Corpus.from_builtin_corpus('english-NewsArticles').sample(3)\n",
    "doc_labels = corpus.keys()\n",
    "doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "\n",
    "We can now tokenize these documents. We use `corpus.values()` to pass a list of documents. We get a list of tokenized documents back (i.e. a list of lists). We peak into the documents by only showing the first 10 words at maximum."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[['White',\n  'House',\n  'aides',\n  'told',\n  'to',\n  'keep',\n  'Russia-related',\n  'materials',\n  'Lawyers',\n  'for'],\n ['Frustration',\n  'as',\n  'cabin',\n  'electronics',\n  'ban',\n  'comes',\n  'into',\n  'force',\n  'Passengers',\n  'decry'],\n ['Should',\n  'you',\n  'have',\n  'two',\n  'bins',\n  'in',\n  'your',\n  'bathroom',\n  '?',\n  'Our']]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "docs = tokenize(corpus.values())\n",
    "[doc[:10] for doc in docs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Corpus language\n",
    "\n",
    "Some preprocessing steps are language-dependent, i.e. they're trained for different languages and hence you have to tell in which language your documents are written. At the moment, tmtoolkit only supports two languages off the shelf: English and German. \n",
    "\n",
    "In the functional API, all functions that are language-dependent have a `language` argument. Examples of such functions are [tokenize()](api.rst#tmtoolkit.preprocess.tokenize), [pos_tag()](api.rst#tmtoolkit.preprocess.pos_tag), [stem()](api.rst#tmtoolkit.preprocess.stem) and [lemmatize()](api.rst#tmtoolkit.preprocess.lemmatize). The default language for the `language` parameter of the preprocessing functions is set in [tmtoolkit.defaults.language](api.rst#tmtoolkit.defaults.language). If you don't change it, it's set to `\"english\"`. So you have two options when you use the functional API and work with a corpus that is not in English: you either or pass the `language` parameter each time you use a language-dependent function; or you set `tmtoolkit.defaults.language` right at the beginning which will be used as default for all further language-dependent preprocessing functions. Let's try both options with a German sample corpus:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from tmtoolkit.preprocess import stem\n",
    "\n",
    "docs_de = [\n",
    "    'Von der Wiege bis zur Bahre, Formulare, Formulare.',\n",
    "    'Fischers Fritz fischt frische Fische.',\n",
    "    'Viel schon ist getan, mehr noch ist zu tun, sagt der Wasserhahn zum Wasserhuhn.'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Option 1, passing the `language` parameter each time:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[['von',\n  'der',\n  'wieg',\n  'bis',\n  'zur',\n  'bahr',\n  ',',\n  'formular',\n  ',',\n  'formular',\n  '.'],\n ['fisch', 'fritz', 'fischt', 'frisch', 'fisch', '.'],\n ['viel',\n  'schon',\n  'ist',\n  'getan',\n  ',',\n  'mehr',\n  'noch',\n  'ist',\n  'zu',\n  'tun',\n  ',',\n  'sagt',\n  'der',\n  'wasserhahn',\n  'zum',\n  'wasserhuhn',\n  '.']]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "tokens_de = tokenize(docs_de, language='german')\n",
    "stemmed_de = stem(tokens_de, language='german')\n",
    "stemmed_de"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Option 2, setting `tmtoolkit.defaults.language` provides the same output:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "import tmtoolkit.defaults\n",
    "tmtoolkit.defaults.language = 'german'\n",
    "\n",
    "tokens_de = tokenize(docs_de)\n",
    "stemmed_de == stem(tokens_de) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will return to the English corpus hence we can reset the default language and clean up:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tmtoolkit.defaults.language = 'english'\n",
    "\n",
    "del docs_de, tokens_de, stemmed_de "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A small tour around the functional preprocessing API\n",
    "\n",
    "We will continue with the most important functions in the preprocessing API and apply them to our English sample corpus.\n",
    "\n",
    "#### Document length\n",
    "\n",
    "The document length is the number of tokens per document and can be obtained with [doc_lengths()](api.rst#tmtoolkit.preprocess.doc_lengths):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[227, 646, 1052]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import doc_lengths\n",
    "\n",
    "doc_lengths(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vocabulary and document frequencies\n",
    "\n",
    "The vocabulary is the set of unique tokens in the corpus, i.e. all tokens that occur at least once in at least one of the documents. You can use [vocabulary()](api.rst#tmtoolkit.preprocess.vocabulary) for that and [vocabulary_counts()](api.rst#tmtoolkit.preprocess.vocabulary_counts) to additionally get the number of times each token appears in the corpus. \n",
    "\n",
    "The document frequency of a token is the number of documents in which this token occurs at least once. The function [doc_frequencies()](api.rst#tmtoolkit.preprocess.doc_frequencies) returns this measure for all tokens in the vocabulary. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['%', \"'\", \"''\", \"'s\", '(', ')', ',', '-', '-Al', '.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import vocabulary, vocabulary_counts, doc_frequencies\n",
    "\n",
    "# first 10 entries from the sorted vocab\n",
    "vocabulary(docs, sort=True)[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 82),\n (',', 70),\n ('.', 60),\n ('to', 53),\n ('and', 45),\n ('in', 38),\n ('a', 31),\n ('``', 28),\n ('of', 25),\n (\"''\", 23)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "# get unsorted vocabulary counts as Counter object\n",
    "vocab_counts = vocabulary_counts(docs)\n",
    "# get top 10 tokens by occurrence\n",
    "vocab_counts.most_common(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "doc_freq = doc_frequencies(docs)\n",
    "\n",
    "# \"the\" occurs in all three documents, \"Lawyers\" only in one\n",
    "doc_freq['the'], doc_freq['Lawyers']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Part-of-speech (POS) tagging\n",
    "\n",
    "Part-of-speech (POS) tagging finds the grammatical word-category for each token in a document. The function [pos_tag()](api.rst#tmtoolkit.preprocess.pos_tag) employs this for the whole corpus. It returns a list of tags for each document. These tags conform to a specific *tagset*. For English this is the [Penn Treebank tagset](https://ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and for German this is the [STTS tagset](http://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/TagSets/stts-table.html).\n",
    "\n",
    "These tags can be used to filter, annotate or lemmatize the documents.\n",
    "\n",
    "Remember that this is a language-dependent function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[('White', 'NNP'),\n ('House', 'NNP'),\n ('aides', 'NNS'),\n ('told', 'VBD'),\n ('to', 'TO'),\n ('keep', 'VB'),\n ('Russia-related', 'JJ'),\n ('materials', 'NNS'),\n ('Lawyers', 'NNS'),\n ('for', 'IN')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 11
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import pos_tag\n",
    "\n",
    "docs_pos = pos_tag(docs)\n",
    "\n",
    "# show pairs of tokens and POS tags for the first 10 tokens in the first document\n",
    "list(zip(docs[0][:10], docs_pos[0][:10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Stemming and lemmatization\n",
    "\n",
    "Stemming and lemmatization bring a token, if it is a word, to a base form. The former method is rule-based and creates base forms by cutting of common pre- and suffixes. The resulting token may not be a lexicographically correct word any more. We've already used [stem()](api.rst#tmtoolkit.preprocess.stem) in an example above.\n",
    "\n",
    "Lemmatization is a more sophisticated process that tries to find lexicographically correct base form of a given word by also considering its POS tag and possibly its context (tokens and POS tags nearby). It's usually not rule-based but a trained model that predicts the base form from the mentioned parameters. Lemmatization can be applied with [lemmatize()](api.rst#tmtoolkit.preprocess.lemmatize).\n",
    "\n",
    "Remember that both functions are language-dependent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[('White', 'White'),\n ('House', 'House'),\n ('aides', 'aide'),\n ('told', 'tell'),\n ('to', 'to'),\n ('keep', 'keep'),\n ('Russia-related', 'Russia-related'),\n ('materials', 'material'),\n ('Lawyers', 'Lawyers'),\n ('for', 'for')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import lemmatize\n",
    "\n",
    "docs_lem = lemmatize(docs, docs_pos)\n",
    "# show pairs of original tokens and lemmata for the first 10 tokens of first document\n",
    "list(zip(docs[0][:10], docs_lem[0][:10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### \"Cleaning\" tokens\n",
    "\n",
    "Depending on your methodology, it may be necessary to \"clean\" or \"normalize\" your tokens in different ways in order to remove noise from the corpus, such as punctuation tokens or numbers, upper/lowercase forms of the same word, etc. Note that this is usually not necessary when you work with more modern approaches such as word embeddings (word vectors).   \n",
    "\n",
    "If you want to remove certain characters in *all* tokens in your corpus, you can use [remove_chars()](api.rst#tmtoolkit.preprocess.remove_chars) and pass it a sequence of characters to remove.\n",
    "\n",
    "Note that for the following examples I continue working with the lemmatized documents `docs_lem`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['Wht', 'Hs', 'd', 'tll', 't', 'kp', 'Rss-rltd', 'mtrl', 'Lwyrs', 'fr']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import remove_chars\n",
    "\n",
    "# remove all vowels from the documents, show first 10 tokens from first document\n",
    "remove_chars(docs_lem, 'aeiou')[0][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can for example use this to remove all punctuation characters from all tokens:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Should', 'Should'),\n ('you', 'you'),\n ('have', 'have'),\n ('two', 'two'),\n ('bin', 'bin'),\n ('in', 'in'),\n ('your', 'your'),\n ('bathroom', 'bathroom'),\n ('?', ''),\n ('Our', 'Our')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 14
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "docs_clean = remove_chars(docs_lem, string.punctuation)\n",
    "# show pairs of original tokens and cleaned tokens for the first 10 tokens of 2nd doc.\n",
    "list(zip(docs_lem[2][:10], docs_clean[2][:10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice how the token `'?'` was transformed to an empty string `''`, because \"?\" is a punctuation character.\n",
    "\n",
    "A common (but harsh) practice is to transform all tokens to lowercase forms, which can be done with :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['should', 'you', 'have', 'two', 'bin', 'in', 'your', 'bathroom', '', 'our']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 15
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import to_lowercase\n",
    "\n",
    "docs_clean = to_lowercase(docs_clean)\n",
    "docs_clean[2][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function [clean_tokens()](api.rst#tmtoolkit.preprocess.clean_tokens) finally applies several steps that remove tokens that meet certain criteria. This includes removing:\n",
    "\n",
    "- punctuation tokens\n",
    "- stopwords (very common words for the given language)\n",
    "- empty tokens (i.e. `''`)\n",
    "- tokens that are longer or shorter than a certain number of characters\n",
    "- numbers  \n",
    "\n",
    "Note that this is a language-dependent function, because the default stopword list is determined per language. This function has lot's of parameters to tweak, so it's recommended to check out the documentation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "['two',\n 'bin',\n 'bathroom',\n 'bathroom',\n 'fill',\n 'shampoo',\n 'bottle',\n 'toilet',\n 'roll',\n 'cleaning']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import clean_tokens\n",
    "\n",
    "# remove punct., stopwords, empty tokens (this is the default)\n",
    "# plus tokens shorter than 2 characters and numeric tokens like \"2019\"\n",
    "docs_final = clean_tokens(docs_clean, remove_shorter_than=2, remove_numbers=True)\n",
    "\n",
    "# first 10 tokens of doc. #2\n",
    "docs_final[2][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to the removal of several tokens in the previous step, the document lengths for the processed corpus are much smaller than for the original corpus:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "([227, 646, 1052], [129, 310, 504])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 17
    }
   ],
   "source": [
    "doc_lengths(docs), doc_lengths(docs_final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also observe that the vocabulary got smaller after the processing steps, which, for large corpora, is also important in terms of computation time and memory consumption for later analyses:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(681, 478)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 18
    }
   ],
   "source": [
    "len(vocabulary(docs)), len(vocabulary(docs_final))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also apply custom token transform functions by using [transform()](api.rst#tmtoolkit.preprocess.transform) and passing it a function that should be applied to each token in each document (hence it must accept one string argument).\n",
    "\n",
    "First let's define such a function. Here we create a simple function that should return a token's \"shape\" in terms of the case of its characters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "('XXX', 'XxxxxXxxx', 'xxxxx')"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "def token_shape(t):\n",
    "    return ''.join(['X' if str.isupper(c) else 'x' for c in t])\n",
    "\n",
    "token_shape('USA'), token_shape('CamelCase'), token_shape('lower')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now apply this function to our corpus:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "[('White', 'Xxxxx'),\n ('House', 'Xxxxx'),\n ('aides', 'xxxxx'),\n ('told', 'xxxx'),\n ('to', 'xx'),\n ('keep', 'xxxx'),\n ('Russia-related', 'Xxxxxxxxxxxxxx'),\n ('materials', 'xxxxxxxxx'),\n ('Lawyers', 'Xxxxxxx'),\n ('for', 'xxx')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 20
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import transform\n",
    "\n",
    "doc_shapes = transform(docs, token_shape)\n",
    "\n",
    "# show pairs of tokens and POS tags for the first 10 tokens in the first document\n",
    "list(zip(docs[0][:10], doc_shapes[0][:10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Keywords-in-context (KWIC)\n",
    "\n",
    "*Keywords-in-context (KWIC)* allow you to quickly to investigate certain keywords and their neighborhood of tokens, i.e. the tokens that appear right before and after this keyword.\n",
    "\n",
    "tmtoolkit provides two functions for this purpose:\n",
    "\n",
    "- [kwic()](api.rst#tmtoolkit.preprocess.kwic) is the base function accepting the input documents, a search pattern and several options that control how the search pattern is matched (more on that below); use this function when you want to further process the output of a KWIC search;\n",
    "- [kwic_table()](api.rst#tmtoolkit.preprocess.kwic_table) is the more \"user friendly\" version of the above function as it produces a datatable with the highlighted keyword by default\n",
    "\n",
    "Let's see both functions in action:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[[],\n [['told', 'Reuters', 'news', 'agency', '.'],\n  ['Jazeera', 'and', 'news', 'agencies']],\n []]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 21
    }
   ],
   "source": [
    "from tmtoolkit.preprocess import kwic, kwic_table\n",
    "\n",
    "kwic(docs, 'news')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the first and last document do not contain any keyword that matches `\"news\"`, hence we get empty results for these documents. In the second document, we get two result contexts for the requested keyword. This keyword stands in the middle and is surrounded by its \"context tokens\", which by default means two tokens to the left and two tokens to the right. Notice that in the second result context only one token to the right is shown since the document ends after \"agencies\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "",
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool { background: #DDDD99; }\n.datatable .obj  { background: #565656; }\n.datatable .int  { background: #5D9E5D; }\n.datatable .real { background: #4040CC; }\n.datatable .str  { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n<div class='datatable'>\n  <table class='frame'>\n  <thead>\n    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n    <tr class='coltypes'><td class='row_index'></td><td class='int' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='int' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n  </thead>\n  <tbody>\n    <tr><td class='row_index'>0</td><td>1</td><td>0</td><td>told Reuters *news* agency .</td></tr>\n    <tr><td class='row_index'>1</td><td>1</td><td>1</td><td>Jazeera and *news* agencies</td></tr>\n  </tbody>\n  </table>\n  <div class='footer'>\n    <div class='frame_dimensions'>2 rows &times; 3 columns</div>\n  </div>\n</div>\n"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 22
    }
   ],
   "source": [
    "kwic_table(docs, 'news')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With `kwic_table()`, we get back a datatable which provides a better formatting for quick investigation. See how the matched tokens are highlighted as `*news*` and empty results are removed (only document \"1\" contains the keyword which is the *second* document â€“ remember that Python indexing starts with 0).\n",
    "\n",
    "We can also pass the document labels via `doc_labels` to get proper labels in the `doc` column instead of document indices:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "",
      "text/html": "<div class='datatable'>\n  <table class='frame'>\n  <thead>\n    <tr class='colnames'><td class='row_index'></td><th>doc</th><th>context</th><th>kwic</th></tr>\n    <tr class='coltypes'><td class='row_index'></td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='int' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n  </thead>\n  <tbody>\n    <tr><td class='row_index'>0</td><td>NewsArticles-3350</td><td>0</td><td>told Reuters *news* agency .</td></tr>\n    <tr><td class='row_index'>1</td><td>NewsArticles-3350</td><td>1</td><td>Jazeera and *news* agencies</td></tr>\n  </tbody>\n  </table>\n  <div class='footer'>\n    <div class='frame_dimensions'>2 rows &times; 3 columns</div>\n  </div>\n</div>\n"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 23
    }
   ],
   "source": [
    "kwic_table(docs, 'news', doc_labels=doc_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Filtering tokens\n",
    "\n",
    "token_match\n",
    "filter_tokens, remove_tokens, filter_documents, remove_documents,\n",
    "filter_documents_by_name, remove_documents_by_name, filter_for_pos\n",
    "remove_common_tokens, remove_uncommon_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Expanding contractions and \"gluing\" tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generating n-grams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generating a sparse document-term matrix (DTM)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}