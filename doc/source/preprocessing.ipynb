{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing and basic text mining\n",
    "\n",
    "During text preprocessing, a corpus of documents is tokenized (i.e. the document strings are split into individual words, punctuation, numbers, etc.) and then these tokens can be transformed, filtered or annotated. The goal is to prepare the raw texts in a way that makes it easier to perform eventual analysis methods in a later stage, e.g. by reducing noise in the dataset. tmtoolkit provides a rich set of tools for this purpose implemented as *corpus functions* in the [tmtoolkit.corpus](api.rst#tmtoolkit-corpus) module.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Reminder: Corpus functions\n",
    "\n",
    "All *corpus functions* accept a [`Corpus`](api.rst#TODO) object as first argument and operate on it. A corpus function may retrieve information from a corpus and/or modify the corpus object.\n",
    "\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: enabling logging output\n",
    "\n",
    "By default, tmtoolkit does not expose any internal logging messages. Sometimes, for example for diagnostic output during debugging or in order to see progress for long running operations, it's helpful to enable logging output display. For that, you can use the [`enable_logging`](api.rst#TODO) function. By default, it enables logging to console for the `INFO` level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.utils import enable_logging\n",
    "\n",
    "enable_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading example data\n",
    "\n",
    "Let's load a sample of ten documents from the built-in *NewsArticles* dataset. We'll use only a small number of documents here to have a better overview at the beginning. We can later use a larger sample. To apply sampling right at the beginning when loading the data, we pass the `sample=10` parameter to the [`from_builtin_corpus`](api.rst#TODO) class method. We also use [`print_summary`](api.rst#TODO) like shown in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 15:03:27,051:INFO:tmtoolkit:creating Corpus instance with no documents\n",
      "2022-01-19 15:03:27,052:INFO:tmtoolkit:using serial processing\n",
      "2022-01-19 15:03:27,557:INFO:tmtoolkit:sampling 10 documents(s) out of 3824\n",
      "2022-01-19 15:03:27,560:INFO:tmtoolkit:adding text from 10 documents(s)\n",
      "2022-01-19 15:03:28,451:INFO:tmtoolkit:generating document texts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 10 documents in English\n",
      "> NewsArticles-2433 (842 tokens): DOJ : 2 Russian spies indicted in Yahoo hack    Wa...\n",
      "> NewsArticles-2225 (539 tokens): Rutte and Wilders face - off in Dutch general elec...\n",
      "> NewsArticles-2487 (1015 tokens): Dutch election : High turnout in key national vote...\n",
      "> NewsArticles-49 (1112 tokens): Trump vs. America : The fight for democracy    Fri...\n",
      "> NewsArticles-469 (398 tokens): Warning of tight times ahead for insurers    Analy...\n",
      "> NewsArticles-2766 (700 tokens): Depeche Mode releases ' Spirit , ' an unusually po...\n",
      "> NewsArticles-2712 (571 tokens): Grieving families speak out as police hunt for kil...\n",
      "> NewsArticles-2301 (464 tokens): DOJ seeks more time on Trump wiretapping inquiry  ...\n",
      "> NewsArticles-1377 (774 tokens): Turkey - backed rebels in ' near full control ' of...\n",
      "> NewsArticles-3428 (776 tokens): In Breakthrough Discovery , Scientists Mass - Prod...\n",
      "total number of tokens: 7191 / vocabulary size: 2149\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(20220119)   # to make the sampling reproducible\n",
    "\n",
    "from tmtoolkit.corpus import Corpus, print_summary\n",
    "\n",
    "corpus_small = Corpus.from_builtin_corpus('en-NewsArticles', sample=10)\n",
    "print_summary(corpus_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing tokens and token attributes\n",
    "\n",
    "We start with accessing the documents' tokens and their *token attributes* using [`doc_tokens`](api.rst#TODO) and [`tokens_table`](api.rst#TODO). Token attributes are meta information attached to each token. These can be linguistic features, such as the Part of Speech (POS) tag, indicators for stopwords or punctuation, etc. The default attributes are a subset of [SpaCy's token attributes](https://spacy.io/api/token#attributes). You can configure which of these attributes are stored using the `spacy_token_attrs` parameter of the [`Corpus`](api.rst#TODO) constructor. You can also add your own token attributes. This will be shown later on.\n",
    "\n",
    "At first we load the tokens along with their attributes via `doc_tokens`, which gives us a dictionary mapping document labels to document data. Each document data is another dictionary that contains the tokens and their attributes. We start by checking which token attributes are loaded by default in any document (here, we use `'NewsArticles-2433'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token', 'is_punct', 'is_stop', 'like_num', 'tag', 'pos', 'lemma'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import doc_tokens, tokens_table\n",
    "\n",
    "# with_attr=True adds default set of token attributes\n",
    "tok = doc_tokens(corpus_small, with_attr=True)\n",
    "tok['NewsArticles-2433'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each document's data can be accessed like in the example above and it will contain the seven data entries listed above. The `'token'` entry gives the actual tokens of the document. Let's show the first five tokens for a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOJ', ':', '2', 'Russian', 'spies']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok['NewsArticles-2433']['token'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other entries are the attributes corresponding to each token. Here, we display the first five lemmata for the same document and the first five punctuation indicator values. The colon is correctly identified as punctuation character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doj', ':', '2', 'russian', 'spy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok['NewsArticles-2433']['lemma'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False, False, False]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok['NewsArticles-2433']['is_punct'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your NLP pipeline performs sentence recognition, you can pass the parameter `sentences=True` which will add another level to the output representing sentences. This means that for each item like `'token'`, `'lemma'`, etc. we will get a list of sentences. For example, the following will print the tokens of the eighth sentences (index 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Justice',\n",
       " 'Department',\n",
       " 'official',\n",
       " 'said',\n",
       " 'the',\n",
       " 'agency',\n",
       " 'has',\n",
       " 'not',\n",
       " 'confirmed',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'same',\n",
       " 'person',\n",
       " 'and',\n",
       " 'declined',\n",
       " 'further',\n",
       " 'comment',\n",
       " 'to',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sents = doc_tokens(corpus_small, sentences=True, with_attr=True)\n",
    "tok_sents['NewsArticles-2433']['token'][7]   # index 7 means 8th sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more compact overview, it's better to use the [`tokens_table`](api.rst#TODO) function. This will generate a [pandas DataFrame](https://pandas.pydata.org/) from the documents in the corpus and it will be default include all token attributes, along with a column for the document label (`doc`) and the token position inside the document (`position`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>0</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>False</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-</td>\n",
       "      <td>False</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>HYPH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>2</td>\n",
       "      <td>backed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>back</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>3</td>\n",
       "      <td>rebels</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>rebel</td>\n",
       "      <td>False</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>4</td>\n",
       "      <td>in</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>in</td>\n",
       "      <td>False</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1107</td>\n",
       "      <td>fight</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>fight</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7187</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1108</td>\n",
       "      <td>to</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>to</td>\n",
       "      <td>False</td>\n",
       "      <td>PART</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7188</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1109</td>\n",
       "      <td>defend</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>defend</td>\n",
       "      <td>False</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7189</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1110</td>\n",
       "      <td>it</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>it</td>\n",
       "      <td>False</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1111</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7191 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc  position   token  is_punct  is_stop   lemma  \\\n",
       "0     NewsArticles-1377         0  Turkey     False    False  Turkey   \n",
       "1     NewsArticles-1377         1       -      True    False       -   \n",
       "2     NewsArticles-1377         2  backed     False    False    back   \n",
       "3     NewsArticles-1377         3  rebels     False    False   rebel   \n",
       "4     NewsArticles-1377         4      in     False     True      in   \n",
       "...                 ...       ...     ...       ...      ...     ...   \n",
       "7186    NewsArticles-49      1107   fight     False    False   fight   \n",
       "7187    NewsArticles-49      1108      to     False     True      to   \n",
       "7188    NewsArticles-49      1109  defend     False    False  defend   \n",
       "7189    NewsArticles-49      1110      it     False     True      it   \n",
       "7190    NewsArticles-49      1111       .      True    False       .   \n",
       "\n",
       "      like_num    pos   tag  \n",
       "0        False  PROPN   NNP  \n",
       "1        False  PUNCT  HYPH  \n",
       "2        False   VERB   VBN  \n",
       "3        False   NOUN   NNS  \n",
       "4        False    ADP    IN  \n",
       "...        ...    ...   ...  \n",
       "7186     False   VERB    VB  \n",
       "7187     False   PART    TO  \n",
       "7188     False   VERB    VB  \n",
       "7189     False   PRON   PRP  \n",
       "7190     False  PUNCT     .  \n",
       "\n",
       "[7191 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = tokens_table(corpus_small)\n",
    "tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use all sorts of filtering operations on this dataframe. See the [pandas documentation](https://pandas.pydata.org/docs/user_guide/indexing.html) for details. Here, we select all tokens that were identified as \"number-like\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>like_num</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>88</td>\n",
       "      <td>three</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>three</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>115</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>292</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>NewsArticles-1377</td>\n",
       "      <td>522</td>\n",
       "      <td>first</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>first</td>\n",
       "      <td>True</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>565</td>\n",
       "      <td>one</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>one</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6795</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>716</td>\n",
       "      <td>2011</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2011</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>747</td>\n",
       "      <td>1999</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1999</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>932</td>\n",
       "      <td>seven</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>seven</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7135</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1056</td>\n",
       "      <td>two</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>two</td>\n",
       "      <td>True</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc  position  token  is_punct  is_stop  lemma  like_num  \\\n",
       "73    NewsArticles-1377        73     25     False    False     25      True   \n",
       "88    NewsArticles-1377        88  three     False     True  three      True   \n",
       "115   NewsArticles-1377       115   2014     False    False   2014      True   \n",
       "292   NewsArticles-1377       292     13     False    False     13      True   \n",
       "522   NewsArticles-1377       522  first     False     True  first      True   \n",
       "...                 ...       ...    ...       ...      ...    ...       ...   \n",
       "6644    NewsArticles-49       565    one     False     True    one      True   \n",
       "6795    NewsArticles-49       716   2011     False    False   2011      True   \n",
       "6826    NewsArticles-49       747   1999     False    False   1999      True   \n",
       "7011    NewsArticles-49       932  seven     False    False  seven      True   \n",
       "7135    NewsArticles-49      1056    two     False     True    two      True   \n",
       "\n",
       "      pos tag  \n",
       "73    NUM  CD  \n",
       "88    NUM  CD  \n",
       "115   NUM  CD  \n",
       "292   NUM  CD  \n",
       "522   ADV  RB  \n",
       "...   ...  ..  \n",
       "6644  NUM  CD  \n",
       "6795  NUM  CD  \n",
       "6826  NUM  CD  \n",
       "7011  NUM  CD  \n",
       "7135  NUM  CD  \n",
       "\n",
       "[132 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl[tbl.like_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This however only filters the table output. We will later see how to filter corpus documents and tokens.\n",
    "\n",
    "If you want to generate the table only for a subset of documents, you can use the `select` parameter and provide one or more document labels. Similar to that, you can use the `with_attr` parameter to list only a subset of the token attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sent</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Russian</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>spies</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>837</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>838</td>\n",
       "      <td>reflect</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>839</td>\n",
       "      <td>new</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>840</td>\n",
       "      <td>developments</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>27</td>\n",
       "      <td>841</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>842 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   doc  sent  position         token    pos\n",
       "0    NewsArticles-2433     0         0           DOJ   NOUN\n",
       "1    NewsArticles-2433     0         1             :  PUNCT\n",
       "2    NewsArticles-2433     0         2             2    NUM\n",
       "3    NewsArticles-2433     0         3       Russian    ADJ\n",
       "4    NewsArticles-2433     0         4         spies   NOUN\n",
       "..                 ...   ...       ...           ...    ...\n",
       "837  NewsArticles-2433    27       837            to   PART\n",
       "838  NewsArticles-2433    27       838       reflect   VERB\n",
       "839  NewsArticles-2433    27       839           new    ADJ\n",
       "840  NewsArticles-2433    27       840  developments   NOUN\n",
       "841  NewsArticles-2433    27       841             .  PUNCT\n",
       "\n",
       "[842 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a single document and only show the \"pos\" attribute (coarse POS tag)\n",
    "tokens_table(corpus_small, select='NewsArticles-2433', sentences=True, with_attr='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>position</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>0</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>3</td>\n",
       "      <td>Russian</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsArticles-2433</td>\n",
       "      <td>4</td>\n",
       "      <td>spies</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1107</td>\n",
       "      <td>fight</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1108</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1109</td>\n",
       "      <td>defend</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1110</td>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>NewsArticles-49</td>\n",
       "      <td>1111</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1954 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc  position    token    pos  tag\n",
       "0     NewsArticles-2433         0      DOJ   NOUN   NN\n",
       "1     NewsArticles-2433         1        :  PUNCT    :\n",
       "2     NewsArticles-2433         2        2    NUM   CD\n",
       "3     NewsArticles-2433         3  Russian    ADJ   JJ\n",
       "4     NewsArticles-2433         4    spies   NOUN  NNS\n",
       "...                 ...       ...      ...    ...  ...\n",
       "1949    NewsArticles-49      1107    fight   VERB   VB\n",
       "1950    NewsArticles-49      1108       to   PART   TO\n",
       "1951    NewsArticles-49      1109   defend   VERB   VB\n",
       "1952    NewsArticles-49      1110       it   PRON  PRP\n",
       "1953    NewsArticles-49      1111        .  PUNCT    .\n",
       "\n",
       "[1954 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select two documents and only show the \"pos\" and \"tag\" attributes (coarse and detailed POS tags)\n",
    "tokens_table(corpus_small, select=['NewsArticles-2433', 'NewsArticles-49'], with_attr=['pos', 'tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Side note: Common corpus function parameters\n",
    "    \n",
    "Many corpus functions share the same parameter names and when they do, they implicate the same behavior. As already explained, all corpus functions accept a `Corpus` object as first parameter. But next to that, many corpus functions also accept a `select` parameter, which can always be used to specify a subset of the documents to which the respective function is applied. We also already got to know the `sentences` parameter that some corpus functions accept in order to also represent the sentence structure of a document in their output.\n",
    "    \n",
    "To know which functions accept which parameter, check their documentation.\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus vocabulary\n",
    "\n",
    "The corpus *vocabulary* is the set of unique tokens in a corpus. We can get that set via [`vocabulary`](api.rst#TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'safe',\n",
       " 'France',\n",
       " 'You',\n",
       " 'movement',\n",
       " 'spokesperson',\n",
       " 'between',\n",
       " 'as',\n",
       " 'Attorney',\n",
       " '200',\n",
       " 'four',\n",
       " 'me',\n",
       " 'outstanding',\n",
       " 'Thursday',\n",
       " 'MORE',\n",
       " 'legal',\n",
       " 'requires',\n",
       " 'selling',\n",
       " 'growing',\n",
       " 'Election',\n",
       " 'restaurant',\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import vocabulary\n",
    "\n",
    "vocabulary(corpus_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus function also accepts a `select` parameter. We can also sort the vocabulary via `sort=True`, which returns a list instead of a set. To get the sorted vocabulary for document \"NewsArticles-2433\", we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n',\n",
       " '\"',\n",
       " \"'s\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '2',\n",
       " '2014',\n",
       " '22',\n",
       " '29',\n",
       " '33',\n",
       " '43',\n",
       " '500',\n",
       " ':',\n",
       " 'A',\n",
       " 'Akehmet',\n",
       " 'Aleksandrovich',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary(corpus_small, select='NewsArticles-2433', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the number of unique tokens in the corpus, i.e. the vocabulary size, we can use [`vocabulary_size`](api.rst#vocabulary_size), which is basically a shortcut for `len(vocabulary(<Corpus object>))`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2149"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import vocabulary_size\n",
    "\n",
    "vocabulary_size(corpus_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus function [`vocabulary_counts`](api.rst#vocabulary_size) is useful to find out how often each token in the vocabulary occurs in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Where': 2,\n",
       " 'types.-': 1,\n",
       " 'Reflection': 1,\n",
       " 'approach': 1,\n",
       " 'average': 1,\n",
       " 'cell': 4,\n",
       " 'conduct': 1,\n",
       " 'angry': 1,\n",
       " 'alone': 1,\n",
       " 'veterans': 2,\n",
       " 'tell': 1,\n",
       " 'Bill': 1,\n",
       " 'cyberexperts': 1,\n",
       " 'surge': 1,\n",
       " 'lead': 2,\n",
       " 'countering': 1,\n",
       " 'basically': 1,\n",
       " 'cleared': 3,\n",
       " 'Health': 2,\n",
       " 'boils': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.corpus import vocabulary_counts\n",
    "\n",
    "vocabulary_counts(corpus_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to obtain absolute counts, you can use the `proportions` parameter. Setting it to `1` gives you ordinary proportions (i.e. $\\frac{x_i}{\\sum_j x_j}$) and `2` gives you proportions on a log10 scale ($\\log_{10} \\frac{x_i}{\\sum_j x_j}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Where': 0.0002781254345709915,\n",
       " 'types.-': 0.00013906271728549575,\n",
       " 'Reflection': 0.00013906271728549575,\n",
       " 'approach': 0.00013906271728549575,\n",
       " 'average': 0.00013906271728549575,\n",
       " 'cell': 0.000556250869141983,\n",
       " 'conduct': 0.00013906271728549575,\n",
       " 'angry': 0.00013906271728549575,\n",
       " 'alone': 0.00013906271728549575,\n",
       " 'veterans': 0.0002781254345709915,\n",
       " 'tell': 0.00013906271728549575,\n",
       " 'Bill': 0.00013906271728549575,\n",
       " 'cyberexperts': 0.00013906271728549575,\n",
       " 'surge': 0.00013906271728549575,\n",
       " 'lead': 0.0002781254345709915,\n",
       " 'countering': 0.00013906271728549575,\n",
       " 'basically': 0.00013906271728549575,\n",
       " 'cleared': 0.00041718815185648727,\n",
       " 'Health': 0.0002781254345709915,\n",
       " 'boils': 0.00013906271728549575,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_proportions = vocabulary_counts(corpus_small, proportions=1)\n",
    "vocab_proportions   # will reuse that later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular output is often more convenient for displaying results. You can set the `as_table` parameter to `True` to get a dataframe of tokens and their frequency. You can also specify to sort the dataframe by specifying the column to sort by in the `as_table` parameter. By default, this will sort in ascending order, but if you prefix the column name by \"-\", you obtain a descending sort order. Here, we will get a table of tokens with their frequencies in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>the</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>,</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>.</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>to</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>\"</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>Kay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>rhetoric</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>series</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>answer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>ongoing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2149 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  freq\n",
       "849        the   321\n",
       "290          ,   307\n",
       "1461         .   250\n",
       "444         to   187\n",
       "1854         \"   169\n",
       "...        ...   ...\n",
       "1198       Kay     1\n",
       "1197  rhetoric     1\n",
       "1196    series     1\n",
       "1195    answer     1\n",
       "1074   ongoing     1\n",
       "\n",
       "[2149 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_counts(corpus_small, as_table='-freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that \"the\" and \"to\" are top-ranking tokens, along with some punctuation characters. We can check the share of tokens for \"the\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04463913224864414"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_proportions['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the token \"the\" occurs more the 4% of the time in the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) tagging\n",
    "\n",
    "Part-of-speech (POS) tagging finds the grammatical word-category for each token in a document. The method [pos_tag()](api.rst#tmtoolkit.preprocess.TMPreproc.pos_tag) employs this for the whole corpus. The found POS tags are added as metadata to each token. These tags conform to a specific *tagset* which is explained in the [spaCy documentation](https://spacy.io/api/annotation#pos-tagging). The POS tags can be used to annotate and filter the documents. Let's apply POS tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.pos_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see a new column `pos` with the found POS tag for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: TMPreproc as \"state machine\"\n",
    "\n",
    "Before continuing, we should clarify that a TMPreproc instance is a \"state machine\", i.e. its contents (the documents) and behavior can change when you call a method. An example:\n",
    "\n",
    "\n",
    "```python\n",
    "corpus = {\n",
    "    \"doc1\": \"Hello world!\",\n",
    "    \"doc2\": \"Another example\"\n",
    "}\n",
    "\n",
    "preproc = TMPreproc(corpus)     # documents are directly tokenized\n",
    "preproc.tokens\n",
    "\n",
    "# Out:\n",
    "# {\n",
    "#   'doc1': ['Hello', 'world', '!'],\n",
    "#   'doc2': ['Another', 'example']\n",
    "# }\n",
    "\n",
    "preproc.tokens_to_lowercase()   # this changes the documents\n",
    "preproc.tokens\n",
    "\n",
    "# Out:\n",
    "# {\n",
    "#   'doc1': ['hello', 'world', '!'],\n",
    "#   'doc2': ['another', 'example']\n",
    "# }\n",
    "```\n",
    "\n",
    "As you can see, the tokens \"inside\" `preproc` are changed *in place*. It's important to see that after calling the method `tokens_to_lowercase()`, the tokens in `preproc` were transformed and the original tokens from before calling this method are not available anymore. In Python, assigning a *mutable* object to a variable binds the same object only to a different name, it doesn't copy it. Since a `TMPreproc` object is a mutable object (you can change its state by calling its methods), when we simply assign such an object to a different variable (say `preproc_upper`) we essentially only have two names for the same object and by calling a method on one of these variable names, the values will be changed for *both* names.\n",
    "\n",
    "#### Copying `TMPreproc` objects\n",
    "\n",
    "What can we do about that? We need to *copy* the object which can be done with the [TMPreproc.copy()](api.rst#tmtoolkit.preprocess.TMPreproc.copy) method. By this, we create another variable `preproc_upper` that points to a separate `TMPreproc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_upper = preproc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the IDs confirm that we have two different objects\n",
    "id(preproc_upper), id(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_upper.transform_tokens(str.upper)\n",
    "\n",
    "# the transformation now only applied to \"preproc_upper\"\n",
    "preproc.vocabulary == preproc_upper.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample\n",
    "preproc_upper.tokens['NewsArticles-1880'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original \"preproc\" still holds the same data\n",
    "preproc.tokens['NewsArticles-1880'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this also uses up twice as much computer memory now. So you shouldn't create copies that often and also release unused memory by using `del`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the objects again\n",
    "del preproc_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and term normalization\n",
    "\n",
    "Before we start with token normalization, we will create a copy of the original `TMPreproc` object and its data, so that we can later use it for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig = preproc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization brings a token, if it is a word, to its base form. The lemma is already found out during the tokenization process and is available in the `lemma` metadata column. However, when you want to further process the tokens on the base of the lemmata, you should use the [lemmatize()](api.rst#tmtoolkit.preprocess.TMPreproc.lemmatize) method. This method sets the lemmata as tokens and all further processing will happen using the lemmatized tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.lemmatize()\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `lemma` column was copied over to the `token` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Stemming\n",
    "    \n",
    "tmtoolkit doesn't support stemming directly, since lemmatization is generally accepted as a better approach to bring different word forms of one word to a common base form. However, you may install [NLTK](https://www.nltk.org/) and apply stemming by using the [transform_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.transform_tokens) method together with the [stem()](api.rst#tmtoolkit.preprocess.stem) function.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how you further want to analyze the data, it may be necessary to \"clean\" or \"normalize\" your tokens in different ways in order to remove noise from the corpus, such as punctuation tokens or numbers, upper/lowercase forms of the same word, etc. Note that this is usually not necessary when you work with more modern approaches such as word embeddings (word vectors).   \n",
    "\n",
    "If you want to remove certain characters in *all* tokens in your documents, you can use [remove_chars_in_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_chars_in_tokens) and pass it a sequence of characters to remove. There is also a shortcut [remove_special_chars_in_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_special_chars_in_tokens) which will remove all \"special characters\" (all characters in [string.punction](https://docs.python.org/3/library/string.html#string.punctuation) by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.remove_chars_in_tokens(['-'])  # remove only \"-\"\n",
    "preproc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all punctuation\n",
    "preproc.remove_special_chars_in_tokens()\n",
    "preproc.print_summary()   # the \"?\" also vanishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common (but harsh) practice is to transform all tokens to lowercase forms, which can be done with [tokens_to_lowercase()](api.rst#tmtoolkit.preprocess.TMPreproc.tokens_to_lowercase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.tokens_to_lowercase()\n",
    "preproc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method [clean_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.clean_tokens) finally applies several steps that remove tokens that meet certain criteria. This includes removing:\n",
    "\n",
    "- punctuation tokens\n",
    "- stopwords (very common words for the given language)\n",
    "- empty tokens (i.e. `''`)\n",
    "- tokens that are longer or shorter than a certain number of characters\n",
    "- numbers  \n",
    "\n",
    "Note that this is a language-dependent method, because the default stopword list is determined per language. This method has many parameters to tweak, so it's recommended to check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punct., stopwords, empty tokens (this is the default)\n",
    "# plus tokens shorter than 2 characters and numeric tokens like \"2019\"\n",
    "preproc.clean_tokens(remove_numbers=True, remove_shorter_than=2)\n",
    "preproc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the removal of several tokens in the previous step, the document lengths for the processed corpus are much smaller than for the original corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.doc_lengths, preproc_orig.doc_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe that the vocabulary got smaller after the processing steps, which, for large corpora, is also important in terms of computation time and memory consumption for later analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preproc.vocabulary), len(preproc_orig.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also apply custom token transform functions by using [transform_tokens()](api.rst#tmtoolkit.preprocess.TMpreproc.transform_tokens) and passing it a function that should be applied to each token in each document (hence it must accept one string argument).\n",
    "\n",
    "First let's define such a function. Here we create a simple function that should return a token's \"shape\" in terms of the case of its characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def token_shape(t):\n",
    "    return ''.join(['X' if str.isupper(c) else 'x' for c in t])\n",
    "\n",
    "token_shape('EU'), token_shape('CamelCase'), token_shape('lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply this function to our documents (we will use the original documents here, because they were not transformed to lower case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy() # swap instances for later\n",
    "\n",
    "preproc_orig.transform_tokens(token_shape)   # apply function\n",
    "preproc_orig.print_summary()\n",
    "\n",
    "# remove instance\n",
    "del preproc_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding compound words and joining tokens\n",
    "\n",
    "Compound words like \"US-Student\" or \"non-recyclable\" can be expanded to separate tokens like \"US\", \"Student\" and \"non\", \"recyclable\" using [expand_compound_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.expand_compound_tokens). However, depending on the language model, most of these compounds will already be separated on initial tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_vocab = preproc.vocabulary\n",
    "preproc.expand_compound_tokens()\n",
    "\n",
    "# create set difference to show vocabulary tokens\n",
    "# that were expanded\n",
    "set(orig_vocab) - set(preproc.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to join together certain *subsequent* occurrences of tokens or token patterns. This means you can for example transform all of the subsequent tokens \"White\" and \"House\" to single tokens \"White_House\". In case you don't use n-grams (described in a separate section), this is very helpful when you want to capture a named entity that is made up by several tokens, such as persons, institutions or concepts like \"Climate Change\", as a single token. The method to use for this is [glue_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.glue_tokens). It accepts the following parameters:\n",
    "\n",
    "- a `patterns` sequence of length *N* that is used to match the subsequent *N* tokens;\n",
    "- a `glue` string that is used to join the matched subsequent tokens (by default: `\"_\"`).\n",
    "\n",
    "Along with that, you can adjust the token matching with the [common token matching parameters](#Common-parameters-for-pattern-matching-functions) described below.\n",
    "\n",
    "Let's \"glue\" all subsequent occurrences of \"White\" and \"House\". The `glue_tokens()` method will return a set of glued tokens that matched the provided pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig = preproc.copy()  # make a copy of full orig. data for later use\n",
    "preproc.glue_tokens(['White', 'House'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.tokens['NewsArticles-1880'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords-in-context (KWIC) and general filtering methods\n",
    "\n",
    "*Keywords-in-context (KWIC)* allow you to quickly investigate certain keywords and their neighborhood of tokens, i.e. the tokens that appear right before and after this keyword.\n",
    "\n",
    "`TMPreproc` provides three methods for this purpose:\n",
    "\n",
    "- [get_kwic()](api.rst#tmtoolkit.preprocess.TMPreproc.get_kwic) is the base method accepting a search pattern and several options that control how the search pattern is matched (more on that below); use this function when you want to further process the output of a KWIC search;\n",
    "- [get_kwic_table()](api.rst#tmtoolkit.preprocess.TMPreproc.get_kwic_table) is the more \"user friendly\" version of the above method as it produces a datatable with the highlighted keyword by default\n",
    "- [filter_tokens_with_kwic()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens_with_kwic) works similar to the above functions but applies the result by filtering the documents again; it is explained in the [section on filtering](#Filtering-tokens-and-documents)\n",
    "\n",
    "Let's see the KWIC methods in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # use orig. full data\n",
    "preproc.get_kwic('house', ignore_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method returns a dictionary that maps document labels to the KWIC results. Each document contains a list of \"contexts\", i.e. a list of tokens that surround a keyword, here `\"house\"`. This keyword stands in the middle and is surrounded by its \"context tokens\", which by default means two tokens to the left and two tokens to the right (which may be less when the keyword is near the start or the end of a document). \n",
    "\n",
    "We can see that `NewsArticles-1880` contains four contexts, `NewsArticles-99` one context and `NewsArticles-3350` none.\n",
    "\n",
    "With `get_kwic_table()`, we get back a datatable which provides a better formatting for quick investigation. See how the matched tokens are highlighted as `*house*` and empty results are removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.get_kwic_table('house', ignore_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter is `context_size`. It determines the number of tokens to display left and right to the found keyword. You can either pass a single integer for a symmetric context or a tuple with integers `(<left>, <right>)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.get_kwic_table('house', ignore_case=True, context_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.get_kwic_table('house', ignore_case=True, context_size=(1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KWIC functions become really powerful when using the pattern matching options. So far, we were looking for *exact* (but case insensitive) matches between the corpus tokens and our keyword `\"house\"`. However, it is also possible to match patterns like `\"new*\"` (matches any word starting with \"new\") or `\"agenc(y|ies)\"` (a regular expression matching \"agency\" and \"agencies\"). The next section gives an introduction on the different options for pattern matching.\n",
    "\n",
    "#### Common parameters for pattern matching functions\n",
    "\n",
    "Several functions and methods in tmtoolkit support pattern matching, including the already mentioned KWIC functions but also functions for filtering tokens or documents as you will see later. They all share similar function signatures, i.e. similar parameters:\n",
    "\n",
    "- `search_token` or `search_tokens`: allows to specify one or more patterns as strings\n",
    "- `match_type`: sets the matching type and can be one of the following options:\n",
    "  - `'exact'` (default): exact string matching (optionally ignoring character case), i.e. no pattern matching\n",
    "  - `'regex'` uses [regular expression](https://docs.python.org/3/library/re.html) matching\n",
    "  - `'glob'` uses \"glob patterns\" like `\"politic*\"` which matches for example \"politic\", \"politics\" or \"politician\" (see [globre package](https://pypi.org/project/globre/))\n",
    "- `ignore_case`: ignore character case (applies to all three match types)\n",
    "- `glob_method`: if `match_type` is 'glob', use this glob method. Must be `'match'` or `'search'` (similar behavior as Python's [re.match](https://docs.python.org/3/library/re.html#re.match) or [re.search](https://docs.python.org/3/library/re.html#re.search))\n",
    "- `inverse`: inverse the match results, i.e. if matching for \"hello\", return all results that do *not* match \"hello\"\n",
    "\n",
    "Let's try out some of these options with `get_kwic_table()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a regular expression, ignoring case\n",
    "preproc.get_kwic_table(r'agenc(y|ies)', match_type='regex', ignore_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a glob, ignoring case\n",
    "preproc.get_kwic_table('pol*', match_type='glob', ignore_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a glob, ignoring case\n",
    "preproc.get_kwic_table('*sol*', match_type='glob', ignore_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a regex that matches all tokens with at least one vowel and\n",
    "# inverting these matches, i.e. all tokens *without* any vowels\n",
    "preproc.get_kwic_table(r'[AEIOUaeiou]', match_type='regex', inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering tokens and documents\n",
    "\n",
    "We can use the pattern matching parameters in numerous filtering methods. The heart of many of these methods is [token_match()](api.rst#tmtoolkit.preprocess.token_match). Given a search pattern, a list of tokens and optionally some pattern matching parameters, it returns a binary NumPy array of the same length as the input tokens. Each occurrence of `True` in this binary array signals a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.preprocess import token_match\n",
    "\n",
    "# first 10 tokens of document \"NewsArticles-1880\"\n",
    "doc_snippet = preproc.tokens['NewsArticles-1880'][:10]\n",
    "# get all tokens that match \"to*\"\n",
    "matches = token_match('to*', doc_snippet, match_type='glob')\n",
    "\n",
    "# iterate through tokens and matches, show pair-wise results\n",
    "for tok, match in zip(doc_snippet, matches):\n",
    "    print(tok, ':', match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `token_match()` function is a rather low-level function that you may use for pattern matching against any list/array of strings, e.g. a list of tokens, file names, etc.\n",
    "\n",
    "The following methods cover common use-cases for filtering during text preprocessing. Many of these methods start either with `filter_...()` or `remove_...()` and these pairs of filter and remove functions are complements. A filter method will always *retain* the matched elements whereas a remove method will always *drop* the matched elements. We can observe that with the first pair of method, [filter_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens) and [remove_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_tokens):\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "So much `.copy()`\n",
    "    \n",
    "Note that the following code snippets make lot of use of the `copy()` methods. This is because we want to show how the different methods work with the *same original data* (remember that a `TMPreproc` instance behaves like a state machine) and also want to \"clean up\" the temporary instances. Under normal circumstances, you wouldn't use `copy()` so excessively.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the tokens that match the pattern in each document\n",
    "preproc.filter_tokens('*house*', match_type='glob', ignore_case=True)\n",
    "preproc.print_summary()\n",
    "\n",
    "del preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.remove_tokens('*house*', match_type='glob', ignore_case=True)\n",
    "preproc.print_summary()\n",
    "\n",
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pair [filter_documents()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_documents) and [remove_documents()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_documents) works similarily, but filters or drops whole documents regarding the supplied match criteria. Both accept the standard pattern matching parameters but also a parameter `matches_threshold` with default value `1`. When this number of matching tokens is hit, the document will be part of the result set (`filter_documents()`) or removed from the result set (`remove_documents()`). By this, we can for example retain only those documents that contain certain token patterns.\n",
    "\n",
    "Let's try these methods out in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.filter_documents('*house*', match_type='glob', ignore_case=True)\n",
    "preproc.print_summary()\n",
    "\n",
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that two out of three documents contained the pattern `'*house*'` and hence were retained.\n",
    "\n",
    "We can also adjust `matches_threshold` to set the minimum number of token matches for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.filter_documents('*house*', match_type='glob', ignore_case=True,\n",
    "                         matches_threshold=4)\n",
    "preproc.print_summary()\n",
    "\n",
    "del preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.remove_documents('*house*', match_type='glob', ignore_case=True)\n",
    "preproc.print_summary()\n",
    "\n",
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `remove_documents()` we get only the documents that did *not* contain the specified pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful pair of methods is [filter_documents_by_name()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_documents_by_name) and [remove_documents_by_name()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_documents_by_name). Both methods again accept the same pattern matching parameters but they only apply them to the document names, i.e. document *labels*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.filter_documents_by_name(r'-\\d{4}$', match_type='regex')\n",
    "preproc.print_summary()\n",
    "\n",
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we wanted to retain only the documents whose document labels ended with exactly 4 digits, like \"...-1234\". Hence, we only get \"NewsArticles-1880\" and \"NewsArticles-3350\" but not \"NewsArticles-99\". Again, `remove_documents_by_name()` will do the exact opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also use [Keywords-in-context (KWIC)](#Keywords-in-context-(KWIC)-and-general-filtering-methods) to filter your tokens in the neighborhood around certain keyword pattern(s). The method for that is called [filter_tokens_with_kwic()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens_with_kwic) and works very similar to [get_kwic()](api.rst#tmtoolkit.preprocess.TMPreproc.get_kwic) but filters the documents in the `TMPreproc` instance with which you can continue working as usual. Here, we filter the tokens in each document to get the tokens directly in front and after the glob pattern `'*house*'` (`context_size=1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.filter_tokens_with_kwic('*house*', context_size=1,\n",
    "                                match_type='glob', ignore_case=True)\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you annotated your documents' tokens with Part-of-Speech (POS) tags, you can also filter them using [filter_for_pos()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_for_pos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc \n",
    "\n",
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "# apply POS tagging and retain only nouns\n",
    "preproc.pos_tag().filter_for_pos('N').tokens_datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we filtered for tokens that were identified as nouns by passing the *simplified POS tag* `'N'` (for more on simplified tags, see the method documentation). We can also filter for more than one tag, e.g. nouns or verbs by passing a list of required POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter_for_pos()` has no `remove_...()` counterpart, but you can set the `inverse` parameter to `True` to achieve the same effect.\n",
    "\n",
    "Finally there are two methods for removing tokens based on their [document frequency](#Accessing-tokens,-vocabulary-and-other-important-properties): [remove_common_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_common_tokens) and [remove_uncommon_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_uncommon_tokens). The former removes all tokens that have a document frequency greater or equal a certain threshold defined by parameter `df_threshold`. The latter does the same for all tokens that have a document frequency lower or equal `df_threshold`. This parameter can either be a relative frequency (default) or absolute count (by setting parameter `absolute=True`).\n",
    "\n",
    "Before applying the method, let's have a look at the number of tokens per document again, to later see how many we will remove. We will also store the vocabulary in `orig_vocab` for later comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "orig_vocab = preproc.vocabulary\n",
    "preproc.doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.remove_common_tokens(df_threshold=0.9).doc_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing all tokens with a document frequency threshold of 0.9, we removed quite a number of tokens in each document. Let's investigate the vocabulary in order to see which tokens were removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set difference gives removed vocabulary tokens\n",
    "set(orig_vocab) - set(preproc.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remove_uncommon_tokens()` works similarily. This time, let's use an absolute number as threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.remove_uncommon_tokens(df_threshold=1, absolute=True)\n",
    "\n",
    "# set difference gives removed vocabulary tokens\n",
    "# this time, show only the first 10 tokens that were removed\n",
    "sorted(set(orig_vocab) - set(preproc.vocabulary))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above means that we remove all tokens that appear only in exactly one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with token metadata\n",
    "\n",
    "`TMPreproc` allows to attach arbitrary metadata to each token in each document. This kind of \"annotations\" for tokens is very useful. For example, you may add metadata that records a token's length or whether it is all uppercase letters and later use that for filtering or in further analyses. One method to add such metadata is [add_metadata_per_doc()](api.rst#tmtoolkit.preprocess.TMPreproc.add_metadata_per_doc). This method requires to pass a dict that maps document labels to the respective token metadata list. The list's length must match the number of tokens in the respective document. At first we need to create such a metadata dict. Let's do that for the tokens' length first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "meta_tok_lengths = {doc_label: list(map(len, doc_tokens))\n",
    "                    for doc_label, doc_tokens in preproc.tokens.items()}\n",
    "\n",
    "# show first 5 tokens and their string length for a sample document\n",
    "list(zip(preproc.tokens['NewsArticles-1880'][:10],\n",
    "         meta_tok_lengths['NewsArticles-1880'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add these metadata via [add_metadata_per_doc()](api.rst#tmtoolkit.preprocess.TMPreproc.add_metadata_per_doc). We pass a label, the metadata key, and the previously generated metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.add_metadata_per_doc('length', meta_tok_lengths)\n",
    "del meta_tok_lengths  # we don't need that object anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property `.tokens_datatable` now shows an additional column with `meta_length` (the metadata key in always prefixed with `meta_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a boolean indicator for whether the given token is all uppercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_tok_upper = {doc_label: list(map(str.isupper, doc_tokens))\n",
    "                  for doc_label, doc_tokens in preproc.tokens.items()}\n",
    "\n",
    "preproc.add_metadata_per_doc('upper', meta_tok_upper)\n",
    "del meta_tok_upper\n",
    "\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use these newly added columns now for example for filtering the datatable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "\n",
    "preproc.tokens_datatable[dt.f.meta_upper == 1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which metadata keys were already created, you can use [get_available_metadata_keys()](api.rst#tmtoolkit.preprocess.TMPreproc.get_available_metadata_keys):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.get_available_metadata_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token metadata can be removed with [remove_metadata()](api.rst#tmtoolkit.preprocess.TMPreproc.remove_metadata):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.remove_metadata('upper')\n",
    "preproc.get_available_metadata_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell [filter_tokens()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens) and similar methods to use metadata instead of the tokens for matching. For example, we can use the metadata `meta_length`, which we created before, to filter for tokens of a certain length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_meta_example = preproc.copy()\n",
    "preproc_meta_example.filter_tokens(3, by_meta='length')\n",
    "preproc_meta_example.tokens_datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc_meta_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all matching options then apply to the metadata column, in this case to the `meta_length` column which contains integers. Since `filter_tokens()` by default employs exact matching, we get all tokens where `meta_length` equals the first argument, `3`. If we used regular expression or glob matching instead, this method would fail because you can only use that for string data.\n",
    "\n",
    "If you want to use more complex filter queries, you should create a \"filter mask\" and pass it to [filter_tokens_by_mask()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens_by_mask). A filter mask is a dictionary that maps a document label to a sequence of booleans. For all occurrences of `True`, the respective token in the document will be retained, all others will be removed. Let's try that out with a small sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.pos_tag().tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate the filter mask, which means for each document we create a boolean list or array that for each token in that document indicates whether that token should be kept or removed.\n",
    "\n",
    "We will iterate through the [tokens_with_metadata](api.rst#tmtoolkit.preprocess.TMPreproc.tokens_with_metadata) property, which is a dict that for each document contains a datatable with its tokens and metadata. Let's have a look at the first document's datatable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(preproc.tokens_with_metadata.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the filter mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "filter_mask = {}\n",
    "for doc_label, doc_data in preproc.tokens_with_metadata.items():\n",
    "    # extract the columns \"meta_length\" and \"pos\"\n",
    "    # and convert them to NumPy arrays\n",
    "    doc_data_subset = doc_data[:, [dt.f.meta_length, dt.f.pos]]\n",
    "    tok_lengths, tok_pos = map(np.array, doc_data_subset.to_list())\n",
    "    \n",
    "    # create a boolean array for nouns with token length less or equal 5\n",
    "    filter_mask[doc_label] = (tok_lengths <= 5) & np.isin(tok_pos, ['NOUN', 'PROPN'])\n",
    "\n",
    "# it's not necessary to add the filter mask as metadata\n",
    "# but it's a good way to check the mask\n",
    "preproc.add_metadata_per_doc('small_nouns', filter_mask)\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can pass the mask dict to [filter_tokens_by_mask()](api.rst#tmtoolkit.preprocess.TMPreproc.filter_tokens_by_mask):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.filter_tokens_by_mask(filter_mask)\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating n-grams\n",
    "\n",
    "So far, we worked with *unigrams*, i.e. each document consisted of a sequence of discrete tokens. We can also generate *n-grams* from our corpus where each document consists of a sequence of *n* subsequent tokens. An example would be:\n",
    "\n",
    "Document: \"This is a simple example.\"\n",
    "\n",
    "**n=1 (unigrams):**\n",
    "\n",
    "    ['This', 'is', 'a', 'simple', 'example', '.']\n",
    "\n",
    "**n=2 (bigrams):**\n",
    "\n",
    "    ['This is', 'is a', 'a simple', 'simple example', 'example .']\n",
    "\n",
    "**n=3 (trigrams):**\n",
    "\n",
    "    ['This is a', 'is a simple', 'a simple example', 'simple example .']\n",
    "\n",
    "The method [generate_ngrams()](api.rst#tmtoolkit.preprocess.TMPreproc.generate_ngrams) allows us to generate n-grams from tokenized documents. We can then get the results with the `ngrams` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc\n",
    "\n",
    "preproc = preproc_orig.copy()  # make a copy from full data\n",
    "\n",
    "preproc.generate_ngrams(2)  # generate bigrams\n",
    "preproc.ngrams['NewsArticles-1880'][:10]  # show first 10 bigrams of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may afterwards use [join_ngrams()](api.rst#tmtoolkit.preprocess.TMPreproc.join_ngrams) to merge the generated n-grams to joint tokens and use these as new tokens in this TMPreproc instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.join_ngrams()\n",
    "preproc.tokens_datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a sparse document-term matrix (DTM)\n",
    "\n",
    "If you're working with a bag-of-words representation of your data, you usually convert the preprocessed documents to a document-term matrix (DTM), which represents of the number of occurrences of each term (i.e. vocabulary token) in each document. This is a *N* rows by *M* columns matrix, where *N* is the number of documents and *M* is the vocabulary size (i.e. the number of unique tokens in the corpus).\n",
    "\n",
    "Not all tokens from the vocabulary occur in all documents. In fact, many tokens will occur only in a small subset of the documents if you're dealing with a \"real world\" dataset. This means that most entries in such a DTM will be zero. Almost all functions in tmtoolkit therefore generate and work with *sparse* matrices, where only non-zero values are stored in computer memory.\n",
    "\n",
    "For this example, we'll generate a DTM from the `preproc_orig` instance. First, let's check the number of documents and the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.n_docs, preproc_orig.vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [dtm](api.rst#tmtoolkit.preprocess.TMPreproc.dtm) property to generate a sparse DTM from the current instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a sparse matrix with 3 rows (which corresponds with the number of documents) and 683 columns was generated (which corresponds to the vocabulary size). 816 elements in this matrix are non-zero.\n",
    "\n",
    "We can convert this matrix to a non-sparse, i.e. *dense*, representation and see parts of its elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.dtm.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that you should only convert a sparse matrix to a dense representation when you're either dealing with a small amount of data (which is what we're doing in this example), or use only a part of the full matrix. Converting a sparse matrix to a dense representation can otherwise easily exceed the available computer memory.\n",
    "\n",
    "There exist different \"formats\" for sparse matrices, which have different advantages and disadvantages (see for example the [SciPy \"sparse\" module documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html#usage-information)). **Not all formats support all operations that you can usually apply to an ordinary, dense matrix.** By default, the generated DTM is in *Compressed Sparse Row (CSR)* format. This format allows indexing and is especially optimized for fast row access. You may convert it to any other sparse matrix format; see the mentioned SciPy documentation for this.\n",
    "\n",
    "The rows of the DTM are aligned to the sequence of the document labels and its columns are aligned to the vocabulary. For example, let's find the frequency of the term \"House\" in the document \"NewsArticles-1880\". To do this, we find out the indices into the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.doc_labels.index('NewsArticles-1880')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.vocabulary.index('House')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the frequency of the term \"House\" in the document \"NewsArticles-1880\" is located in row 0 and column 4 of the DTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.dtm[0, 67]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the following example of finding out the index for \"administration\" and then getting an array that represents the number of occurrences of this token across all three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_admin_ix = preproc_orig.vocabulary.index('administration')\n",
    "preproc_orig.dtm[:, vocab_admin_ix].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the [dtm](api.rst#tmtoolkit.preprocess.TMPreproc.dtm) property, there's also the [get_dtm()](api.rst#tmtoolkit.preprocess.TMPreproc.get_dtm) method which allows to also return the result as datatable or pandas DataFrame. Note that these representations are not sparse and hence can consume much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.get_dtm(as_datatable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization: Saving and loading `TMPreproc` objects\n",
    "\n",
    "The current state of a `TMPreproc` object can also be stored to a file on disk so that you (or someone else who has tmtoolkit installed) can later restore it using that file. The methods for that are [save_state()](api.rst#tmtoolkit.preprocess.TMPreproc.save_state) and [load_state()](api.rst#tmtoolkit.preprocess.TMPreproc.load_state) / [from_state()](api.rst#tmtoolkit.preprocess.TMPreproc.from_state).\n",
    "\n",
    "Let's store the current state of the `preproc_orig` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.print_summary()\n",
    "preproc_orig.save_state('data/preproc_state.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the object by retaining only documents that contain the token \"house\" (see the reduced number of documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_orig.filter_documents('*house*', match_type='glob', ignore_case=True)\n",
    "preproc_orig.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restore the saved data using [from_state()](api.rst#tmtoolkit.preprocess.TMPreproc.from_state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_restored = TMPreproc.from_state('data/preproc_state.pickle')\n",
    "preproc_restored.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the full dataset with three documents was restored.\n",
    "\n",
    "This is very useful especially when you have a large amount of data and run time consuming operations, e.g. POS tagging. When you're finished running these operations, you can easily store the current state to disk and later retrieve it without the need to re-run these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n",
    "The `TMPreproc` class provides a convenient object-oriented interface for parallel text processing and analysis. There is also a *functional API* provided in the [tmtoolkit.preprocess](api.rst#tmtoolkit-preprocess) module. Most of these functions accept a list of spaCy documents along with additional parameters. You may use these functions for quick prototyping, but it is generally much more convenient to use `TMPreproc`. The functional API does not provide parallel processing.\n",
    "\n",
    "To initialize the functional API for a certain language, you need to start with [init_for_language()](api.rst#tmtoolkit.preprocess.init_for_language) and may then tokenize your raw text documents via [tokenize()](api.rst#tmtoolkit.preprocess.tokenize), which will generate a list of spaCy documents. Most other functions in this API accept such a list of list of spaCy documents as input.\n",
    "\n",
    "```\n",
    "init_for_language('en')\n",
    "docs = tokenize(['Hello this is a test.', 'And here comes another one.'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result after applying preprocessing steps and hence transforming the text data is often a document-term matrix (DTM). The [bow module](api.rst#tmtoolkit-bow) contains several functions to work with DTMs, e.g. apply transformations such as *tf-idf* or compute some important summary statistics. The [next chapter](bow.ipynb) will introduce some of these functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
